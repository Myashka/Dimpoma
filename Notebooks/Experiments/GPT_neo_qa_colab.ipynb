{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc5a668",
   "metadata": {
    "id": "7dc5a668"
   },
   "source": [
    "# Общая информация\n",
    "__Цель:__ сделать fine-tuning mGPT by Sber\n",
    "\n",
    "__Задачи:__\n",
    "1) Выбрать вопросы с определенным тегом, например python\n",
    "\n",
    "2) Понять формат входных и выходных данных, например перед вопросов, возможно надо ставить [QUESTION]\n",
    "\n",
    "3) Сделать torch Dataset\n",
    "\n",
    "4) Определить, как делать evaluation\n",
    "\n",
    "5) Способ трэкинга\n",
    "\n",
    "6) Проанализировать результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf5036",
   "metadata": {
    "id": "20cf5036"
   },
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "KFowklRjGeJG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFowklRjGeJG",
    "outputId": "b1140073-59bb-4595-d916-1b83cf981a25"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zvK3TNhzDJBj",
   "metadata": {
    "id": "zvK3TNhzDJBj"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f0cd35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "e6a80162bdf140b1808a882cdddbe9fc",
      "069aab3228b4488cbbac21db25394558",
      "3146863510284503bd73c85038250799",
      "481af281fe894850a446b5810f092576",
      "cd897e9ff98c454ba0df61dcdee7d67d",
      "7dea1008a50f482eab194de2b6870ea9",
      "e85337d1de3b48fcb5ffebceb5877614",
      "aa7afef9e81d4040b6ce820d1e3eda74",
      "e3758c7ae1384eae90279b18e46bbc3e",
      "b0b533a5ed8b4c069710a7f7a0b5e83d",
      "efff7ee2f2d5466e95000fe4c489d787"
     ]
    },
    "id": "73f0cd35",
    "outputId": "4694f5fe-a46a-40cb-b5d6-a6ff3a2c24dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a80162bdf140b1808a882cdddbe9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          IntervalStrategy, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa2d2a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fa2d2a1",
    "outputId": "37228a05-7b85-4af6-f69e-979097052f2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efcbf549870>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# зафиксируем random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1fe0d",
   "metadata": {
    "id": "7bf1fe0d"
   },
   "source": [
    "# Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0QXDhwLrFloX",
   "metadata": {
    "id": "0QXDhwLrFloX"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/vkr_data/filtered_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Vrd74IkvG2mv",
   "metadata": {
    "id": "Vrd74IkvG2mv"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79da9c71",
   "metadata": {
    "id": "79da9c71"
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# with open( f'../../data/filtered_df.p', 'rb') as f:\n",
    "#     df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d30bef",
   "metadata": {
    "id": "c0d30bef"
   },
   "source": [
    "Отсортируем датасет по времени и разобьем его на train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad6cbf6",
   "metadata": {
    "id": "fad6cbf6"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(\"Q_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f0d0e4",
   "metadata": {
    "id": "95f0d0e4"
   },
   "outputs": [],
   "source": [
    "df = df.loc[df.apply(lambda x: f\"python\" in x.Tag, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61090fb5",
   "metadata": {
    "id": "61090fb5"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = np.split(df, [int(0.75 * len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88795940",
   "metadata": {
    "id": "88795940"
   },
   "source": [
    "# Загрузим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "N8LsmADQDwPr",
   "metadata": {
    "id": "N8LsmADQDwPr"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6215b66",
   "metadata": {
    "id": "e6215b66"
   },
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a55a4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31a55a4c",
    "outputId": "cab9ceb9-029b-494d-f2f7-fa7bb8410c7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    bos_token=\"<|startoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|pad|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "925bde66",
   "metadata": {
    "id": "925bde66"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54ed943e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54ed943e",
    "outputId": "265012cb-6c45-4395-cf90-acfdc7a668fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "CS_yzJboLHFl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CS_yzJboLHFl",
    "outputId": "0f0f0e55-a156-4357-cb96-568c348f5337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze transformer.h.1.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.1.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.1.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.1.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.1.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.1.mlp.c_fc.weight\n",
      "Freeze transformer.h.1.mlp.c_fc.bias\n",
      "Freeze transformer.h.1.mlp.c_proj.weight\n",
      "Freeze transformer.h.1.mlp.c_proj.bias\n",
      "Freeze transformer.h.2.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.2.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.2.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.2.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.2.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.2.mlp.c_fc.weight\n",
      "Freeze transformer.h.2.mlp.c_fc.bias\n",
      "Freeze transformer.h.2.mlp.c_proj.weight\n",
      "Freeze transformer.h.2.mlp.c_proj.bias\n",
      "Freeze transformer.h.3.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.3.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.3.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.3.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.3.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.3.mlp.c_fc.weight\n",
      "Freeze transformer.h.3.mlp.c_fc.bias\n",
      "Freeze transformer.h.3.mlp.c_proj.weight\n",
      "Freeze transformer.h.3.mlp.c_proj.bias\n",
      "Freeze transformer.h.4.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.4.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.4.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.4.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.4.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.4.mlp.c_fc.weight\n",
      "Freeze transformer.h.4.mlp.c_fc.bias\n",
      "Freeze transformer.h.4.mlp.c_proj.weight\n",
      "Freeze transformer.h.4.mlp.c_proj.bias\n",
      "Freeze transformer.h.5.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.5.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.5.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.5.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.5.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.5.mlp.c_fc.weight\n",
      "Freeze transformer.h.5.mlp.c_fc.bias\n",
      "Freeze transformer.h.5.mlp.c_proj.weight\n",
      "Freeze transformer.h.5.mlp.c_proj.bias\n",
      "Freeze transformer.h.6.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.6.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.6.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.6.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.6.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.6.mlp.c_fc.weight\n",
      "Freeze transformer.h.6.mlp.c_fc.bias\n",
      "Freeze transformer.h.6.mlp.c_proj.weight\n",
      "Freeze transformer.h.6.mlp.c_proj.bias\n",
      "Freeze transformer.h.7.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.7.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.7.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.7.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.7.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.7.mlp.c_fc.weight\n",
      "Freeze transformer.h.7.mlp.c_fc.bias\n",
      "Freeze transformer.h.7.mlp.c_proj.weight\n",
      "Freeze transformer.h.7.mlp.c_proj.bias\n",
      "Freeze transformer.h.8.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.8.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.8.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.8.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.8.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.8.mlp.c_fc.weight\n",
      "Freeze transformer.h.8.mlp.c_fc.bias\n",
      "Freeze transformer.h.8.mlp.c_proj.weight\n",
      "Freeze transformer.h.8.mlp.c_proj.bias\n",
      "Freeze transformer.h.9.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.9.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.9.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.9.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.9.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.9.mlp.c_fc.weight\n",
      "Freeze transformer.h.9.mlp.c_fc.bias\n",
      "Freeze transformer.h.9.mlp.c_proj.weight\n",
      "Freeze transformer.h.9.mlp.c_proj.bias\n",
      "Freeze transformer.h.10.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.10.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.10.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.10.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.10.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.10.mlp.c_fc.weight\n",
      "Freeze transformer.h.10.mlp.c_fc.bias\n",
      "Freeze transformer.h.10.mlp.c_proj.weight\n",
      "Freeze transformer.h.10.mlp.c_proj.bias\n",
      "Freeze transformer.h.11.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.11.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.11.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.11.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.11.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.11.mlp.c_fc.weight\n",
      "Freeze transformer.h.11.mlp.c_fc.bias\n",
      "Freeze transformer.h.11.mlp.c_proj.weight\n",
      "Freeze transformer.h.11.mlp.c_proj.bias\n",
      "Freeze transformer.h.12.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.12.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.12.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.12.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.12.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.12.mlp.c_fc.weight\n",
      "Freeze transformer.h.12.mlp.c_fc.bias\n",
      "Freeze transformer.h.12.mlp.c_proj.weight\n",
      "Freeze transformer.h.12.mlp.c_proj.bias\n",
      "Freeze transformer.h.13.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.13.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.13.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.13.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.13.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.13.mlp.c_fc.weight\n",
      "Freeze transformer.h.13.mlp.c_fc.bias\n",
      "Freeze transformer.h.13.mlp.c_proj.weight\n",
      "Freeze transformer.h.13.mlp.c_proj.bias\n",
      "Freeze transformer.h.14.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.14.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.14.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.14.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.14.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.14.mlp.c_fc.weight\n",
      "Freeze transformer.h.14.mlp.c_fc.bias\n",
      "Freeze transformer.h.14.mlp.c_proj.weight\n",
      "Freeze transformer.h.14.mlp.c_proj.bias\n",
      "Freeze transformer.h.15.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.15.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.15.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.15.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.15.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.15.mlp.c_fc.weight\n",
      "Freeze transformer.h.15.mlp.c_fc.bias\n",
      "Freeze transformer.h.15.mlp.c_proj.weight\n",
      "Freeze transformer.h.15.mlp.c_proj.bias\n",
      "Freeze transformer.h.16.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.16.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.16.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.16.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.16.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.16.mlp.c_fc.weight\n",
      "Freeze transformer.h.16.mlp.c_fc.bias\n",
      "Freeze transformer.h.16.mlp.c_proj.weight\n",
      "Freeze transformer.h.16.mlp.c_proj.bias\n",
      "Freeze transformer.h.17.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.17.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.17.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.17.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.17.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.17.mlp.c_fc.weight\n",
      "Freeze transformer.h.17.mlp.c_fc.bias\n",
      "Freeze transformer.h.17.mlp.c_proj.weight\n",
      "Freeze transformer.h.17.mlp.c_proj.bias\n",
      "Freeze transformer.h.18.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.18.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.18.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.18.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.18.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.18.mlp.c_fc.weight\n",
      "Freeze transformer.h.18.mlp.c_fc.bias\n",
      "Freeze transformer.h.18.mlp.c_proj.weight\n",
      "Freeze transformer.h.18.mlp.c_proj.bias\n",
      "Freeze transformer.h.19.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.19.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.19.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.19.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.19.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.19.mlp.c_fc.weight\n",
      "Freeze transformer.h.19.mlp.c_fc.bias\n",
      "Freeze transformer.h.19.mlp.c_proj.weight\n",
      "Freeze transformer.h.19.mlp.c_proj.bias\n",
      "Freeze transformer.h.20.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.20.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.20.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.20.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.20.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.20.mlp.c_fc.weight\n",
      "Freeze transformer.h.20.mlp.c_fc.bias\n",
      "Freeze transformer.h.20.mlp.c_proj.weight\n",
      "Freeze transformer.h.20.mlp.c_proj.bias\n",
      "Freeze transformer.h.21.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.21.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.21.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.21.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.21.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.21.mlp.c_fc.weight\n",
      "Freeze transformer.h.21.mlp.c_fc.bias\n",
      "Freeze transformer.h.21.mlp.c_proj.weight\n",
      "Freeze transformer.h.21.mlp.c_proj.bias\n",
      "Freeze transformer.h.22.attn.attention.k_proj.weight\n",
      "Freeze transformer.h.22.attn.attention.v_proj.weight\n",
      "Freeze transformer.h.22.attn.attention.q_proj.weight\n",
      "Freeze transformer.h.22.attn.attention.out_proj.weight\n",
      "Freeze transformer.h.22.attn.attention.out_proj.bias\n",
      "Freeze transformer.h.22.mlp.c_fc.weight\n",
      "Freeze transformer.h.22.mlp.c_fc.bias\n",
      "Freeze transformer.h.22.mlp.c_proj.weight\n",
      "Freeze transformer.h.22.mlp.c_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if \"transformer.h\" in n:\n",
    "        layer_num = int(n.split(\".\")[2])\n",
    "        if \"ln_\" not in n and layer_num > 0 and layer_num < 23:\n",
    "            p.requires_grad = False\n",
    "            print(\"Freeze\", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911c6fc",
   "metadata": {
    "id": "3911c6fc"
   },
   "source": [
    "Отберем только те строки, в тегах которых есть слово _python_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5156e7d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "5156e7d0",
    "outputId": "0458b5b6-aeba-467b-890b-9741c65d96dc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-113de4c76018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_Body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max length: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "questions = df.Q_Body\n",
    "max_length = max([len(tokenizer.encode(question)) for question in questions])\n",
    "print(f\"Max length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb78b459",
   "metadata": {
    "id": "eb78b459"
   },
   "outputs": [],
   "source": [
    "class Q_A_Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, tag):\n",
    "        df = df.loc[df.apply(lambda x: f\"{tag}\" in x.Tag, axis=1)]\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.answers = []\n",
    "        self.questions = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            prep_text = f\"<|startoftext|>Question: {row.Q_Body}\\nAnswer: {row.A_Body}<|endoftext|>\"\n",
    "\n",
    "            question_len = len(\n",
    "                tokenizer(\n",
    "                    f\"<|startoftext|>Question: {row.Q_Body}\\nAnswer:\",\n",
    "                )[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            encoding_dict = tokenizer(\n",
    "                prep_text, truncation=True, max_length=max_length, padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encoding_dict[\"input_ids\"]))\n",
    "            self.attn_masks.append(torch.tensor(encoding_dict[\"attention_mask\"]))\n",
    "            self.labels.append(torch.tensor(encoding_dict[\"input_ids\"]))\n",
    "            self.labels[-1][:question_len] = -100\n",
    "\n",
    "            self.answers.append(row.A_Body)\n",
    "            self.questions.append(row.Q_Body)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.input_ids[idx],\n",
    "            self.attn_masks[idx],\n",
    "            self.labels[idx],\n",
    "            self.answers[idx],\n",
    "            self.questions[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8022f386",
   "metadata": {
    "id": "8022f386"
   },
   "outputs": [],
   "source": [
    "train_dataset = Q_A_Dataset(train_df, tokenizer, max_length=max_length, tag=\"python\")\n",
    "test_dataset = Q_A_Dataset(test_df, tokenizer, max_length=max_length, tag=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "852d170b",
   "metadata": {
    "id": "852d170b"
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cdfcd",
   "metadata": {
    "id": "7f4cdfcd"
   },
   "source": [
    "# Определим evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1db983",
   "metadata": {
    "id": "0b1db983"
   },
   "source": [
    "Определить метрики качества \\ __BLEU, ROUGE__\n",
    "\n",
    "Трекинг модели в wandb\n",
    "\n",
    "API HF Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "i77DFPhCHOqI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i77DFPhCHOqI",
    "outputId": "54c6dac6-9791-4dac-e4e6-e3496cf72165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.2.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.6)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (2022.6.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (1.1.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=59fb2769ab08c05632a3b7acd45f6cb5a05189ec694e142484513ab96f071c1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1004533",
   "metadata": {
    "id": "f1004533"
   },
   "outputs": [],
   "source": [
    "bleu = load_metric(\"bleu\")\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cfd9f",
   "metadata": {
    "id": "3e6cfd9f"
   },
   "source": [
    "# Авторизумеся в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "_zWNj9dLKSB5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zWNj9dLKSB5",
    "outputId": "30c6a2a5-ad57-41c5-f2b7-c4672b3d0ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 23.6 MB/s \n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 60.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 65.7 MB/s \n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 71.1 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 72.9 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 74.7 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 67.6 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 66.2 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 72.1 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 71.7 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 71.8 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 67.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f566b282a8e3ad3f70f35be264af918c09f4a07f5a86541dd6e6c2676c90d612\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d7196a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "1d7196a0",
    "outputId": "bcf34275-d896-4044-9cc1-9d150e227e2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a2411e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "2a2411e1",
    "outputId": "a01a9f65-d528-4b7f-9808-647897ea7053"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyashka\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20220929_073309-d5zhb45i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/myashka/QA%20specific%20domain/runs/d5zhb45i\" target=\"_blank\">easy-durian-2</a></strong> to <a href=\"https://wandb.ai/myashka/QA%20specific%20domain\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/myashka/QA%20specific%20domain/runs/d5zhb45i?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efc291b8cd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"QA specific domain\", entity=\"myashka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0f1e532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f1e532",
    "outputId": "c2910f3b-b875-4b94-995f-428cf6814f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_LOG_MODEL=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5bc0fe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5bc0fe4",
    "outputId": "ddba3c82-ed0c-4db0-cab8-048e2af198fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5a327",
   "metadata": {
    "id": "e9b5a327"
   },
   "source": [
    "# Определим Trainer и запустим обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80817459",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80817459",
    "outputId": "6ce1e909-b086-4b3b-d5c0-e36d68d737dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"gpt_neo_first_run\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=lambda data: {\n",
    "        \"input_ids\": torch.stack([f[0] for f in data]),\n",
    "        \"attention_mask\": torch.stack([f[1] for f in data]),\n",
    "        \"labels\": torch.stack([f[0] for f in data]),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1TkEZlHDMAhp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "1TkEZlHDMAhp",
    "outputId": "acacbbae-e35f-4b9f-9fdd-5a8e88b7d198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1315\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3290\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m         )\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m                 if (\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    627\u001b[0m                     \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m                 )\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 14.76 GiB total capacity; 13.61 GiB already allocated; 61.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f308c",
   "metadata": {
    "id": "3b5f308c"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd0c2d",
   "metadata": {
    "id": "19bd0c2d"
   },
   "source": [
    "### #TODO\n",
    "1. Дописать evaluate\n",
    "2. Прогнать через модель dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75d6c1f9",
   "metadata": {
    "id": "75d6c1f9"
   },
   "outputs": [],
   "source": [
    "text = \"Question: What is the weather like?\\nAnswer: Today is a good day!\\n\"\n",
    "question = \"Question: What is the weather like?\\nAnswer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764af776",
   "metadata": {
    "id": "764af776"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    original_text, predicted_text, original_answer, predicted_answer = [], [], [], []\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "\n",
    "    for encoded_ids, _, _, answer, question in tqdm(test_dataset):\n",
    "\n",
    "        original_text.append(f\"Question: {question}\\nAnswer: {answer}\")\n",
    "        original_answer.append(f\"Answer: {answer}\")\n",
    "\n",
    "        question_len = len(f\"Question: {question}\\nAnswer: \")\n",
    "\n",
    "        text_to_answer = f\"<|startoftext|>Question: {question}\\nAnswer:\"\n",
    "\n",
    "        enc_text_to_answer = tokenizer(text_to_answer, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        generated_output = model.generate(\n",
    "            enc_text_to_answer,\n",
    "            do_sample=False,\n",
    "            top_k=50,\n",
    "            max_length=max_length,\n",
    "            top_p=0.90,\n",
    "            temperature=0,\n",
    "            num_return_sequences=0,\n",
    "        )\n",
    "        # возвращается с pad\n",
    "        generated_q_a = tokenizer.batch_decode(\n",
    "            generated_output[0], skip_special_rokens=True\n",
    "        )\n",
    "        generated_a = generated_q_a[question_len:]\n",
    "\n",
    "        predicted_text.append(generated_q_a)\n",
    "        predicted_answer.append(generated_a)\n",
    "\n",
    "        bleu_scores.append(\n",
    "            bleu.compute(predictions=generated_a, references=answer)[\"bleu\"]\n",
    "        )\n",
    "        rouge_scores.append(\n",
    "            rouge.compute(predictions=[generated_a], references=[answer])[\n",
    "                \"rouge1\"\n",
    "            ].mid.fmeasure\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        original_text,\n",
    "        predicted_text,\n",
    "        original_answer,\n",
    "        predicted_answer,\n",
    "        np.mean(bleu_scores),\n",
    "        np.mean(rouge_scores),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76999665",
   "metadata": {
    "id": "76999665"
   },
   "outputs": [],
   "source": [
    "generated_q_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c0bd9",
   "metadata": {
    "id": "5a5c0bd9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "069aab3228b4488cbbac21db25394558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dea1008a50f482eab194de2b6870ea9",
      "placeholder": "​",
      "style": "IPY_MODEL_e85337d1de3b48fcb5ffebceb5877614",
      "value": ""
     }
    },
    "3146863510284503bd73c85038250799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa7afef9e81d4040b6ce820d1e3eda74",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e3758c7ae1384eae90279b18e46bbc3e",
      "value": 0
     }
    },
    "481af281fe894850a446b5810f092576": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0b533a5ed8b4c069710a7f7a0b5e83d",
      "placeholder": "​",
      "style": "IPY_MODEL_efff7ee2f2d5466e95000fe4c489d787",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "7dea1008a50f482eab194de2b6870ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa7afef9e81d4040b6ce820d1e3eda74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "b0b533a5ed8b4c069710a7f7a0b5e83d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd897e9ff98c454ba0df61dcdee7d67d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3758c7ae1384eae90279b18e46bbc3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6a80162bdf140b1808a882cdddbe9fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_069aab3228b4488cbbac21db25394558",
       "IPY_MODEL_3146863510284503bd73c85038250799",
       "IPY_MODEL_481af281fe894850a446b5810f092576"
      ],
      "layout": "IPY_MODEL_cd897e9ff98c454ba0df61dcdee7d67d"
     }
    },
    "e85337d1de3b48fcb5ffebceb5877614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efff7ee2f2d5466e95000fe4c489d787": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
