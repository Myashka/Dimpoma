{"_type": "table", "column_types": {"params": {"type_map": {"bert_f1": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bert_precision": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bert_recall": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bleu_score": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "generated_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "len_of_generated_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "original_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "question": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "rouge_score": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}}}, "wb_type": "typedDict"}, "columns": ["question", "generated_answer", "original_answer", "bert_precision", "bert_recall", "bert_f1", "rouge_score", "bleu_score", "len_of_generated_answer"], "data": [["I'm a complete newbie to Python (and programming itself for that matter) and so i bought a Raspberry Pi. I also bought a magazine to help me get a grasp of the basics. Now, I've downloaded Geany and I am using the Terminal tab of that for very simple commands. In the magazine, the first variable you come across should happen like this:\n\n<<< a=2\n<<< print(a*3)\n6\n\nHowever, when I write this, it actually ends up as the following:\n\n<<< a=2\n<<< print(a*3)\nbash:syntax error near unexpected token 'a*3'\n\nso, I tried it with spaces and it came back with the following:\n\n<<< a=2\n<<< print(a * 3)\nbash: syntax error near unexpected token 'a'\n\nI have also tried putting the same things into the LXTerminal of the Raspberry Pi (I am using the Raspbian OS). Please can someone point out where I am going wrong. Thank you! By the way, where I have put <<< it should be >>>. \n", " The problem is that you are using a space in the print command.\nThe correct syntax is:   print(\"a=%s\" % a) \n\n", "It looks like your errors are coming from bash, so you are not running the python interpreter.  Try entering python on a line on its own before entering your code fragments.\n", 0.8404656052589417, 0.8589053153991699, 0.8495854735374451, 0.15384615384615383, 1.2183324802375697e-231, 110], ["While writing an application parsing command line arguments I would like to run it with various parameters.\nI don't want to create a Run Configuration for every possible command line argument that I want my script to test with. Is there a way in PyCharm (and I guess with any JetBrains IDE) to make a Run Configuration that asks for the Script parameters when executed?\nI am currently using PyCharm 3.1 (EAP).\n", "\nYou can use the Run Configurations feature of Pycharm to define a run configuration that will run your script with the given parameters, and then you can add the run configurations to your project. \nTo add a new run config, go to File > Settings > Run configurations. Then, in the new Run configuration, click the Add button. In the dialog that appears, select the script you want the configuration to execute. Click the Save button to save the settings.\n\n", "Currently the only possibility is to use the \"Before launch | Show this page\" option.\n", 0.8255247473716736, 0.8695303201675415, 0.8469563722610474, 0.08695652173913043, 8.629015171116436e-232, 457], ["Here goes a difficult one for the expert Numpyer!\nDoes someone know or can come up with a way to use pure Numpy arrays and functions to draw as well as fill colored polygons on top of a numpy array grid? \nThis I think would take two steps:\n\nThe ability to fill the grid array with color values so that the\npolygon filling could be written in one color, and the outline in\nanother one. What would be ideal and fastest for such a system, eg\ntuples of rgb values, color name strings, etc?\nThe ability to draw and fill the inside of a polygon based on an\narray of its pixel coordinates. Drawing the outline of it I think\nshould be pretty easy by just using the coordinates as indexes to\nthe grid array and setting them to the outline color values. More\ndifficult would be to fill the polygon. One way would be to iterate\nthrough all pixel cell coordinates (or better yet only for the\nneighboring cells of each polygon coordinate point) and test if its\ncoordinates are within the polygon pixel coordinates. I know there\nare some simple inside-outside/biggerthan-smallerthan algorithms to\ntest for point in poly using regular Python, but for my purpose this\nwould be too slow and I am wondering if someone has the numpy skills\nto set up such an advanced linked code using only the speed of the\nnumpy builtin functions to return True if a numpy pixel coordinate\nis inside a polygon.\nLastly, if someone would know how to draw a line between two\npixel/grid/array coordinates by filling in all the cells in between\ntwo pixels with a True or color value?\n\nI know this is a tall order, but I was just curious to see if anyone would know. If so, then this could make for a pretty powerful and fast Numpy-based drawing library. In the end I would like to save the grid array as an image which is easy by passing it to PIL. \nI am aware that PIL and Aggdraw can do the polygon drawings and that this has been suggested in many similar posts, but they are not very effective when receiving a numpy array of xy polygon/line coordinates. The assumption here is that my polygon coordinates are already in a numpy array format and I want to avoid the overhead of having to copy them into lists for every drawing (when we're talking about thousands of polygons frequently). So the difference in this post is about how to fill a polygon using pure Numpy.\n", " I have a solution for this problem. It is not as fast as the one I posted above, because it is based only on the NUMPY_PYTHON_VERSION. However, it does work. Here is the code:  \n\n ", "In this case the point to achieve speed is more the used algorithms than the language of choice. Drawing and filling poligons rasterized over a grid of pixel falls into the domain of image processing algorithms and for sure AggDraw is using algorithms from that field.\nThe idea is that if you evaluate for each points a function that considers the vectorial nature of the polygon you need to do a number of operations that is at least O(2*p*A) where:\n\nA = image area\np = average number of points in the perimeter of the polygons.\n\nConversely if you use image processing algorithms for each point you can consider to have a fixed and low number of operations. For example if you consider the FloodFill algorithm it is O(A) and I can say it is less than 30*A (about 30 operations per pixel).\nSo basically since the GADM polygons has many vertex is better to eliminate the vectorial nature of the problem as soon as possible and go with something like this:\n\nconstruct the pixel map of the boundary\nfind one internal pixel\nuse the Floodfill algorithm that will work without any need to know about polygons as vectorial entities\n\nThe same algorithms can for sure be implemented in Numpy but before going for a Numpy graphical lib I would suggest to do the following:\n\nmeasure the time spent in your code for the various steps:\n\nNumpy array to AggDraw lists/sequences conversion\ntime taken by AggDraw\n\ntry to decimate the vertex of the polygons removing the ones that stay in the same pixel based on the current Zoom level and see if an how the times will be reduced\n\n", 0.8446235656738281, 0.786358118057251, 0.8144500851631165, 0.14420062695924762, 1.1720040253306071e-231, 181], ["I am using py.test (version 2.4, on Windows 7) with xdist to run a number of numerical regression and interface tests for a C++ library that provides a Python interface through a C module.\nThe number of tests has grown to ~2,000 over time, but we are running into some memory issues now. Whether using xdist or not, the memory usage of the python process running the tests seems to be ever increasing. \nIn single-process mode we have even seen a few issues of bad allocation errors, whereas with xdist total memory usage may bring down the OS (8 processes, each using >1GB towards the end). \nIs this expected behaviour? Or did somebody else experience the same issue when using py.test for a large number of tests? Is there something I can do in tearDown(Class) to reduce the memory usage over time?\nAt the moment I cannot exclude the possibility of the problem lying somewhere inside the C/C++ code, but when running some long-running program using that code through the Python interface outside of py.test, I do see relatively constant memory usage over time. I also do not see any excessive memory usage when using nose instead of py.test (we are using py.test as we need junit-xml reporting to work with multiple processes)\n", "\nI have not seen this behaviour before. The memory consumption seems constant, and the issue seems not to lie in the code. It seems that the pytest process is using a lot of memory, even though the test suite is running in single process mode. This is not a problem with py itself, as I have seen the exact same behaviour in other tests. However, it is a bug in py, which I will report to py-dev. In the meantime, you can try to use the --no-fork option to the xDist command to avoid the fork.\n\n", "py.test's memory usage will grow with the number of tests.  Each test is collected before they are executed and for each test run a test report is stored in memory, which will be much larger for failures, so that all the information can be reported at the end.  So to some extend this is expected and normal.\nHowever I have no hard numbers and have never closely investigated this.  We did run out of memory on some CI hosts ourselves before but just gave them more memory to solve it instead of investigating.  Currently our CI hosts have 2G of mem and run about 3500 tests in one test run, it would probably work on half of that but might involve more swapping.  Pypy is also a project that manages to run a huge test suite with py.test so this should certainly be possible.\nIf you suspect the C code to leak memory I recommend building a (small) test script which just tests the extension module API (with or without py.test) and invoke that in an infinite loop while gathering memory stats after every loop.  After a few loops the memory should never increase anymore.\n", 0.8600969314575195, 0.8363215327262878, 0.8480425477027893, 0.32996632996632996, 8.96306453293408e-232, 495], ["I need to play .wav files stored on my PC using Python script from Cygwin.\nPlease advice if this is possible? If so please provide pointers etc, to Python script code which can be used from Cygwin. I am working on a 64-bit Windows 7 machine.\nThis is what I have done so far.\n\nDownloaded and installed setup-x86_64.exe from cygwin website.\nInstalled packages as part of Cygwin:  make,gcc,g++,git,ssh,sox,python ver >= 2.7, curl,wget. \n\nPlease advice on how to play .wav files using Python (version >= 2.7) from Cygwin.\n", " have used the following code to read the.wav files from my computer. It works fine. But I want to know if it is a good idea to use Cygnus to do this. Please advise. Thanks.\n\n   import win32com.client\n  from winimport win\nimport os\nfrom win import shell\ndef main():\nclient = win.Dispatch(\"WScript.Shell\")\nshell.Run(\"cmd.com\", shell.SW_HIDE)\nprint client.StdOut.ReadAll()\nos.system(\"wmic audio find \" + os.getenv(\"HOME\") + \"\\\\Desktop\\\\test.wav\")\n\n", "try os.system(\"cat /path/foo.wav > /dev/dsp\")\nYou need to install audio package for Cygwin first.\n", 0.8114315867424011, 0.8459393978118896, 0.8283262252807617, 0.10526315789473682, 9.119354881980023e-232, 446], ["I downloaded the sources for Android NDK from the git repository, I noticed that the sources for perl and python are bundled with the other dependencies: what are this 2 interpreters for ?\nDoes this means that I can build python for Android with the NDK ? Or that if I have a python application I can port it to Android with the NDK ?\n", "e python interpreter is a separate package, and is not part of the Android SDK.\nYou can use the python NDk toolchain to build a Python application for the platform. The toolchains are available at http://developer.android.com/tools/sdk/ndk-build.html\nIf you want to port your application to the android platform, you will need to use a different tool chain.\n\n", "Python and perl are used internally by NDK tools to make the cross-compile environment more friendly. You only need them on the host. NDK can be built for Windows, Mac, or Linux. So the git repository contains all opensource that is required to compile NDK for any of these platforms.\n", 0.8559468984603882, 0.8584697246551514, 0.8572064638137817, 0.2956521739130435, 1.0031438070996198e-231, 359], ["In Django, is there an easy way to test that a model field on an object has already been queried from the database (e.g. an object coming from a foreign-key relationship)?\nI would like to make an assertion like this in one of my tests to ensure that accessing a particular attribute on one of my objects won't trigger an additional database query.\n", "\nYou can use the get_or_create() method to check if a field has been created or not. \nFor example:  \n\n  class MyModel(models.Model):\n  #...\nclass MyOtherModel:\n\n def getOrCreate(self, name):\n\n if self.name == name: # check that the field is already created\n return self\n else: return super(MyOtherModels, self).getOrcreate(name)\n # return MyModel\n def __str__( self ):\n if not self: raise Exception(\"Field '%s' not found\" % self)\n\n return \"MyModel\"\n class Meta: model = MyObject\n...\n\n", "In the particular case of a ForeignKey, you can check the existence of the _FOO_cache attribute. For instance, if your Employee object has a ForeignKey to Company, then if my_employee.company is populated then my_employee._company_cache will exist, so you can do hasattr(my_employee, '_company_cache').\n", 0.7928541302680969, 0.8380897045135498, 0.8148446083068848, 0.19672131147540983, 9.379219774760097e-232, 484], ["I have a python file that will synchronize my MySql Database from my own server to the local server. I want to install it as a windows services every time my local server boot up. Can you help me? I want to add also that can I make a GUI for that services just like an Apache that will display beside the task bar clock? Thank you so much in advance.\n", "\nYes, you can install a Windows service as an application. You can use the Windows Installer to create a service. The Windows installer will create the service and install the application to your computer. To create an installer, right-click on the file you want the installer to run and select \"Run as Administrator\".\nYou can then create your service using the \"Service\" tab. Click on \"Create Service\" and then select the name of your application and click \"OK\". You will then be prompted to select a location for the executable file. Select the location you created and the path to where the program is located. Then click on OK. When you are finished, click the Start button and you will be asked to restart the computer to complete the installation. Once the process is complete, the new service will start automatically. If you would like to stop the services, simply right click your services and choose \"Stop\". To start the applications, just rightclick the icon and it will open a command prompt. Type \"start\" in the command line and press enter. This will run the programs. For more information on installing a program, see the following articles:", "A service is nothing but a process/program that run on regular interval checks and runs accordingly.\nIf you have script already written, then another script,service_script which will do the following\n\nIt should check if the program is required to run ? (Syn is required\nif two parties are not in same state)\nAt what interval you should check, there is a chance that this script is required to\nrun. Say you DB updated every 10 mintues. Then code you script to syn\nwith it. If job is there do it else set it to sleep.\nIf possible make sure your script is optimised, following standards &\nall basic things.\n\nAs for GUI, you store these success/failure details in a Log file. If you want GUI - a small php interface/python simple http will help you set up a interface.\nI have some experience in doing some monitoring scipts & dashboard, but not quiet simmilar to your work. \nGodspeed.\n", 0.828960120677948, 0.8166189193725586, 0.8227431774139404, 0.2633053221288515, 7.488543381221802e-232, 1156], ["i have use funcargs in my tests:\ndef test_name(fooarg1, fooarg2):\nall of them have pytest_funcarg__ factories, which returns request.cached_setup, so all of them have setup/teardown sections.\nsometimes i have a problem with fooarg2 teardown, so i raise exception in here. in this case ignore all the others teardowns(fooarg1.teardown, teardown_module, etc) and just goes to pytest_sessionfinished section. \nis there any option in pytest not to collect exceptions and execute all remaining teardowns functions?\n", " yes, you can use py.test.skip(...).\nExample:   # skip all other teardsown functions\n@pytest\nclass TestName(unittest.TestCase):\n\n  def testName_1(self): \n\n", "Are you using pytest-2.5.1?  pytest-2.5 and in particular issue287 is supposed to have brought support for running all finalizers and re-raising the first failed exception if any. \n", 0.8078719973564148, 0.8212509155273438, 0.814506471157074, 0.14545454545454545, 1.1053001340649414e-231, 155], ["I'm just starting exploring IAM Roles. So far I launched an instance, created an IAM Role. Everything seems to work as expected. Currently I'm using boto (Python sdk). \nWhat I don't understand :\n\nDoes the boto takes care of credential rotation? (For example, imagine I have an instance that should be up for a long time, and it constantly have to upload keys to s3 bucket. In case if credentials are expired, do I need to 'catch' an exception and reconnect? or boto will silently do this for me?)\nIs it possible to manually trigger IAM to change credentials on the Role? (I want to do this, because I want to test above example. Or if there is there an alternative to this testcase? )\n\n", "\nIAM does not take care about credential changes. It is up to you to handle this.\nYou can manually change the credentials of the role. You can do it by using the following command:  \n\n  aws iam update-role-binding --roleName <role_name> --bindingName  <binding_id>\n  --policy-document <policy_document>  # <PolicyDocument> is the policy document\nThe policy_documnet is a JSON file that contains the IamPolicy.  The IAmPolicy is an object that has the  IaamRole and IaaamUser. The  is used to identify the roles and users.\n\n", "The boto library does handle credential rotation.  Or, rather, AWS rotates the credentials and boto automatically picks up the new credentials.  Currently, boto does this by checking the expiration timestamp of the temporary credentials.  If the expiration is within 5 minutes of the current time, it will query the metadata service on the instance for the IAM role credentials.  The service is responsible for rotating the credentials.\nI'm not aware of a way to force the service to rotate the credentials but you could probably force boto to look for updated credentials by manually adjusting the expiration timestamp of the current credentials.\n", 0.8230416774749756, 0.8522192239761353, 0.8373764157295227, 0.3404255319148936, 8.9892472272761e-232, 523], ["Is there a way to backup Python modules? I installed lots of modules. If my system does not work properly, I will lose them all. Is there a way to do this?\n", "\nYes, you can use the following command to create a backup of your Python module: python -m pip install -r requirements.txt.\nThis will create the backup file requirements-<version>.txt, which you will then use to restore your module.\n\n", "If you installed them with pip, you can use pip freeze to list the currently installed modules. Save this to a file and use pip install -r file on a new system to install the modules from the file. \n", 0.8723421096801758, 0.8868055939674377, 0.8795143961906433, 0.37974683544303806, 1.0298978900746947e-231, 235], ["Is there any difference between threading.Lock(), threading.Semaphore() behaviour in usual python thread and greenthread (eventlet)?\n", "\nThe thread.lock() method is used to lock a mutex. The thread is blocked until the lock is released.\nIn eventlet, the thread can be blocked in a loop until a condition is met. This is done by using the event loop. In this case, a semaphores is created and the condition can only be met when the sema is unlocked. \nSo, in the case of thread, it is not necessary to use the Lock() function.\n\n", "There is no difference in the behavior. However, green thread isn't actually a thread since it runs all its tasks in a single OS thread, so the threading.Lock and threading.Semaphore will behave as if it's being locked and unlocked from a single thread. \nThis means if you try to acquire a locked Lock or an zeroed Semaphore when using green threads, then the whole program will block forever (or until the specified timeout). Also, an RLock can only be released from the same thread it's locked, since green threads actually all run on the same thread you will be able to release an RLock from a different green thread. \nIn short, don't use threading locks when using green threads. \n", 0.8660027980804443, 0.8521521091461182, 0.8590216636657715, 0.35820895522388063, 9.750562409185714e-232, 390], ["I'm using the built in python version in OSX, I also installed pip by sudo easy_install pip and secondly I installed numpy by sudo pip install numpy.\nHowever, when I run any python file which uses numpy I get an error message like:\n\n\nImport error: No module named numpy\n\n\nLike numpy isn't installed in system. When I called locate numpy I found out most of outputs tell numpy is installed at: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/Python/numpy\nHow can I get it to work?\n", "\nYou can install it by using pip. You can also install by pip using sudo.\n\n", "Using the built-in python for OS X is not recommended and will likely cause more headaches in the future (assuming it's not behind your current problems).\nAssuming your python is fine, there's still the issue of getting numpy working. In my experience, installing numpy with pip will often run into problems.\nIn addition to CT Zhu's advice, if you just want numpy and python, the Enthought distribution is quite good and free for students. \nAlso getting Homebrew working is a good idea and, because it's quite well supported, is not hard. With homebrew, installing numpy is as easy as brew install numpy -- and it makes installing other packages that also often don't install right with pip (sklearn, scipy, etc) easy too.\n", 0.8773714303970337, 0.8113458156585693, 0.8430678844451904, 0.11267605633802817, 1.2570577736112734e-231, 75], ["I have a class that returns large NumPy arrays. These arrays are cached within the class. I would like the returned arrays to be copy-on-write arrays. If the caller ends up just reading from the array, no copy is ever made. This will case no extra memory will be used. However, the array is \"modifiable\", but does not modify the internal cached arrays.\nMy solution at the moment is to make any cached arrays readonly (a.flags.writeable = False). This means that if the caller of the function may have to make their own copy of the array if they want to modify it. Of course, if the source was not from cache and the array was already writable, then they would duplicate the data unnecessarily.\nSo, optimally I would love something like a.view(flag=copy_on_write). There seems to be a flag for the reverse of this UPDATEIFCOPY which causes a copy to update the original once deallocated.\nThanks!\n", "\nI think you are looking for a dictionary. The following code will do what you want: \nclass MyClass(object):\n   def __init__(self, a, b):  # a and b are the arrays you wish to cache\n# ...\na = np.array(a)\nb = a\nc = MyArray(b)  ## a is a new array\nd = dict(c) # d is the dictionary you created\nprint(d)\n\n", "Copy-on-write is a nice concept, but explicit copying seems to be \"the NumPy philosophy\". So personally I would keep the \"readonly\" solution if it isn't too clumsy.\nBut I admit having written my own copy-on-write wrapper class. I don't try to detect write access to the array. Instead the class has a method \"get_array(readonly)\" returning its (otherwise private) numpy array. The first time you call it with \"readonly=False\" it makes a copy. This is very explicit, easy to read and quickly understood.\nIf your copy-on-write numpy array looks like a classical numpy array, the reader of your code (possibly you in 2 years) may have a hard time.\n", 0.7990697622299194, 0.8165227174758911, 0.8077019453048706, 0.21348314606741572, 1.0312097516520933e-231, 302], ["It is easily verified that depending of the version/operating system, the plots done with Python differ meaningly in its appearance/resolution: how to solve that?\n", "\nThe plots are generated with the help of matplotlib.pyplot. The plots can be generated in different ways.\nFor example, if you want to generate a plot with a given resolution, you can use the following code: \nimport matplolib\nmatplo = matlib_import.matplotlibrc()\nfig, ax = plt.subplots(figsize=(10,10))\nax.plot(range(10), range(100))   # or any other plot\nplt_mat = ax.get_figure()  ## or pl.figure\nprint(pl_fig)\n# or\nfor i in range (10):\n  pl_pl = fig.add_subplot([0,0],projection='3d')\n pl = i\n print(i) \n\n", "I would say, ensure that you're using the same backend, fonts, etc. by having identical .matplotlibrc files, and specify the dpi of your plots in your code.\n", 0.7830837368965149, 0.8554471731185913, 0.8176674842834473, 0.15254237288135594, 8.661083288382197e-232, 509], ["In php, people often call a bootstrap file to set variables used throughout a program. I have a python program that calls methods from different modules. I want those methods from different modules to share some variables. Can I set these variables up in something like a boostrap.py? Or is this not very \"pythonic\" because a module should contain all of the variables it needs?\n", "\nYes, you can set up variables in a Python script.\nThe variables you want to use are called globals. You can use the glob function to create a list of variables that you will use throughout your script, and then you use a dictionary to store the values of those variables: glob.glob(filename)\nYou can also use glob to get a set of all variables, but you have to be careful with the order of your variables because they are not guaranteed to appear in the same order in your code. For example, if you had a variable called foo, then the following code would not work: foo = glob('foo.txt')\nIf you are using Python 3, the above code will work, because the Python interpreter will automatically sort the list before it uses it. If you don't want that, use: import glob\nIn your case, your module will need to have the variable foo declared somewhere, so you could do something along the lines of: module.foo = 'bar'\nThis is a very basic example of how you would set a global variable in Python.\n\n", "The best way would be to create something called a settings.py file, that houses are your shared variables of importance.\nThis approach is followed by the django team for their web framework called django, whcih creates a settings.py file to house all the data that needs to be shared, for example database logins, and static file roots.\n", 0.8292427062988281, 0.847185492515564, 0.8381180763244629, 0.2113821138211382, 7.467408561283578e-232, 992], ["I'm embedding Python in an Objective C application using PyObjC, setting the Python environment up by hand on the ObjC side (i.e. not using py2app).  I'm starting the Python in a separate thread (on the ObjC side) with a call to PyRun_File(), and then one to PyObject_CallFunction() to start it doing something (specifically update some menus, register and handle menu callbacks).  The Python function starts the run loop for the secondary thread and the callback class instance hangs around.  All of this works.  I can pass basic data such as strings to the initial Python function without problem and menu actions work as I'd like.  \nI'd like to provide a pre-instantiated delegate instance to the Python function for ease of configuration (from the point of view of the Objective C developer).  How do I pass an Objective C instance (my delegate) to the Python function?  Is it possible?  Are there bridge functions I'm missing?  Do I need to create a suitably configured PyObject by hand?  What sort of conversion should I do on the Python side to ensure the delegate.methods() are usable from Python and proxying works as it should?  Are there any memory-management issues I should be aware of (on either side of the bridge)?\nI'm using Python 2.7, PyObjC 2.5 and targeting OSX 10.6.  Happy to consider changing any of those if the solution specifically demands it. TIA.\n", "\nThe problem is that the PyCall_Function method is not called when the thread is started. I've tried calling PyThread_start() and Py_Initialize() in the main thread, but that doesn't work either. The PyCFunction_Start() method does work, however. So I think the problem lies in how I set up the environment on my Obj C side. Here's what I have:  (I've also tried setting up a new thread with PyEval_Evals() instead of Pythread_create() but the same problem occurs.)\n#import Pyobjc\nimport sys\nfrom PyQt4 import QtCore, QtGui\nclass MyApp(QObject):\n  def __init__(self):\n\n  # Create a Pyobject to hold the object\nself.object = Pyqlet_Object()\n\n # Set up Pycall_function to call the function\ndef PycFunction(arg): \n\n self.call = QtObject.QCallEventObject(MyApp, self, \"PyCall\", arg)\n self._caller = self\n #self._thread = QThread()\n if self: self = MyThread(None, None, 0, QCoreApplication.instance())\n else: MyApplication().instance().thread(0).start", "The easiest way to ensure problem free usage of the bridge is to ensure that the delegate methods that you use don't use C arrays as arguments or return values and don't use variadic signatures (\"...\"). Futhermore ensure that all pass-by-reference arguments (such as the commonly used \"NSError**\" argument) are marked up with \"in\", \"out\" or \"inout\" to indicate in which direction values are passed. This ensures that the bridge can get all information it needs from the Objective-C runtime.\nThere are two options to pass a preconstructed object to Python code:\n\nCreate a class method that returns the object\nUse the PyObjC API to create the python proxy for the Objective-C object.\n\nThe latter uses an internal PyObjC API (also used by the framework wrappers) and could break in future versions of PyObjC. That said, I don't have active plans to break the solution I describe here.\nFirst ensure that the right version of \"pyobjc-api.h\" and \"pyobjc-compat.h\" are available for the Objective-C compiler.\nUse #include \"pyobjc-api.h\" to make the API available.\nCall \"PyObjC_ImportAPI\" after initialising the Python interpreter, but before you use any other PyObjC function.\nUse \"pyValue = PyObjC_IdToPython(objcValue)\" to create a Python representation for an Objective-C object.\n", 0.808199405670166, 0.8141100406646729, 0.8111439943313599, 0.23369565217391303, 8.508219154116519e-232, 946], ["I have a project with many scripts using Matplotlib. I'd like to build a web interface for this project. \nHow do you place a Bokeh chart within a Chameleon template? I'm using Pyramid and the Deform bootstrap if that matters. \nDoes anyone have a good example out there?\n", "\nI'm not sure if this is the best way to do it, but I've done it like this:  \n\n  1. Create a new project in PyCharm.\n  2. Add a script to the project called \"my_script.py\"\n3. In the \"main.html\" file, create a \"script_template.hmtl\" and add a  \"Bokehy\" template to it. This template will contain the BKE and Chamaeleon charts.\n\n", "You want to use plot.create_html_snippet.  This function returns the code that you want to appear in the HTML, the function also writes out an embed file. \nThis is what an embed snippet looks like \n<script src=\"http://localhost:5006/static/dc0c7cfd-e657-4c79-8150-6a66be4dccb8.embed.js\" bokeh_plottype=\"embeddata\"  bokeh_modelid=\"dc0c7cfd-e657-4c79-8150-6a66be4dccb8\" bokeh_modeltype=\"Plot\" async=\"true\"></script> \nthe following arguments control how the embed file is written out, and where the js code searches for the embed files.\nembed_base_url controls the url path (it can be absolute or relative) that the javascript will search for the embed file in.  \nembed_save_loc controls the directory that python will write out the embed file in.  embed_save_loc isn't necessary when server=True\nstatic_path controls the url path (it can absolute or relative) that the javascript will use to construct URLS for bokeh.js and bokeh.css.  It defaults to http://localhost:5006/static/, but could just as easily point to a CDN\nWhen running the bokeh server, navigate to http://localhost:5006/bokeh/generate_embed/static .  I think this requires you to be running on master because of a bug.\nI hope this helps.\n", 0.8371643424034119, 0.7654179334640503, 0.7996851801872253, 0.22058823529411767, 1.0420496126176955e-231, 327], ["As a newbie... I am having difficulties installing pyPDF2 module.  I have downloaded. Where and how do I install (setup.py) so I can use module in python interpreter?\n", "\nYou can install pypdf2 using pip.\npip install -U py-pdf\nThen you can import it in your python code.\n\n", "Here's how I did it: \nAfter you have downloaded and installed Python (it usually installs under C:\\Python** (** being python version - usually 27)), copy the extracted PyPDF2 contents to C:\\Python** folder, after that enter in command prompt/terminal \"cd C:\\Python27\\python.exe setup.py install\". If you did everything right it should start installing PyPDF2.\n", 0.8605368137359619, 0.8235422968864441, 0.841633141040802, 0.20512820512820515, 1.2554234588576635e-231, 102], ["I'm running a python program that's a fairly intensive test of many possible scenarios using a big-O of n algorithm. It's just brute-forcing it by testing over a billion different possibilities using at least five nested loops. \nAnyway, I'm not concerned with how much time the program takes. It's fine to run in the background for long periods of time, it's just that I can't have it clogging up the CPU. Is there any way in Python (3.3) to devote less CPU to a program in exchange for giving it more time?\nThanks in advance.\n", "\nYou can use the timeit module to measure the execution time of your program. For example, the following code will measure how long it takes to execute the code in a loop:  \n\n  import time\n  def loop(n):\n#  loop is a function that takes a number and returns a list of lists\nfor i in range(1, n):  # n is the number of iterations\n #  the loop body is executed n times\n return [list(i) for i, list in enumerate(list)]\nloop(100000)\ntimeit('loop', 'from __main__ import loop', number=100)  ## 100000 iterations in 1 second\nprint('The time taken to loop 100,000 times is: %s seconds' % time.time() - time(loop))\n## The time took to iterate 100 million times in 100 seconds is 0.00 seconds\n```\nThe above code is just a simple example. You can also use time() to get the current time and time it.sleep() for a few seconds to give your code a chance to sleep.\n\n", "First recommendation is the simpler lower than process priority to absolute minimum.\nIf still not reponsive, you could sprinkle in sleep() calls from from time module to surrender CPU\nOr buy a new computer with 4 cores and just let it run. I do this all the time -- works great.\nADDED\nAdding time.sleep() calls will leave a single cpu system running \"bursty\". Also sleep(0) may be effective in an inner loop as is will yield the cpu, but get rescheduled quickly if nothing else wants to use the cpu. OOPS, forgot to check, you are using Linux -- sleep(0) does nothing. You can call the native sched_yield() API, don't think it is built into Python anywhere.\n", 0.7854684591293335, 0.8120588660240173, 0.7985424399375916, 0.2652329749103943, 8.257464037665603e-232, 853], ["Using Eclipse with the PyDev plugin, if you choose myProject>Properties>PyDev-PYTHONPATH, you then see two tabs: Source Folders and External Libraries.  You can also choose myProject>Properties>Project References and see a widget with a checkable list of other parallel subprojects in your Eclipse/Pydev IDE workspace.  I understand that the values in these widgets configure the PYTHONPATH when you run your project.\nBut the documentation does not seem to say the ordering of the values you specify.  For example, are Project References values always after Source Folders and before External Libraries, in the generated PYTHONPATH?  (That is the ordering I wish, so that I can Python install one of my subprojects, and my main project will find the installed version if I have turned off Project References, but my main project will find the same project from my workspace if I turn on a Project Reference to it, while I am changing and debugging the subproject.)\nSimilarly (recursively) are the External Libraries of a Referenced Project inserted in the PYTHONPATH AFTER the Source Folder of a Referenced Project?  \nIt seems like my PYTHONPATH has site-packages external library directory BEFORE the source folder of my subproject, so Python never finds the development version of my subproject, only the subproject version as installed in site-packages.\nI have tried several times to 'Force restore internal info' and to restart Eclipse.  I suppose I could have made a mistake somewhere outside of Eclipse.\n", "\nThe answer is that you have to specify the order of values. The order is: Project references, Source folders, External libraries. You have two choices:  Specify the Project reference first, then the external libraries, or Specifiy the project references first and then External library. I prefer the second option.\n\n", "The PYTHONPATH in PyDev is computed in the following order:\n\nSource folders of the project have highest priority (since this is the code you're expecting to be editing).\nExternal source folders of the project being used for the launch.\nComputed PYTHONPATH of the dependent projects (again, first source then external).\nPYTHONPATH of the related interpreter selected.\n\nNote that the final sys.path is actually computed by Python itself (so, it may be a bit different depending on your Python version -- i.e.: it could add things from the current working dir, current module or eggs even if you remove it from what's configured in PyDev -- although for PyDev, modules not added won't be available for code completion and would be present as errors when searched for as they won't be indexed), PyDev only changes the PYTHONPATH environment variable to match the order presented above.\nIf you somehow have a different outcome, please report this as a bug... (you can see what will be actually used before running in the launch run configuration > interpreter tab > see resulting command-line).\n", 0.8579024076461792, 0.8041577339172363, 0.8301610946655273, 0.2510822510822511, 1.0269067359059098e-231, 317], ["I have a project that already written in php, and now i am using python in google app engine, actually i want to use the api that google support for python, for example : datastore, blobstore ... and also to save my time to re write the code again in python !\nso, is it possible to run php script in python code ?\n", "s, you can run PHP scripts in Python.\nYou can use Python to access the Google Cloud Storage API. You can also use Google App Engine to store your data in Google Storage. The Python SDK for Google is available here.\n\n", "Those runtimes (Py, PHP, Java. etc.) are isolated from each other and are tightly sandboxed. So when you deploy a Python app, for example, it doesn't have access to the PHP or Java runtime. So, it's not possible to run PHP inside a python sandbox, at least not in the appengine platform.\n", 0.8643044829368591, 0.8466145992279053, 0.8553680777549744, 0.29473684210526313, 1.0832677820940877e-231, 216], ["I have a raspberry pi that is setup to run different videos depending on the key press on a keyboard. If someone accidentally hits two keys at once, it causes the unit to temporarily freeze up. What is the best way and code to limit one key press of keys x,y,z for two seconds?\n", "\nThe best solution is to use a keylogger.\nThere are many free and paid key loggers available. I would recommend the free one from the following link: https://www.keyloggers.com/\nIt is a very simple and easy to install key logger. It will log all the keys pressed on your keyboard and you can use it to find out what keys are pressed. You can also use the log to see what key is pressed when you press a certain key. The log will also show you the time when the last key was pressed and the current time. This will help you to know when to press the next key and when it is time to stop pressing the same key again. \nYou can install the logger on any computer you want. Just download the zip file and unzip it. Then you will have to copy the files to your raspberry. For example, if you have the raspberry on an external hard drive, you would have two files: one for the hard disk and one on raspberry for your logger to work. Copy the file on harddisk and copy it on to raspberry and then you are ready to go.\n\n", "Just record the time when each keypress comes in, and store the last couple. If the time of the next keypress is shorter than your required threshold, just ignore it.\n", 0.8234091997146606, 0.8680105209350586, 0.8451218008995056, 0.12931034482758622, 7.364808786842386e-232, 1011], ["If I learn Jython first, does that mean I will also be learning all of the Python language?  When I say all, I mean all the basic constructs of the language, nothing else.  What will I not learn about Python or CPython, if I start with Jython?  Thanks.\n", "\nJython is a very powerful language. It is very easy to learn and very flexible. You can use it to write a lot of code. However, it is not a language that you will use for a long time. If you want to use JPython for long term, you should learn Python first.\n\n", "There is no drawback in learning Jython - \nit is a conformant implementation of Python 2's syntax - and the differences to Python3 are just the one you will find documented everywhere.\nI don't know where jython stands in terms of implementation of Python's stdlib - but I believe it has most of Python's 2.7 stdlib in place and working - some modules won't work, like \"ctypes\" for example. But as far as the language constructs go, you will be fine.\n(IMO it is a good tool, not only for what you want, but a nice tool for exploring Java's libraries themselves in an interactive way, since you can use any Java class from the jython interactive shell)\nAs for the comments talking about unavailable modules: those are 3rd party modules installable on CPython. You certainly don't need them to get the language constructs, like you want. It is a trade off: you loose a lot of the Python ecosystem, but you can use the Java ecosystem in its place. And certainly, when starting a new project, you can just use normal CPython with whatever modules you need: the language is the same.\n", 0.8742971420288086, 0.8226817846298218, 0.8477044701576233, 0.256, 1.0628297349174446e-231, 259], ["I rather like Python's syntactic sugar; and standard library functions.\nHowever the one feature which I dislike; is implicit typing.\nIs there a distribution of Python with explicit typing; which is still compatible with e.g.: packages on PyPi?\n[I was looking into RPython]\n", "\nI don't think there is a \"standard\" distribution. There are a few distributions which are based on the standard libraries, but they are not compatible. For example, the Python standard distribution is based around the CPython standard, which has a lot of compatibility issues. The PyPI distribution has the same issues, and is not a standard. I don\u2019t think it is possible to have a \u201cstandard\u201d distribution, because the libraries are so different. If you want to use a library, you have to install it yourself. You can\u2018t just download it and use it. It is very difficult to find a package which works with all the different libraries. So, I think the best way to go is to stick to the official Python distribution and not to try to make your own.\n\n", "The short answer is no.  What you are asking for is deeply built into Python, and can't be changed without changing the language so drastically that is wouldn't be Python.\nI'm assuming you don't like variables that are re-typed when re-assigned to? You might consider other ways to check for this if this is a problem with your code.\n", 0.8346065282821655, 0.830021858215332, 0.8323078155517578, 0.27860696517412936, 8.220028958947207e-232, 748], ["I rather like Python's syntactic sugar; and standard library functions.\nHowever the one feature which I dislike; is implicit typing.\nIs there a distribution of Python with explicit typing; which is still compatible with e.g.: packages on PyPi?\n[I was looking into RPython]\n", "\nI don't think there is a \"standard\" distribution. There are a few distributions which are based on the standard libraries, but they are not compatible. For example, the Python standard distribution is based around the CPython standard, which has a lot of compatibility issues. The PyPI distribution has the same issues, and is not a standard. I don\u2019t think it is possible to have a \u201cstandard\u201d distribution, because the libraries are so different. If you want to use a library, you have to install it yourself. You can\u2018t just download it and use it. It is very difficult to find a package which works with all the different libraries. So, I think the best way to go is to stick to the official Python distribution and not to try to make your own.\n\n", "No You can not have cake and eat cake.\nPython is great because its dynamically typed! Period. (That's why it have such nice standard library too)\nThere is only 2 advantages of statically typed languages 1) speed - when algorithms are right to begin with and 2) compilation errors\nAs for 1)\n\nUse PyPi, \nProfile,\nUse ctype libs for great performance. \n\nIts typical to have only 10% or less code that is performance critical. All that other 90%? Enjoy advantages of dynamic typing.\nAs for 2)\n\nUse Classes (And contracts)\nUse Unit Testing\nUse refactoring\nUse good code editor\n\nIts typical to have data NOT FITTING into standard data types, which are too strict or too loose in what they allow to be stored in them. Make sure that You validate Your data on Your own. \nUnit Testing is must have for algorithm testing, which no compiler can do for You, and should catch any problems arising from wrong data types (and unlike compiler they are as fine grained as you need them to be)\nRefactoring solves all those issues when You are not sure if given changes wont break Your code (and again, strongly typed data can not guarantee that either).\nAnd good code editor can solve so many problems... Use Sublime Text for a while. And then You will know what I mean.\n(To be sure, I do not give You answer You want to have. But rather I question Your needs, especially those that You did not included in Your question)\n", 0.8358310461044312, 0.794316828250885, 0.8145453333854675, 0.2923076923076923, 8.533133678910327e-232, 748], ["I'm having a hard time wrapping my head around Python threading, especially since the documentation explicitly tells you to RTFS at some points, instead of kindly including the relevant info. I'll admit I don't feel qualified to read the threading module. I've seen lots of dirt-simple examples, but they all use global variables, which is offensive and makes me wonder if anyone really knows when or where it's required to use them as opposed to just convenient.\nIn particular, I'd like to know:\n\nIn threading.Thread(target=x), is x shared or private? Does each thread have its own stack, or are all threads using the same context simultaneously?\nWhat is the preferred way to pass mutable variables to threads? Immutable ones are obviously through Thread(args=[],kwargs={}) and that's what all the examples cover. If it's global, I'll have to hold my nose and use it, but it seems like there has to be a better way. I suppose I could wrap everything in a class and just pass the instance in, but it'd be nice to point at regular variables, too.\nWhen do I need threading.local()? In the x above?\nDo I have to subclass Thread to update data, as many examples show?\n\nI'm used to Win32 threads and pthreads, where it's explicitly laid out in docs what is and isn't shared with different uses of threads. Those are pretty low-level, and I'd like to avoid _thread if possible to be pythonic.\nI'm not sure if it's relevant, but I'm trying to thread OpenMP-style to get the hang of it - make a for loop run concurrently using a queue and some threads. It was easy with globals and locks, but now I'd like to nail down scopes for better lock use.\n", " The documentation is pretty clear about what's shared and what isn\u2019t. The only thing that\u2018s not clear is whether the variables are shared across threads or not. In general, it\u201a\u00c4\u00f4s best to think of variables as global to the scope in which they are used. For example, if you have a global variable x, then you can\u201b\u00c7\u00f4t use x in multiple threads without sharing it. However, you could use a local variable, such as x_local, to share it across multiple thread. This is because the local variables have the ability to change the value of the global one. So, for example: x = x + 1 is a good way of sharing a variable across different threads, because it doesn\u201ft change x. But, x += 1 would be better if x was shared. You can also use the lock keyword to make sure that the variable is only modified by one thread at a time. Here\u2021 is an example of using", "\nIn threading.Thread(target=x), is x shared or private?\n\nIt is private. Each thread has its own private invocation of x.\nThis is similar to recursion, for example (regardless of multithreading). If x calls itself, each invocation of x gets its own \"private\" frame, with its own private local variables.\n\nWhat is the preferred way to pass mutable variables to threads? Do I have to subclass Thread to update data?\n\nI view the target argument as a quick shortcut, good for quick experiments, but not much else.  Using it where it ought not be used leads to all the limitations you describe in your question (and the hacks you describe in the possible solutions you contemplate).\nMost of the time, you'd want to subclass threading.Thread.  The code creating/managing the threads would pass all mutable shared objects to your thread-classes' __init__, and they should keep those objects as their attributes, and access them when running (within their run method).\n\nWhen do I need threading.local()?\n\nYou rarely do, so you probably don't.\n\nI'd like to avoid _thread if possible to be pythonic\n\nWithout a doubt, avoid it.\n", 0.8319032192230225, 0.8164793848991394, 0.8241190910339355, 0.41899441340782123, 8.029654430073048e-232, 848], ["I was wondering if Python had a limit on the length of a dictionary key.\nFor clarification, I'm not talking about the number of keys, but the length of each individual key.  I'm going to be building my dictionaries based on dynamic values (after validation), but I'm not sure if I should be taking length into account in this case.\n", "\nThe length is not a problem. The length will be the same for all keys. If you want to add a new key, you can just add it to the end of the dictionary.\n\n", "As I know there's no limit but consider that more the key is long, the more the time to create/access the keys\n", 0.8774895668029785, 0.8598871231079102, 0.8685991764068604, 0.27586206896551724, 1.1344031528025553e-231, 153], ["Can we run scikit-learn models on Pandas DataFrames or do we need to convert DataFrames into NumPy arrays?\n", "\nYes, you can run the scipy.sparse.csr_matrix and scicosparse models directly on pandas dataframes.\nHowever, if you want to use scilab, then you need the pandacyaml package. You can install it with: pip install pandaxyaml.\n\n", "Pandas DataFrames are very good at acting like Numpy arrays when they need to. If in doubt, you can always use the values attribute to get a Numpy representation (df.values will give you a Numpy array of the values in DataFrame df.\n", 0.8413839936256409, 0.8510358333587646, 0.8461824655532837, 0.275, 1.063382860129351e-231, 224], ["I have recently installed Python 2.7.6 in windows 7. When I tried to open the IDLE, it gave fatal errors.\nThere are two error message popping up subsequently showing error messages:\n\n\n\"IDLE Subprocess Error\"\n\"Socket Error:No connection could be made because the target machine\n  actively refused it\"\n\n\n\"Subprocess Startup Error\"\n\"IDLE's subprocess didn't make connection. Either IDLE can't start a\n  subprocess or personal firewall software is blocking the connection.\"\n\n\nthen nothing happens.\nI figured out that this error occurs only when Proxifier is on. No issue with firewall. I exited Proxifier and the python Shell was working fine. Then I tried to open Proxifier after opening shell. Then the shell immediately stopped compiling and running python code and hanged up. See if someone can help to get the shell work well while Proxifier is on, or any other suggestions.\n", "\nThe problem is that ProXifier has a problem with the Python Shell. It is not able to start the sub-process. The reason is because Pro Xifier uses the same port as the Shell, and it is blocked by the firewall of the computer. So, the problem occurs when the Pro-Xifer is running. If you want to run the code in the IDE, you need to use the port of your IDE. For example, if you are using IDEA, then you can use port 8080.\n\n", "I encountered the same problem. After trying a lot of things I arrived at this solution. The trick is to make a new Proxification Rule in proxifier. Name it anything you prefer, say PythonIdle.\n  In application box add python.exe and pythonw.exe.\n  Set action to Direct.\nHope it will solves your problem! \n", 0.8368570804595947, 0.8423044681549072, 0.8395719528198242, 0.2589928057553957, 9.55456075400679e-232, 423], ["I've just downloaded PyCharm community edition for my Mac. It seems to work great but for some reason project type -dropdown is missing in Create project dialog. I'm newbie to PyCharm (and Python overall) so I don't know if there is some obvious reason for this. \nI was able to create projects however - even in virtualenv. But they are always empty projects. \nAny ideas?\n", "\nThe reason is that Pycharm is using the default project template.\nYou can change the template by going to Preferences -> PyDev -> Project Templates. You can also change it by opening the PyProjects folder in the project explorer and changing the file name.\n\n", "This may be due to Community Edition which doesn't provide all functionality. Download the professional edition of pycharm. The professional edition is free to use for one month.\n", 0.8481364846229553, 0.8627827763557434, 0.8553969264030457, 0.1408450704225352, 1.0051623035972895e-231, 259], ["When I check the shape of an array using numpy.shape(), I sometimes get (length,1) and sometimes (length,).  It looks like the difference is a column vs. row vector... but It doesn't seem like that changes anything about the array itself [except some functions complain when I pass an array with shape (length,1)].\nWhat is the difference between these two?\nWhy isn't the shape just, (length)?\n", "\nThe difference in the two cases is that the first case is an ndarray, and the second is just a list.  The first is not a npy array, but a python list, so it is treated as a Python list in numpython.\nIn the nnpy.ndarray case, the length is determined by the number of dimensions, not the size of the data.\n\n", "In Python, (length,) is a tuple, with one 1 item.  (length) is just parenthesis around a number.\nIn numpy, an array can have any number of dimensions, 0, 1, 2, etc.  You are asking about the difference between 1 and 2 dimensional objects.  (length,1) is a 2 item tuple, giving you the dimensions of a 2d array.\nIf you are used to working with MATLAB, you might be confused by the fact that there, all arrays are 2 dimensional or larger.\n", 0.8691316246986389, 0.8545185327529907, 0.8617631196975708, 0.34722222222222227, 1.001182471642428e-231, 307], ["how do i use python for a Spark program in eclipse?\nI've installed PyDev plugin in eclipse and installed Python on the system but how do i use PySpark.\n", "\nYou can use the PyCharm IDE to run Spark programs. You can also use a standalone version of Pycharm. To do this, you need to install PyCHARM_PYTHON_EXECUTABLE. This is a file that contains the Python interpreter and the necessary libraries. The file is located in the same directory as the Spark project. If you are using PyDEV, then you can simply open PyDevelop and select the project in PyProject. Then, in your Pyproject, select PySPARK_PROJECT.py. In the Project Properties, set the path to the file Pyspark_project.pyd. Now, when you run the program, PyProgram will be executed.\n\n", "I started a new Python project in PyDev, then went into Project -> Properties -> PyDev - PYTHONPATH -> External libraries. I added a \"source path\" entry for \n\n/path/to/spark/spark-0.9.1/python\n\nThis allowed PyDev to see all Spark-related code and provide auto complete, etc.\nHope this helps.\n", 0.8477352857589722, 0.8366148471832275, 0.8421383500099182, 0.25165562913907286, 9.189559570867237e-232, 587], ["First of all, I was not studying math in English language, so I may use wrong words in my text.\nFloat numbers can be finite(42.36) and infinite (42.363636...)\nIn C/C++ numbers are stored at base 2. Our minds operate floats at base 10. \nThe problem is -\nmany (a lot, actually) of float numbers with base 10, that are finite, have no exact finite representation in base 2, and vice-versa.\nThis doesn't mean anything most of the time. The last digit of double may be off by 1 bit - not a problem.\nA problem arises when we compute two floats that are actually integers. 99.0/3.0 on C++ can result in 33.0 as well as 32.9999...99. And if you convert it to integer then - you are in for a surprise. I always add a special value (2*smallest value for given type and architecture) before rounding up in C for this reason. Should I do it in Python or not?\nI have run some tests in Python and it seems float division always results as expected. But some tests are not enough because the problem is architecture-dependent. Do somebody know for sure if it is taken care of, and on what level - in float type itself or only in rounding up and shortening functions?\nP.S. And if somebody can clarify the same thing for Haskell, which I am only starting with - it would be great.\nUPDATE \nFolks pointed out to an official document stating there is uncertainty in floating point arithmetic. The remaining question is - do math functions like ceil take care of them or should I do it on my own? This must be pointed out to beginner users every time we speak of these functions, because otherwise they will all stumble on that problem. \n", "\nfloat division is not taken into account by ceils. It is a matter of rounding. If you want to know the exact value of a float, you should use ceill. You can use the ceildiv function to get the decimal representation of your float. This is the function that is used by the standard library. For example, the following code will give you the value 99:  \n\n  float f = 99;\n  ceilt = ceilediv(f);\nprint(ceil(99));\n# 99\nceill(float(100))\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143", "The format C and C++ use for representing float and double is standardized (IEEE 754), and the problems you describe are inherent in that representation. Since Python is implemented in C, its floating point types are prone to the same rounding problems. \nHaskell's Float and Double are a somewhat higher level abstraction, but since most (all?) modern CPUs use IEEE754 for floating point calculations, you most probably will have that kind of rounding errors there as well. \nIn other words: Only languages/libraries which choose to not base their floating point types on the underlying architecture might be able to circumvent the IEEE754 rounding problems to a certain degree, but since the underlying hardware does not support other representations directly, there has to be a performance penalty. Therefore, probably most languages will stick to the standard, not least because its limitations are well known.\n", 0.7516165971755981, 0.8134632110595703, 0.781317949295044, 0.20074349442379183, 8.912817872294283e-232, 611], ["I am pretty new to Python and I have been pretty annoyed with this problem. I am not sure if this matters, but I run my .py file with Python 2.7.6 with python installed on my computer, not using it on any online thing or other program. Every time I come across an error, my program works fine until it comes to the error, but the window disappears right before I can possibly read whatever the error said it was... Anyways, I haven't been able to find out what is wrong with my programming, and I am tired of guessing and guessing what is wrong. How can I extend the time so I can read the error message? Or something like that? Thanks\n", "You can use the following code to extend your time:", "You should run it from command window/terminal instead of double clicking on the file.\n", 0.899285078048706, 0.8555206060409546, 0.8768571019172668, 0.16, 1.423325334883746e-231, 51], ["I'm using xlwt to create tables in excel. In excel there is a feature format as table which makes the table have an automatic filters for each column. Is there a way to do it using python?  \n", " Yes, you can use the xlsxwriter module to write the data to excel, and then use xltable to format the excel file.\nHere is an example of how to use it:\nimport xsx\nfrom xwt import Workbook\nwb = xwb.Workbook()\nws = wb.add_sheet('Sheet1')\nfor i in range(1, 10):\n  ws.write(i)\nwb = Workbooks.open('test.xls')  # open the workbook in xw\nsheet = bw.get_active_worksheet()  ## get the active sheet\nprint(sheet.name)  // print the sheet name\nxl = XlTable(wb, sheet) # create a table from the wx file\ntable = table.create_table(0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10) ## create the first row\nrow = 0  ### the row number\ncol = 1  ##### the column number!!!\ncell = cell.value \n\n", "OK, after searching the web, I realized that with xlwt it's not possible to do it, but with XlsxWriter it's possible and very easy and convenient.\n", 0.767065167427063, 0.865624189376831, 0.8133698105812073, 0.07894736842105263, 8.258847545134382e-232, 663], ["I'm trying to find a way, using built-in functions, to list every way to organize N balls in M slots. The balls can stack in the slots. For example:\n\nN = 2, M = 3 -> {|0|1|1|, |1|0|1|, |1|1|0|, |2|0|0|, |0|2|0|, |0|0|2|}\n\nitertools.permutations() is part of the puzzle, but how can you go through all possible stacks of balls that preserves N?\n", " You can use the following code: \nfrom itertool import permutations\ndef perm_n(n):\n   return [permutation(i,j) for i in range(1,n+1)\nfor j in perm(range(0,1))]\nprint(perm_2(2))\n# {0: 1, 1: 0, 2: 2}\n\n", "To find all assignments of N balls to M slots:\n\nif N is 0 then\n\nleave all M slots empty\n\notherwise, if M is 1, then\n\nput all N balls to the only slot\n\notherwise\n\nFor each i in 0 .. N\n\nput i balls in M-th slot, and\nfind all assignments of remaining N-i balls to remaining M-1 slots\n\n\n\n", 0.7838621139526367, 0.7948729991912842, 0.7893291711807251, 0.25, 1.0736132553884534e-231, 199], ["I need to ask a question on how to set a development environment for Kivy under Windows 7. Normally i work with Kivy on Linux, but i need to develop an application for a client who uses Windows. From the Kivy documents, there are two alternatives:\n1- Download and Unzip the file containing the Kivy environment plus the Python interpreter with it. I have concerns here. Will this damage my existing Python environment (2.7)? if not is it sand boxed well? Plus if i need to add other third party libraries (ex : pyodbc to run on a Kivy application on a PC) where shall they be installed ?\n2- set up Kivy for existing Python environment. Another concern here : is the \"unofficial\" windows installer a good way to get Kivy running under Windows? and same concerns as above for the Python environment.\nThank you in advance.   \n", " I would recommend you to use the official installer. It is a very good installer and it is very easy to install. You can download it from here: http://www.kivymobile.org/download.html\nIf you are using the unofficial installer, you can use it to create a virtual environment and then install the python interpreter. The official windows installers are not very user friendly. They are very hard to understand and use. If you want to learn how the windows installation works, I recommend this tutorial: https://docs.python.net/3.6/tutorial/windows.windows-installer.txt\nI hope this helps.\n\n", "I am using kivy on windows 7 . You can use option 1 . it wont damage your current python 2.7 because you can just change the path of environment to the python interpreter which comes with kivy . \nin case you need to turn back to your older python installed just change the environment variables .\nIno order to add third party libraries , most of them are already installed in kivy . for others you can find them on kivy.org :)\nIf you need to use for example pyQt4 or similar library you need to use different interpreter . I am also doing same stuff .In my case , I use pycharm and keep different configuration (i.e. python interpreter) for different programs .   \n", 0.8460544347763062, 0.8257633447647095, 0.8357858061790466, 0.2909090909090909, 8.654855344092648e-232, 589], ["I'm looking at using Kivy to create a program that needs to display a window on each monitor, is there a way to accomplish this? I'd also prefer not to have a single window spanning across.\nIf not, is there another (good looking, windows/linux) GUI toolkit that can accomplish this?\n", "\nI'm not sure if this is possible, but you could try using a separate window manager for each screen. I don't know if that's possible with Kvylib, though. You could also try to use a different windowing system for the two screens, like Xinerama.\n\n", "You can have two separate windows running two separate kivy apps controlling/communicating with each other via osc/twisted/... However one \"App\" instance is only limited to one App window for that process. It can launch another process(subprocess.popen) which has a new window though.\n", 0.8529826402664185, 0.8369569778442383, 0.8448938131332397, 0.25806451612903225, 1.0475506394233786e-231, 247], ["I have a class where I want to get the object back as a dictionary, so I implemented this in the __dict__().  Is this correct?\nI figured once I did that, I could then use the dict (custom object), and get back the object as a dictionary, but that does not work.\nShould you overload __dict__()?  How can you make it so a custom object can be converted to a dictionary using dict()?\n", "\nYou can use __getitem__(), __setitem_copy__, and __delitem_.  The __init__ method is not required.  You can also use a dict comprehension to create a new dictionary.\n\n", "No. __dict__ is a method used for introspection - it returns object attributes. What you want is a brand new method, call it as_dict, for example - that's the convention. The thing to understand here is that dict objects don't need to be necessarily created with dict constructor.\n", 0.8692061901092529, 0.8644391298294067, 0.8668161034584045, 0.24000000000000002, 1.1200407237786664e-231, 168], ["For example if I have __add__ and __radd__ defined in two classes and I sum the two objects which definition of the operation will python use?\n", " __sum__\n__r__ is the same as __l.__add_self__.\nSo, if you have two instances of class A, and you define __mul__, __rmul_assign__ or __rl_add, then python will use the __sums__ of those two.\n\n", "For an expression lhs + rhs, Python will first try lhs.__add__(rhs), then rhs.__radd__(lhs).\n", 0.858252763748169, 0.8664499521255493, 0.8623319268226624, 0.15384615384615383, 1.0350003944289303e-231, 192], ["I know Python is the standard scripting language for use inside Blender, but I didn't find a way to create a .blend file with python.\nWhat I want to do is not to use python inside blender, but rather \"use blender (libs?) inside python\".\nMy planned workflow would be the following:\n\nDefine some parameters for my model;\nDefine a \"generative recipe\" to create appropriate Blender objects that will be saved to file;\nCreate a python script to store the parameters and procedures. When the script runs, some .blend file is created in the same folder;\nUse Blender to visualize the model. If model needs to be changed, make changes to the script, run it again, and open it again.\n\n", "he answer is \"yes\". You can use blender to \"run\" python scripts. You just need to make sure that blender is installed and that you have the python interpreter installed. The blender python module is called \"blender.bl_mod\".\n\n", "You can start a new Blender process from any application (a C++, Python app or even command line) and tell the new process to run a script file (written in Python). This script will generate your geometry and then can save the new scene to a blend file.\nTo start a new Blender process and force it to execute a script use:\nblender.exe --background --python \"c:\\path to\\script.py\"\n", 0.8623490333557129, 0.8427873253822327, 0.8524560332298279, 0.27272727272727276, 1.0621993492694801e-231, 225], ["Lets say I have a dictionary that specifies some properties for a package:\nd = {'from': 'Bob', 'to': 'Joe', 'item': 'book', 'weight': '3.5lbs'}\nTo check the validity of a package dictionary, it needs to have a 'from' and 'to' key, and any number of properties, but there must be at least one property. So a dictionary can have either 'item' or 'weight', both, but can't have neither. The property keys could be anything, not limited to 'item' or 'weight'.\nHow would I check dictionaries to make sure they're valid, as in having the 'to', 'from', and at least one other key?\nThe only method I can think of is by obtaining d.keys(), removing the 'from' and 'to' keys, and checking if its empty.\nIs there a better way to go about doing this?\n", " I think you're on the right track. You can use the following code to check if a given dictionary is valid: \nif d['from'] == 'Bobby':\n   print('Bobs are from Bobby')\nelif 'bob' in d: # or d[d['to']]\nprint('bobs from bob') # if you want to use a list of keys\nelse: print(\"Bob is not from Bob\")\n# or if the keys are not in the dictionary\nfor key in sorted(d.items()):\n if key notin d or key == d[\"to\"]:\n print(key)\n else:print(\"not in dictionary\") #if you don't want the key to be in a dict\n #or if it's not a key at all\n for key2 in key: if d and key!= d2: continue\n d = d1\n return d\nI think this is a good way of doing it. \n\n", "if d.keys() has a length of at least 3, and it has a from and to attribute, you're golden.\nMy knowledge of Python isn't the greatest but I imagine it goes something like if len(d.keys) > 2 and d['from'] and d['to']\n", 0.8098182082176208, 0.8544981479644775, 0.8315584063529968, 0.27624309392265195, 8.45547127904398e-232, 625], ["I'm a trainee novice programmer.\nI have / am creating simple programs, generally around screen scraping, data caption (postrgres), various processing methods and now a GUI via wxpython\nI find a lot of these programs overlap - ie use same techniques, and get some very long copied and pasted programs!\nOvertime I improve these techniques and find myself having to backtrack over multiple programs to update these.\n\nHow? Can I? Create a more dynamic, systematic process. \nOne where all programs / procedures / classes are shared?\nHas it got a name?\n\nMy logical thought is that like 'procedures' and 'classes' I have would have smaller low level programs and mid level programs that called upon these - the GUI being the top program! but this would mean passing data to and from! Can Classes and Procedures be separate programs?\nMany thanks\nCameron\n", "\nYes, you can create a new class and a separate procedure. You can also create new procedures and classes. The difference is in the way you use them. A procedure is a low-level function that is called by a higher-order function. It is not a class. Classes are higher order functions that are called from a procedure or a function, but they are not procedures. They are classes, which are used to create higher level functions. For example, a table class is used for creating a data table. This is the same as creating an object, except that the object is created by the class, not by an instance of the table object. So, for example: class Table(object): pass\nProcedure: Create table\nClass: Table\nCreate table: table = Table()\nTable: create table() # this is an example of a high- level function\ntable = table # create an empty table instance\ncreate table(): # call a lower-layer function from the procedure\n# this creates a variable table, that can be used in a later procedure,\nprint table['name'] # print the name of table in this example\nclass: Class\nConstructor: Constructor\nSet name: set name = 'name'\nname = name # set the variable name to the value of name\nset name(value): # assign the new value to name, so that it can later be accessed\nName: Name = Name # access the newly created variable\nget name(): get name() -> 'Name' # get the current value\nput name('value') # put the string value into the named variable, as a string\nPut name(\"value\") # puts the text value in named variables\nGet name().get_name(): Get name.get()-> 'get'# get a value from named\nVariable: Variable\nvariable = variable # variable is now an alias for variable.name\nvar = var # var is also an aliased variable for", "There is a range of techniques you can use : \n\nShared modules - A set of modules with well defined interfaces - functions, classes etc. These modules sit in a folders which every application you use can get to - i.e. the path to the folder is added to PYTHON_PATH environment variable. These interfaces should be engineered so when you add functionality to them they don't break the old applications.\n\n Design patterns - Design your applications with a good design pattern - MVC (Master View Controller) is a useful one for GUI programs. V is your GUI - and exposes only a few methods, which aren't actually dependent on the GUI itself (for instances methods such as display_foo). The Master is your data access functionality - again with well defined interfaces. Controller interfaces between View and Master. There may be other patterns which apply to your application too.\n\n", 0.783685564994812, 0.8113974332809448, 0.7973008155822754, 0.20909090909090908, 7.227101999725997e-232, 1696], ["I am developing a tcp/ip server whose purpose is receive packets from client, parse them, do some computation(on data arriving in packet) and store it in database. Till now, everything was being done by single server application written using twisted python. Now I am across RabbitMQ so my question is, if it is possible and if it will lead to better performance if my twisted server application just receives the packets from clients and pass it another c++ application using RabbitMQ. The c++ application will in turn parse packets, do computation on it etc.. Everything will be done on single server.\n", "\nYes, it's possible. You can use a single Twisted server to receive and process the data from multiple clients. This is called a \"multicast server\".\nYou can also use multiple Twisted servers to process data coming from different clients, but this is not a multicast. It's a broadcast.\n\n", "If your server is does not receive packets often, it will not improve much - only gain some tiny overhead on inter server communication. Still it is a very good design idea, because it scales well and once you finally get many packets you will just add an instance of data processing server.\n", 0.8618344068527222, 0.8536878824234009, 0.8577417731285095, 0.2549019607843137, 9.906079691777987e-232, 286], ["I have a long-running daily cron on OpenShift. It takes a couple hours to run. I've added nohup and I'm running it in the background. It still seems to timeout at the default 5 minutes (It works appropriately for this time). I'm receiving no errors and it works perfectly fine locally. \n\nnohup python ${OPENSHIFT_REPO_DIR}wsgi/manage.py do_something >> \\\n  ${OPENSHIFT_DATA_DIR}do_something_data.log 2> \\\n  ${OPENSHIFT_DATA_DIR}do_something_error.log &\n\nAny suggestions is appreciated.\n", "\nI've found that the problem is that I am using the wrong command. The correct command is:   nohuplog python $OPENDIR/wsgiref/bin/python ${RELEASE_NAME} >> $RELEASENOTIF.txt 2>> $LOGFILE.err &  This will run the script in background and will log the output to the file $logfile. This is the correct way to do it.\n\n", "I'm lazy. Cut and paste :)\nI have been told 5 minutes is the limit for the free accounts. That includes all background processes. I asked a similar question here on SO. \n", 0.7981353998184204, 0.8600974082946777, 0.8279587626457214, 0.1839080459770115, 1.0128758706334661e-231, 314], ["I am supposed to be doing research with this huge Fortran 77 program (which I recently ported to Fortran 90 superficially). It is a very old piece of software used for modeling using finite element methods.\n\nIt is a monstrosity. It is roughly 240,000 lines.\nSince it began its life in Fortran 77, it uses some really dirty hacks for dynamic memory allocation; basically it uses the functions from the C standard library, mixed programming with C and Fortran. I am yet to fully grasp how allocation works. The program is built to be easily extendable by the user, and the user generally needs to allocate some globally accessible arrays for later use. This is done by having an array of memory addresses, which point to the beginning addresses of dynamically allocable arrays. Of course, which element of the address array pointing to which information all depends on conventions which has to be learned by the user, before one can start to really program. There are two address arrays, one for integers, and the other for floating points.\nBy dirty hacks, I mean inconsistent ones. For example an update in the optimization algorithm of the GNU compilers caused the program to exit with random memory leaks.\nThe program is far from elegant. Global variable names are generally short (3-4 characters) and cryptic. Passing data across routines is of course accomplished by using common blocks, which include all program switches, and the aforementioned arrays.\nThe usage of the program is roughly like that of an interactive shell, albeit a stupid one. First, an input file is read by the program itself, then per choice, the user is dropped into a pseudo-shell, in which the user has to type 4 character wide commands, followed by the parameters. The parser then parses the command, and corresponding subroutine is called with the parameters. You would guess that there is a loop structure in this pseudo-parser (a goto bonanza, rather) which wraps the subroutine behavior in a manner more complex than it should be in the 21st century. \nThe format of the input file is the same (commands, then parameters), since it is the same parser. But the syntax is not really consistent (by that, I mean it lacks control structures, and some commands cause the finite state machine to do behavior that contradict with other commands; it lacks definite grammar), time to time causing the end user to discover pitfalls. The user must learn these pitfalls by experience; I did not see them in any documentation of the program. This is a problem that can easily be avoided with python, and it is not even necessary to implement a parser.\n\nWhat I want to do:\n\nPort parts of the program into python, namely the parts that don't have anything to do with numerical computation. This includes \n\ncleaning up and abstracting the API with an OOP approach in python,\ngiving meaningful variable names, \nmigrating dynamic allocation to either numpy or Fortran 90 and losing the C part,\nmigrating non-numerical execution to python, and wrap the numerical objects using f2py, so there is no loss in performance. Have I told that the program is damn fast in its current state? Hopefully porting the calls to numerical subroutines and I/O to python will not slow it down to an impractical level (or will it?).\nMaking use of python's interactive shell as a replacement for the pseudo-shell. This way, there will not be any inconsistencies for the end user. The aforementioned commands will be simply replaced by functions defined in python. This will allow the user to actually access the data. Plus, the user will be able to extend the program without going to deep.\n\n\nWhat I wonder:\n\nIs f2py suitable and up-to this task of wrapping numerous subroutines and common blocks without any confusion? I have only seen single-file examples on the net for f2py; I know that numpy has used it to wrap LAPACK and stuff, but I need reassurance that f2py is a tool consistent enough for this task.\nWhether there are any suggestions on the general strategy that I should follow, or pitfalls I should avoid.\nHow can & should I implement a system in this python-wrapped Fortran 90 environment, so that I will be able to modify (allocate and assign) globally accessible arrays and variables inside fortran routines. This should preferably omit address arrays and I should preferably be able to inject verbal representations into the namespaces. These variables should preferably be accessible inside both python and fortran.\n\nNotes:\n\nI may have been asking for too much, something beyond the boundaries of the possible realm. In this case, please forgive me for I am a beginner with this aspect of programming; and don't hesitate to correct me.\nThe \"program\" I have been talking about is open source but it is commercial and the license does not allow its distribution, so I decided not to mention its name. However, you could deduce it from the 2nd sentence and the description I gave throughout.\n\n", " ", "I'm doing something depressingly similar. Instead of dynamic memory allocation via C we have a single global array with integer indices (also at global scope), but otherwise it's much the same. Weird, inconsistent input file and all. \nI'd advise against trying to rewrite the majority of the program, whether in python or anything else. It's time consuming, unpleasant and largely unnecessary. As an alternative, get the F77 code base to the point whether it compiles cleanly enough that you're willing to trust it, then write an interface routine. \nI now have a big, ugly F77 code base which sits behind an interface. The program requires input as a text file so a large part of the interface's job is to produce that text file. Beyond that, the legacy code is reduced to a single gateway routine which takes a few arguments (including a means of identifying the text file) and returns the answer. If you use the iso_c_binding of Fortran 2003 you can expose the interface in a format C understands, at which point you can link it to whatever you wish. \nAs far as the modern code (mostly optimisation routines) is concerned, the legacy code base is the single subroutine behind the C interface. This is much nicer than trying to modify the old code further and probably a valid strategy for your case as well. \n", 0.0, 0.0, 0.0, 0.0, 1.821831989445342e-231, 1], ["Is it good Python practice to have a function return None most of the time, while exceptionally returning useful values by raising an exception where the values are stored?\nI am a little uneasy with this because exceptions are most often used to signal some kind of problem, with some attributes of the exception giving some details about the problem. Here, I would like the exception to actually mean \"here is an exceptional result of this function\".\nUsing an exception for this is tempting because (1)\u00a0this is only done in exceptional circumstances and (2)\u00a0this is efficient, in Python (more than an if \u2026 is not None:\u2026). On the other hand, the exception itself is not the sign of an error of any sort, just a vehicle for the exceptionally returned values.\nIs there any official recommendation against using exceptions for exceptionally returning values from a function?\nPS: Here is a use case:\n\nAn object method updates the internal state of the object based on new data (it's a finite state automaton).\nAt some point (usually after getting many data points), the method considers that some action must be taken (in my case: some date from the object should be stored in a database, and the object's state is reset to the initial state, where it is ready to get more data).\n\nThus, the sequence of events, for the method, is: get data, update state, get data, update state,\u2026 ah! we reached an special state where information about the object should be stored! reset the objet's state and send the relevant information out; get data, update state, get data, update state,\u2026 Thus, most of the time, the function updates the internal state of the object and does not return anything. Exceptionally, it must send important information about the object.\n", " I think that the best practice is to return a value from your function, but not to raise an Exception. The exception is used for exceptional situations, not for normal situations. If you want to use an except clause, you should use a try-except block. For example: \ndef update_state(obj):\n   try: # do something\nexcept Exception as e: pass\nreturn obj\nupdate_states = update(update(state_update, obj))\nprint(f\"{update} returned {update.return_value}\")\n# prints: {'return value':'returned value'}\nUpdate: The best way to do this in python is with a return statement. This is the most pythonic way. \n\n", "Exceptions can model any exception to the normal flow of the code. Errors are the most common use-case, but anything where the normal return type doesn't really make sense can be seen as an opportunity to raise an exception instead.\nRaising an exception for non-error use-cases is perfectly fine, if it simplifies your flow.\nRemember, the Exception object raised and caught, is itself a value. Attaching information to that is a perfectly acceptable method to communicate with a caller to signal a deviation from 'normal'.\nI'd prefer an exception over a sentinel value like None; it makes the handling explicit, and can actually lead to cleaner handling and better feedback to a developer when they forget to handle the exceptional return value. If your code normally returns a list, but in specific, exceptional circumstances, you return None, and the caller doesn't handle it, then you get weird bugs down the line somewhere. TypeError: 'NoneType' object is not iterable is a lot more cryptic than an explicit unhandled exception.\n", 0.8075077533721924, 0.8273048996925354, 0.817286491394043, 0.32727272727272727, 8.892448265410154e-232, 599], ["I am writing a socket based \"python cmd like\" server module which can support cli interactive functions such as autocompletion or command history, by doing that a simple \"telnet\" or \"nc\" client side may able to connect to server to read/set something on server side.\nafter searching, there are a lot of modules can do \"cmd\" part such like python standard module \"cmd\" or \"ipython\" or even vty simulator, however, I cannot find a module can actually bind to socket directly to detect keystrokes such as \"tab\" key or \"control+c\" client side. Most of them just able to process line(s) read, which not suitable for autocompletion with tab press or command history with up/down press. \nI think this question can be simplify to:\nIs that possible to read socket keystroke input non-blocking, then process this key input value somehow server side - for example ASCII code + 1, then echo back to socket to show in client side?\nThank you for your help.\n", "   I think you can use socket.getpeername() to get the socket name, and then use the getpeercredentials() function to obtain the credentials. Then you could use gethostbyname() or getaddrinfo() functions to find the hostname and port number. You can then do something like this:\n\n  socket = socket(AF_INET, SOCK_STREAM)\n  sock = sock.socket(socket.AF(SOCKADDR_ANY), socket._SockType)\n\n", "What you want is not possible. As you say, you want to write a socket based cmd like server. The server will open a socket and listen for data from the client. Now it is possible to read socket input character by character (which is not the same as non-blocking BTW), but that will not help you. \nIt is up to the client program to decide how and when to send the data. So if the client side program decides to \"eat\" tab and control characters, then you will simply not see them. So if you want to process keystrokes one by one, you will also need a client application.\n", 0.7955430150032043, 0.8328235745429993, 0.81375652551651, 0.2, 9.704732401405964e-232, 385], ["I've developed a little Python Dropbox app but I have no I idea how to hide the app key and app secret. Until I solve this problem I'm not sure how I can ship my app as this seems to be a significant security threat.\nI know it is hard to obfuscate code, most especially Python so I'm not really sure that that is an option.. but what else could I do? I thought about using some form of encryption and/or storing them on a server to be retrieved when the app starts. Is it possible to write the part that deals with the keys in another language that's more easily to obfuscate like C? As I don't know much about encryption, I'm not sure if any of these options are feasible or not. \n", "\nYou can obfuscated code in C, but it's not a good idea. You can use a library like pycrypto to encrypt the code and decrypt it when you need it. But you can't obfuscation the key. The key is the secret that you use to decrypt the encrypted code. If you obfuscating the encryption part, you will have to use the same key to unlock the decryption part. So you have two options:   1) use an encryption library that can encrypt and obfuscates the data, and use that to store the secrets, or  2) encrypt your code with a key that will be used to decode the obfuscations. I would recommend the second option. It's much more secure.\n\n", "To prevent casual misuse of your app secret (like someone who copy/pastes code not realizing they're supposed to create their own app key/secret pair), it's probably worth doing a little obfuscation, but as you point out, that won't prevent a determined individual from obtaining the app secret.\nIn a client-side app (like a mobile or desktop app), there's really nothing you can do to keep your OAuth app secret truly secret. That said, the consensus seems to be that this doesn't really matter. In fact, in OAuth 2, the recommended flow for client-side apps is the \"token\" or \"implicit\" flow, which doesn't use the app secret at all.\n", 0.8398134112358093, 0.8316628932952881, 0.8357182145118713, 0.3012552301255231, 8.587343514644926e-232, 628], ["I know python and want to contribute on OpenSource projects that features python. Anyone can help me where to contribute and how.\nI already googled it and find github and code.google as a good place to contribute but how to start it I don't know.\nSuggest how to get started.\n", "\nFirst of all, you should read the documentation of the project you want contribute to. It will help you to understand the process of contributing to the code and the way to do it. You can find the official documentation here: https://github.com/python/cpython\nIf you are interested in contributing, I suggest you start by reading the README file of your project. This file will give you a clear idea of what the contributors are doing and what they are looking for. If you have any questions, feel free to ask them in the comments. Good luck!\nBest regards,\nKirill\nPS: If your question is not related to python, please don\u2019t hesitate to contact me. I\u2018m happy to help.\n\n", "Not sure if this is an appropriate question for SO - you might get voted down. But ...\nWhenever I have seen this question, the answer is almost always:\n\nfind a project you like / you're interested in\nfind something in that project that you feel you can fix / enhance (have a look through their bug tracker)\nfork the project (github makes this easy)\nmake the change, find out what is appropriate for that project (documentation, unit tests, ...)\nsubmit the change back to the project (github has \"request pull\")\n\nGood luck!\n", 0.8302887678146362, 0.828569769859314, 0.8294283747673035, 0.3113207547169812, 8.713021891541866e-232, 669], ["So in my spare time, I've been developing a piece of network monitoring software that essentially can be installed on a bunch of clients, and the clients report data back to the server(RAM/CPU/Storage/Network usage, and the like). For the administrative console as well as reporting, I've decided to use Django, which has been a learning experience in itself. \nThe Clients report to the Server asynchronously, with whatever data they happen to have(As of right now, it's just received and dumped, not stored in a DB). I need to access this data in Django. I have already created the models to match my needs. However, I don't know how to go about getting the actual data into the django DB safely. \nWhat is the way to go about doing this?  I thought of a few options, but they all had some drawbacks:\n\nGive the Django app a reference to the Server, and just start a thread that continuously checks for new data and writes it to the DB. \nHave the Server access the Django DB directly, and write it's data there. \n\nThe problem with 1 is that im even more tightly coupling the server with the django app, but the upside is that I can use the ORM to write the data nicely. \nThe problem with 2 is that I can't use the ORM to write data, and I'm not sure if it could cause blocking on the DB in some cases. \nIs there some obvious good option I'm missing? I'm sorry if this question is vague. I'm very new to Django, and I don't want to write myself into a corner.\n", " I think the best option is to create a Django model that has a field that is a pointer to a database table. The Django ORMs will then be able to read the table and populate the model with data. This is what I did, in the following way:  \n\n  1. Created a model called \"Server\" with a \"server_id\" field.\n\n 2. Added a method to my Server model to return a list of all the servers. It's a simple list, so I just used a for loop to iterate over it. (I'm using Python 2, if that matters.)\n\n 3. In my Django admin, added a new model field called server_list. Set the default value to be a string, \"\".\n\n 4. Then, created a function in admin.py that would return the list. So, for example, the function would be something like this:\n\n def get_server(self):\n  return self.server.all()\n server = Server.objects.get_all_by_name('server')\n return server\nserver = getattr(Server,'server', None)\nreturn server.list()  # returns a python list\n#", "I chose option 1 when I set up my environment, which does much of the same stuff.\nI have a JSON interface that's used to pass data back to the server.  Since I'm on a well-protected VLAN, this works great.  The biggest benefit, like you say, is the Django ORM.  A simple address call with proper data is all that's needed.  I also think this is the simplest method.\nThe \"blocking on the DB\" issue should be non-existent.  I suppose that it would depend on the DB backend, but really, that's one of the benefits of a DB.  For example, a single-threaded file-based sqlite instance may not work.\nI keep things in Django as much as I can.  This could also help with DB security/integrity, since it's only ever accessed in one place.  If your client accesses the DB directly, you'll need to ship username/password with the Client.\nMy recommendation is to go with 1.  It will make your life easier, with fewer lines of code.  Besides, as long as you code Client properly, it should be easy to modify DB access later on down the road.\n", 0.8174656629562378, 0.8333258628845215, 0.8253195881843567, 0.3378016085790885, 8.350266941495815e-232, 929], ["I have a Kivy-based Python project that I'm trying to build. It uses the NavigationDrawer component from Kivy Garden, through an import:\n\nfrom kivy.garden.navigationdrawer import NavigationDrawer\n\nI have a PyInstaller spec file for it which builds a distributable version. This version works well on my machine, but unfortunately not on other machines. Running the interpreter in the 'dist' version with the -v switch, it appears that when I run the distributable on my machine, the navigationdrawer component is not actually coming from inside my build folder. All the other imports show something like:\n\nimport kivy.graphics.gl_instructions # dynamically loaded from C:\\Users\\me\\myapp\\dist\\RACECA~1\\kivy.graphics.gl_instructions.pyd\n\nBut the navigationdrawer import says:\n\nimport kivy.garden.navigationdrawer\n\"\"\"directory C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\n  C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.pyc matches C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.py\n  import kivy.garden.navigationdrawer # precompiled from C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.pyc\"\"\"\n\nBut noo! I don't want you to import them from c:\\users. I want them to get nicely copied into my dist folder like all the other imports. I've tried adding c:\\users\\me to PyInstaller's pathex, the system PATH and PYTHONPATH without any joy. Anyone have any ideas?\n", " The problem is that the KIVY_GARDEN_VERSION environment variable is set to the version of the Garden component that is installed on your machine. The Garden components are not installed in a standard location, so you need to set the environment to point to a location that contains the components.\nThe Garden documentation says that you can set this environment by running the following command:\n\n   $ export KV_PATH=C:\\Program Files\\Kivymobile\\applications\\navigator\\\nThis will set KVI_HOME to C:/ProgramFiles/KIVYN/appliances/navigators/\nYou can then run your build script with: $ python build_kv.sh\nIf you want to use the same version on multiple machines, you will need\nto set it in your environment variables.\n\n", "You could just copy the navigationdrawer code from C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer to your app directory, call the folder 'navigationdrawer' and replace the import with from navigationdrawer import NavigationDrawer.\nIt's not quite the 'right' way to do it (there's probably some way to make pyinstaller copy it in), but it should work fine.\n", 0.8287988901138306, 0.8456096649169922, 0.8371198773384094, 0.192090395480226, 8.4383399901978e-232, 717], ["I've been trying to learn Twill Scripting on Python and I am using Python 3.4 and Twill 1.8.0. Been reading some posts here and I found it interesting to study. But, I do have a problem installing Twill. I just knew that PRINT in Python is now a function so meaning it needs parenthesis and that's where my problem starts. As I look through the codes on Twill 1.8.0, I think it isn't oriented with the change of using PRINT yet. Strings are not enclosed with parenthesis so I was thinking maybe there is a new patch or version of Twill that adapts the changes of Python, is there any? or is there anything I can do about this aside from manually editing Twill files?\nYour response is highly appreciated.\n", "\nI'm sorry, but I don't know what you mean by \"PRINT\" in the Python documentation. The documentation for the print function in Twil 1 is here: http://twill-scripting.readthedocs.org/en/latest/print.html\nThe documentation of the PRINTF function is: https://docs.python.net/3.6/library/stdtypes.typeslib.htm\nSo, you can use the function like this: print(str(1))\nBut, if you want to use it like you did in your code, then you need to enclose the string in parentheses. So, in this case, the code would be: PRint(print(2))\n\n", "I just found the answer. Thanks for viewing this question and probably for that downvote (SAD FACE).. So, the solution that I found was to use the 2to3 Script Tool found in Python Folder. Basically, it refactors Python 2 codes to Python 3 codes.\n", 0.80511075258255, 0.840714156627655, 0.8225273489952087, 0.23943661971830987, 9.21249397654898e-232, 520], ["I have written a library whose main functionality is implemented in C (speed is critical), with a thin Python layer around it to deal with the ctypes nastiness.\nI'm coming to package it and I'm wondering how I might best go about this. The code it must interface with is a shared library. I have a Makefile which builds the C code and creates the .so file, but I don't know how I compile this via distutils. Should I just call out to make with subprocess by overriding the install command (if so, is install the place for this, or is build more appropriate?)\nUpdate: I want to note that this is not a Python extension. That is, the C library contains no code to itself interact with the Python runtime. Python is making foreign function calls to a straight C shared library.\n", "The best way to do this would be to use dist.py to compile the shared C file into a.pyc file. This is the easiest way, and it will work with any C compiler. You can then use the python.org build system to build the library and the executable. \nIf you want a more robust solution, you can use a C extension to wrap the native C functions. For example, if you have the following C header file:  \n\n  #include <stdio.h>\n  int main(int argc, char **argv) {\n}\n#endif\nAnd you compile it with:\n\n $ gcc -c -fPIC -shared -o lib.so -I/usr/local/include -L/home/user/lib -lpython -lpthread -ldl -Wl,-rpath,/tmp/python-build/CMakeFiles/dist.dir/build.make\n $ make\n$ make install\nYou can also use make to create a static library, which is what you would do if the code was written in Python.\n\n", "I'd consider building the python module as a subproject of a normal shared library build.  So, use automake, autoconf or something like that to build the shared library, have a python_bindings directory with a setup.py and your python module.\n", 0.7992061376571655, 0.8576201796531677, 0.8273834586143494, 0.24083769633507857, 7.860766576120148e-232, 779], ["I took a new clean install of OSX 10.9.3 and installed pip, and then did\n\npip install pandas\npip install numpy\n\nBoth installs seemed to be perfectly happy, and ran without any errors (though there were a zillion warnings).  When I tried to run a python script with import pandas, I got the following error:\n\n\n    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): \n    File \"./moen.py\", line 7, in  import pandas File \"/Library/Python/2.7/site-packages/pandas/__init__.py\", line 6, in  from . import hashtable, tslib, lib \n    File \"numpy.pxd\", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) \n    ValueError: numpy.dtype has the wrong size, try recompiling\n\n\nHow do I fix this error and get pandas to load properly?\n", "The problem is that ndarray is not a standard Python type, so you need to use the npy_dtypes module to get the correct dtype.  The following code works for me: \n\n  1. Download the latest version of nnpy from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#nnpy\n2a. Install nnp from the command line: pip install -U nnn\n3. Create a directory called nndata and copy the file n.h into it.\n4. Add the line import n np.ndarray to the top of your file.\n\n", "pip uninstall numpy uninstalls the old version of numpy\npip install numpy finds and installs the latest version of numpy\n", 0.7981982231140137, 0.8560961484909058, 0.8261340260505676, 0.16822429906542055, 8.262950261496882e-232, 449], ["I knew that there is no builtin sph_hankel1 in scipy then i want to know that how to implement it in the right way?\nAdditional: Just show me one correct implementation of sph_hankel1 either using of Scipy or Sympy.\n", "\nThe following is a correct way to do it. It is based on the scikit-learn implementation. The only difference is that the function is called sp_hannkel_solve_one_class. \nfrom sklearn.svm import SVC\nimport numpy as np\ndef spHann(X, y, C=1):\n   # X is the input data\n#  y is y-1\nC = C*np.sqrt(1-C)\ns = SVM(C, kernel='linear')\nreturn s.fit(x=X[:,0], y=y[0])\nX = np.array([[1,2,3],[4,5,6],[7,8,9]]) \n\n", "Although it would be nice if there were an existing routine for calculating the spherical Hankel functions (like there is for the ordinary Hankel functions), they are just a (complex) linear combination of the spherical Bessel functions of the first and second kind so can be easily calculated from existing routines.  Since the Hankel functions are complex and depending on your application of them, it can be advantageous to rewrite your expression in terms of the Bessel functions of the first and second kind, ie entirely real quantities, particularly if your final result is real.\n", 0.7651057243347168, 0.8099159598350525, 0.7868734002113342, 0.15384615384615383, 9.707294186921477e-232, 397], ["I'm storing strings on the order of 150M. It's well-below the maximum size of strings in Redis, but I'm seeing a lot of different, conflicted opinions on the approach I should take, and no clear path.\nOn the one hand, I've seen that I should use a hash with small data chunks, and on the other hand, I've been told that leads to gapping, and that storing the whole string is most efficient.\nOn the one hand, I've seen that I could pass in the one massive string, or do a bunch of string-append operations to build it up. The latter seems like it might be more efficient than the former.\nI'm reading the data from elsewhere, so I'd rather not fill a local, physical file just so that I can pass a whole string. Obviously, it'd be better all around if I can chunk the input data, and feed it into Redis via appends. However, if this isn't efficient with Redis, it might take forever to feed all of the data, one chunk at a time. I'd try it, but I lack the experience, and it might be inefficient for a number of different reasons.\nThat being said, there's a lot of talk of \"small\" strings and \"large\" strings, but it's not clear what Redis considers an optimally \"small\" string. 512K, 1M, 8M?\nDoes anyone have any definitive remarks?\nI'd love it if I could just provide a file-like object or generator to redis-py, but that's more language-specific than I meant this question to be, and most likely impossible for the protocol, anyway: it'd just require internal chunking of the data, anyway, when it's probably better to just impose this on the developer.\n", "\nThe best way to store strings is to use the RediString class. This class is a subclass of RedicomString, which is the base class for all RedistributedString objects. You can use RedirectedString to create a RediredirectString object, then use it to append strings to a string object. Redirection is not required, though it is recommended. See the documentation for more information.\n\n", "One option would be:\nStoring data as long list of chunks\n\nstore data in List - this allows storing the content as sequence of chunks as well as desctroying whole list in one step\nstore the data using pipeline contenxt manager to ensure, you are the only one, who writes at that moment.\nbe aware, that Redis is always processing single request and all others are blocked for that moment. With large files, which take time to write you can not only slow other clients down, but you are also likely to exceed max execution time (see config for this value).\n\nStore data in randomly named list with known pointer\nAlternative approach, also using list, would be to invent random list name, write content chunk by chunk into it, and when you are done, update value in known key in Redis pointing to this randomly named list. Do not forget to remove old one, this can be done from your code, but you might use expiration if it seems usable in your use case.\n", 0.8368761539459229, 0.8127922415733337, 0.8246583342552185, 0.20512820512820515, 9.375276439949499e-232, 385], ["I have the string name = 'one two'. i want to make 'onetwo' from it. \nis there any cool python shortcut for it like .join() but without space?\n", "You can use the following code to do it:  \n\n  import string\n  name=string.ascii_lowercase\nname=name.replace(' ', '')\nprint(name)  #onetwo\n#one\n>>> 'onetwo'.replace(',', '').replace(\" \", ''))\n'onetwothree'\n['onet', 'two', '(', ',',')']\n[['one',\n '', '',  ''],\n ['two']]\n[[['on',], ['e',]], [['e'], []]]\n[]\n''\nNameError: name 'name' is not defined\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError\nI am not sure if this is the best way to solve your problem. But I think it is a good way.\nHope this helps.\n\n", "How about \"\".join(name.split(\" \")) ?\n", 0.7773434519767761, 0.8720886707305908, 0.8219949007034302, 0.024691358024691357, 8.007088433406279e-232, 536], ["I have the string name = 'one two'. i want to make 'onetwo' from it. \nis there any cool python shortcut for it like .join() but without space?\n", "You can use the following code to do it:  \n\n  import string\n  name=string.ascii_lowercase\nname=name.replace(' ', '')\nprint(name)  #onetwo\n#one\n>>> 'onetwo'.replace(',', '').replace(\" \", ''))\n'onetwothree'\n['onet', 'two', '(', ',',')']\n[['one',\n '', '',  ''],\n ['two']]\n[[['on',], ['e',]], [['e'], []]]\n[]\n''\nNameError: name 'name' is not defined\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nTypeError\nI am not sure if this is the best way to solve your problem. But I think it is a good way.\nHope this helps.\n\n", "You can do name.replace(' ','') or ''.join(name.split())\n", 0.7819820046424866, 0.8857288360595703, 0.8306284546852112, 0.1411764705882353, 8.200168874360579e-232, 536], ["Previously I ran python 2.7 on Debian Linux terminal (bash). I conveniently use control-f, control-b to move forward/back word.\nBut it does not work on updated 3.4 version, which generates unreadable symbol.\nIs there a way to configure the control-key recognition?\n", "\nThe problem is that the new version of python does support control keys. The problem with the old version is the lack of support for the key combination. You can use the following command to set the keys: \nsudo apt-get install python-setuptools\nThen you can run the command: python -m setuptool --install-dir=/usr/local/bin python\nThis will install the set of tools that you need to use to install and set up python. \n\n", "If you compiled Python 3.4 from source, you are probably missing the development libraries for readline. The package is typically called libreadline-dev.\n", 0.8279731869697571, 0.8620169162750244, 0.8446521759033203, 0.13861386138613863, 9.087393579154888e-232, 420], ["I am, as a hobby and best-practice exercise, to implement the same domain model (a simple GPS / GIS library, inspired in ISO 191xx stardards and OGC Abstract Model) both in Python and C#.\nIt first, I tought: \"well, ISO/OGC gave me a finished UML, so I will have each class in C# and in Python to have the same signature\".\nI quickly found myself stuck in the \"strict/static vs duck typing\" problem, since I cannot count on method signatures in python. For example:\n\nOverloading constructors is quite common and natural in C#, but in Python you have to resort to *args **kwargs and conditionals;\nProperties are encouraged in C#, but most source code I have seen around in Python tend to set the fields directly, although the use of @property or property() is quite straightforward.\n(and so on).\n\nActually there is (obviously) an obvious and very well documented \"difference in mindsets\" between one language and the other, and I would like to respect those differences while at the same time ending up with \"the same\" application, that is, equivalent domain model, architecture and functionality.\nSo my question basically is:\n\nIf I am to implement the same abstract model (\"UML-like\") in Python and C#, how should I proceed, and specifically, which constructs should be invariant, and which should be different?\n\n", " The answer is \"it depends\". The question is not \"how to do it\", but \"what to use\". I think that the best way to answer this question would be to write a small example, where you can see the differences between the two languages. I don't think it is possible to give a complete answer, because the question itself is too broad. But I can give you a few hints.\n\n", "What you called \"abstract model\" in MDA is called Platform Independent Model (PIM), and its implementation in C# and/or Python is called Platform Specific Model (PSM). It is supposed that there exist tranformations/code-generators from PIM to PSM's, so depending on how these code-generations work you will get appropriate C# and Python source code. Usually, such tools provide some means to control the code generated. And such control usually done via PIM annotations which are specific to PSM you are generating. Hope this helps.\n", 0.8468488454818726, 0.8093946576118469, 0.8276982307434082, 0.17610062893081763, 9.781643639777127e-232, 361], ["I'm developing a django project for agriculture. I want to name an app \"fields\" and inside the app \"fields\" I want to name a model \"Field\" (referring to a farmer field). \nI tried it and it works, so I assume that \"fields\" and \"Field\" are not reserved words in Django or Python. But I was just wondering if using these words can be problematic in the future or it's just fine?\nAnd the general question:\nIs there any way to check if a word is reserved in Django or Python?\n", " Yes, you can use the word \"reserved\" in Python, but it is not recommended to use it.\nFor example, in this question, the questioner says that the words \"field\" or \"farm\" should not be used in a Django project. However, he also says \"I'm not a programmer, I just want a field to be named 'field'\".\nSo, if you want the field name to have a specific meaning, then you should use a reserved word. For example: \"my_field\".\n\n", "little explanation with fun\nAnswer is Simply No, \nBecause Language only has the authority to own anything.Python is the owner of the house\nThe Django guy is paying rent to Python guy. So, How Django guy can reserve the objects of the house?\nsame logic is applied here too\n", 0.8357999324798584, 0.8356221318244934, 0.8357110023498535, 0.16923076923076924, 9.501567134814414e-232, 419], ["Im using openCV on Ubuntu 14.04, but some of the functions that I require particularly in cv2 library (cv2.drawMatches, cv2.drawMatchesKnn) does not work in 2.4.9. How do I uninstall 2.4.9 and install 3.0.0 from the their git? I know the procedure for installing 3.0.0 but how do I make sure that 2.4.9 get completely removed from disk?\n", "\nOpenCV 3 is a fork of OpenCV 2, and it is not compatible with Open CV 2 (see the Opencv 2 compatibility page).\nYou can use the following commands to uninstall OpenCv 2 and then install Open Cv 3:", "The procedure depends on whether or not you built OpenCV from source with CMake, or snatched it from a repository.\nFrom repository\nsudo apt-get purge libopencv* will cleanly remove all traces. Substitute libopencv* as appropriate in case you were using an unofficial ppa.\nFrom source\nIf you still have the files generated by CMake (the directory from where you executed sudo make install), cd there and sudo make uninstall. Otherwise, you can either rebuild them with the exact same configuration and use the above command, or recall your CMAKE_INSTALL_PREFIX (/usr/local by default), and remove everything with opencv in its name within that directory tree.\n", 0.8538944721221924, 0.8167672157287598, 0.8349182605743408, 0.20408163265306123, 1.1489066129300607e-231, 196], ["Running a python script on different nodes at school using SSH.  Each node has 8 cores. I use GNU Screen to be able to detach from a single process.\nIs it more desirable to:\n\nRun several different sessions of screen.\nRun a single screen process and use & in a bash terminal. \n\nAre they equivalent?\nI am not sure if my experiments are poorly coded and taking an inordinate amount of time (very possible) OR my choice to use 1. is slowing the process down considerably.  Thank you!\n", "  I would use the first option.\n\n", "I would think they are about the same.  I would prefer screen just because I have an easier time managing it.  Depending on the scripts usage, that could also have some effect on time to process.\n", 0.9035229086875916, 0.8477853536605835, 0.8747671842575073, 0.14285714285714285, 1.5656618337072542e-231, 33], ["I am planning on using IronPython to develop a GUI interface for some python code. Do I need to know any other programming languages other than python. Also if not are there any other GUI packages/addon's to python that only use python to implement and get the final product working?\n", "\nI would recommend you to use Python for your GUI development. It is a very powerful language and you can use it to create a lot of GUI applications.\nYou can also use PyQt to build a Qt based GUI application. PyGTK is another GUI toolkit that is very popular and easy to learn. You can learn more about it here.\n\n", "You don't need to know any other languages - modulo a few implementation differences, Python is Python is Python. You will, however, need to know the Microsoft windowing library, with which I believe you will have to interface to build a GUI.\n", 0.8716499209403992, 0.8634512424468994, 0.8675311803817749, 0.2912621359223301, 1.0136839094656235e-231, 313], ["Say I have a categorical feature, color, which takes the values\n['red', 'blue', 'green', 'orange'],\nand I want to use it to predict something in a random forest.  If I one-hot encode it (i.e. I change it to four dummy variables), how do I tell sklearn that the four dummy variables are really one variable?  Specifically, when sklearn is randomly selecting features to use at different nodes, it should either include the red, blue, green and orange dummies together, or it shouldn't include any of them.\nI've heard that there's no way to do this, but I'd imagine there must be a way to deal with categorical variables without arbitrarily coding them as numbers or something like that.\n", "\nYou can use the \"one-vs-rest\" trick to get the same effect. The trick is to make sure that you're using the correct number of features. If you have four features, you can code them all as one feature. For example, if you had a feature called \"color\" and you wanted to code it as \"red\", you could code \"1\" for the first feature and \"0\" or \"2\" (depending on whether you want the feature to be \"blue\" OR \"green\") for each of the other three. This is the one vs. rest trick. You can also use this trick with a single feature (e,g., \"age\") to create a onevs.all feature set.\n\n", "You have to make the categorical variable into a series of dummy variables. Yes I know its annoying and seems unnecessary but that is how sklearn works. \nif you are using pandas. use pd.get_dummies, it works really well. \n", 0.8172756433486938, 0.8347460627555847, 0.8259184956550598, 0.2064516129032258, 8.569368607168265e-232, 572], ["Before I updated, I would run my script and output the html file. There would be my one plot in the window. I would make changes to my script, run it, output the html file, look at the new plot. Then I installed the library again to update it using conda. I made some changes to my script, ran it again, and the output file included both the plot before I made some changes AND a plot including the changes. I ran the script again out of curiosity. Three plots in the one file! Ran it again. Four! Deleted the html file (instead of overwriting). Five! Changed the name of the output html file. Six! I even tried changing the name of the script. The plots just keep piling up.\nWhat's going on? Why is it plotting every version of the graph I've ever made?\n", "\nThe problem is that the condas are not updating the libraries. They are updating condadas.py. This is a python script that is used to create a condada. It is not a library. So when you run condadas.python, it is actually running condado. That is why you get the same plots. You need to run the python file with condados. python. If you want to see the plots, you need condads. py. Run condades. Python is the only way to get them. \nIf you are using the latest condadata, then you can use condade. condde. But you will need the newest condata. Condada is an older python library that was created for condadi. And condatas is newer. Both are python libraries that are used for creating condadia.\n\n", "as of 0.5.1 there is now bokeh.plotting.reset_output that will clear all output_modes and state. This is especially useful in situations where a new interpreter is not started in between executions (e.g., Spyder and the notebook)\n", 0.8148713111877441, 0.8218392133712769, 0.8183404207229614, 0.11764705882352941, 8.009388329789305e-232, 696], ["I'm using distutils to compile C code via a python script. If things go wrong, I want to be able to catch the error output. To this end, I've redirected stdout and stderr into temporary files before running the setup() command (you need to use os.dup2 for this).\nOn linux, it works fine. On windows + mingw I get some really weird behaviour:\n\nWithout trying to capture, stdout and stderr are both written to the command prompt.\nWhen I try to capture, stdout works fine but the output to stderr disappears.\n\nDoes anybody understand what's going on here?\n", " The problem is that the stdin of the python interpreter is not a pipe. It's a file descriptor. The standard library doesn't know how to deal with this. You need a wrapper around the standard input. This is what the distlib module does. See the documentation for the module.\n\n", "Woops.\nIt turns out this was something really simple: capturing stdout and stderr output was working just fine, but the particular error message I was looking to catch (which was windows specific) wasn't part of the printed output but the error message of the raised SystemExit exception.\nBig waste of time :(\n", 0.8574299216270447, 0.8381658792495728, 0.8476884961128235, 0.1747572815533981, 1.0093095674422238e-231, 276], ["In our company we using vagrant VM's to have the save env. for all. Is it possible to configure VisualStudio + PTVS (python tools for VS) to use vagrant based python interpreter via ssh for example?  \n", " Yes, you can use Vagrant to run your python scripts.\nYou can install the Vagrantly Python Tools for Visual Studio (PTVSS) on your machine. You can find the PtvSS package on the official Vagrancy website. \n\n", "There's no special support for remote interpreters in PTVS, like what PyCharm has. It's probably possible to hack something based on the existing constraints, but it would be some work...\nTo register an interpreter that can actually run, it would have to have a local (well, CreateProcess'able - so e.g. SMB shares are okay) binary that accepts the same command line options as python.exe. It might be possible to use ssh directly by adding the corresponding command line options to project settings. Otherwise, a proxy binary that just turns around and invokes the remote process would definitely work.\nRunning under debugger is much trickier. For that to work, the invoked Python binary would also have to be able to load the PTVS debugging bits (which is a bunch of .py files in PTVS install directory), and to connect to VS over TCP to establish a debugger connection. I don't see how this could be done without writing significant amounts of code to correctly proxy everything.\nAttaching to a remotely running process using ptvsd, on the other hand, would be trivial.\nFor code editing experience, you'd need a local copy (or a share etc) of the standard library for that interpreter, so that it can be analyzed by the type inference engine. \n", 0.8382163047790527, 0.8112385272979736, 0.8245067596435547, 0.11904761904761905, 1.142360641807185e-231, 207], ["I'm a newbee of python. And recently I heard some people say that numpy is a good module for dealing with huge data.\nI'm curious what can numpy do for us in the daily work.\nAs I know, most of us were not scientists and researchers, at what circumstances numpy can bring us benefit? \nCan you share a good practice with me?\n", "\nThe answer is: ndarray is the best module to deal with big data, because it is easy to use and it has a lot of features.  \n\n", "Long answer short, when you need do huge mathematical operations, like vector multiplications and so on which requires writing lots of loops and what not, yet your codes gets unreadable yet not efficient you should use Numpy.\nFew key benefits:\n\nNumPy arrays have a fixed size at creation, unlike Python lists (which can grow dynamically). Changing the size of an ndarray will create a new array and delete the original. So it is more memory efficient than the other.\nThe elements in a NumPy array are all required to be of the same data type, and thus will be the same size in memory. The exception: one can have arrays of (Python, including NumPy) objects, thereby allowing for arrays of different sized elements.\nNumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. Typically, such operations are executed more efficiently and with less code than is possible using Python\u2019s built-in sequences.\nA growing plethora of scientific and mathematical Python-based packages are using NumPy arrays; though these typically support Python-sequence input, they convert such input to NumPy arrays prior to processing, and they often output NumPy arrays. In other words, in order to efficiently use much (perhaps even most) of today\u2019s scientific/mathematical Python-based software, just knowing how to use Python\u2019s built-in sequence types is insufficient - one also needs to know how to use NumPy arrays.\n-Vector operations comes handy in Numpy. You don't need to go through writing loops but yet pythonic.\n-Object oriented approach\n\n", 0.8789350986480713, 0.7977860569953918, 0.8363968729972839, 0.11228070175438597, 1.2183324802375697e-231, 125], ["I've taken to creative coding on my iPad and iPhone using Codea, Procoding, and Pythonista. I really love the paper.js Javascript library, and I'm wondering how I might have the functionality that I find in paper.js when writing in Python.\nSpecifically, I'd love to have the vector math and path manipulation that paper.js affords. Things like finding the intersection of two paths or binding events to paths (on click, mouse move, etc).\nThere's an ImagePath module provided by Pythonista that does some path stuff but it's not as robust as paper.js (it seems).\nAny ideas?\n", "\nI'm not sure if this is the right place to ask this question, but I'll try to answer anyway. The answer is that you can use the Image Path module from Pythonistas to do the same thing that Paper.JS does. You can find the code here: https://github.com/pythonista/paper.html\nThe code is pretty simple: you just need to create a path object and then use it to draw a rectangle. Here's the example: \nimport paper\nfrom paper import Image, Path\npath = paper('path')\nrect = path.rectangle(0, 0, 100, 50)\nprint(rect)\n\n", "The ui module actually includes a lot of vector drawing functions, inside a ui.ImageContext. ui.ImageContext is a thin wrapper around part of one of the Objective-C APIs (maybe CALayer?) The drawing methods are designed to operate inside the draw method of a custom view class, but you can present these things in other contexts using a UIImageContext, from which you can get a static image.\n", 0.8048170804977417, 0.844245433807373, 0.8240599036216736, 0.20359281437125748, 9.0415631926024e-232, 511], ["This has been bugging me for a while. I'm wondering about the comparative memory-efficiency of assigning variables and calling methods. Consider, for example:\n\ns = \"foo\"\n  x = s.lower()\n\nVersus\n\nx = \"foo\".lower()\n\nWhich one of these is more efficient in terms of memory use? \nThis is obviously a trivial example, but it illustrates what I'm wondering about.\nThere are many instances in which we define some variable var1 = foo, and then define a second variable var2 = var1.method(). Does this total process require more memory than just defining var2 = foo.method()? \n", " No. The first example is just as efficient as the second.  \n\n", "In your first example s is still in memory until the garbage collector deletes it, so your second example is more efficient in terms of memory use. However, considering that's a very very little portion of the RAM, in many cases it's better to go for readability (the first example looks better).\nHope this helps.\n", 0.9058138132095337, 0.8457134962081909, 0.8747325539588928, 0.20588235294117646, 1.3729916628506288e-231, 62], ["I have a series of unit tests that are meant to run in two contexts:\n1) On a buildbot server\n2) in developer's home environments\nIn both our development procedure and in the buildbot server we use virtualenv. The tests run fine in the developer environments, but with buildbot the tests are being run from the python executable in the virtualenv without activating the virtualenv. \nThis works out for most tests, but there are a few that shell out to run scripts, and I want them to run the scripts with the virtualenv's python executable. Is there a way to pull the path to the current python executable inside the tests themselves to build the shell commands that way?\n", " You can use the --python-config option to activate the Python virtual environment.\n$ python --config=python.exe\nPython 3.6.2 (default, Sep  5 2017, 16:05:08)\n[GCC 4.8.4] on linux\nType \"help\", \"copyright\",  \"credits\" or \"license\" for more information.\n\n   >>> import os\n  File \"<stdin>\", line 1\nImportError: No module named os.path\n>>> import sys\nTraceback (most recent call last):\nFile \"<pyshell#1> in <module>()\n<pip>\\n\", file=<stdout>\nimport os, sys, traceback\nsys.exit(1);\n</pypi>\n\n", "The current python executable is always available as sys.executable, which should give full path (but you can ensure this using os.path functions).\n", 0.7684703469276428, 0.8571025729179382, 0.8103702068328857, 0.13592233009708737, 8.844844403089351e-232, 486], ["I have a large buffer of strings (basically 12GB) from a C app.\nI would like to create PyString objects in C for an embedded Python interpreter without copying the strings. Is this possible?\n", "\nYou can use the PyObject_CallMethod() function to call a function in Python. For example, you can call the function Py_BuildValue() to build a PyValue object. Then you call PyEval_EvalsCode() on the resulting PyCodeObject to get the code for the string. Finally, call str() or PyUnicode_FromString() with the result of the call to PyBuildvalue() and the return value of PyCallObject().\nFor example: \nPyObject *py_string = PyInt_AsString(Py_None);\nif (pyObject == NULL) {\n   // handle error\n}\nelse {  /* do something with string */\n PyErr_SetString((PyExc_TypeError)(\"string expected\"), \"string\");\n }\n return py_object;\n /* return the value */ \n\n", "I don't think that is possible for the basic reason that Python String objects are embedded into the PyObject structure.   In other words, the Python string object is the PyObject_HEAD followed by the bytes of the string.    You would have to have room in memory to put the PyObject_HEAD information around the existing bytes.\n", 0.8038482666015625, 0.8548633456230164, 0.8285712599754333, 0.2894736842105263, 8.789561417631751e-232, 646], ["I have a dashboard I am working on, using Python cherrypy framework and Mako template language.\nI had an html file for each of dashboard pages.\nThere, I used Mako to pass some data to html and inline Javascript.\nFor example, to display the names of some processes when I only had the list of ids, I passed a Python dict that maps ids to their corresponding names, and then used the dict in ${} tags.\nHowever, as I am now moving these Javascript codes into a separate file with .js extension, I found out simply putting the same Mako code blocks in the Javascript code does not work.\nIs there any way I could use Mako template language in an external .js file that is imported in an html file?\nIs it considered a bad practice and should I pass all these data using XMLHTTPRequests when I am passing them to Javascript?\n", "ou can use the Makos template engine to generate the HTML code for your dashboard. \nThe Makor template is a templating language that can be used to create HTML templates. It is not a programming language, but it is very similar to Python's Jinja2 templates. You can find more information about Makors here. The Makoros template can also be found here: https://github.com/makor/Mako/wiki/Template-Language\nIn your case, you can create a template file called dashboard-template.html and put the following code in it:  \n\n  <!DOCTYPE html>\n  <!--\n<html lang=\"en\">\n-->\n <head>\n\n <meta charset=\"utf-8\">\n\n <!-- Required meta tags -->\n <!-- <link rel=\"shortcut icon\" href=\"%{base_url}/favicon.ico\" type=\"image/x-icon\"> -->\n\n </head><!-- End of the head tag -->  //\n\n\n</html>  %{html_content}\n%end\n{javascript_code}\n\n", "Ajax is the \"right\" way to do this.\nIn order to insert the values into your separate Javascript file dynamically, it can no longer be served as a static file. Beyond that, it adds an extra layer of problems with security and maintainability as you have to deal with string escaping, possible script injection, and having Mako syntax in your Javascript. Not to mention losing the ability to host your .js files on a CDN or server configured for static files.\n", 0.7648491263389587, 0.8322244882583618, 0.797115683555603, 0.24880382775119617, 8.251353866757173e-232, 808], ["Disclaimer: I'm still not sure I understand fully what setup.py does.\nFrom what I understand, using a setup.py file is convenient for packages that need to be compiled or to notify Disutils that the package has been installed and can be used in another program. setup.py is thus great for libraries or modules.\nBut what about super simple packages that only has a foo.py file to be run? Is a setup.py file making packaging for Linux repository easier?\n", "\nThe setup file for a package is a file that contains the information about the packages and the dependencies. It is not a program that is run by the user. The user can run the setup script to install the software. This is the reason why the file has to exist. If the files are not there, the installation will fail. In the case of a library, it is possible to use the install_requires() function to add the required dependencies to the requirements file. For example, if the library requires a certain version of the Python library (e.g. the version that was installed on the system), the following code will add it to requirements.txt: \ninstall_requirements(['python-setuptools','setools'],\n   requirements_file='requirement.in',\n)\nThis is how the dependency information is stored in the.setup file: the first line contains a list of dependencies, and each dependency is separated by a comma. Each dependency can have a different number of arguments. \n\n", "Using a setup.py script is only useful if:\n\nYour code is a C extension, and then depends on platform-specific features that you really don't want to define manually.\nYour code is pure Python but depends on other modules, in which case dependencies may be resolved automatically.\n\nFor a single file or a few set of files that don't rely on anything else, writing one is not worth the hassle. As a side note, your code is likely to be more attractive to people if trying it up doesn't require a complex setup. \"Installing\" it is then just about copying a directory or a single file in one's project directory.\n", 0.823620617389679, 0.8328724503517151, 0.8282206654548645, 0.302158273381295, 7.913656765322456e-232, 955], ["I'm making a time series monitoring program.\n\nI'd like to change the color of a plot starting at half the range of the x-axis.\nFor a 100 x 20 plot widget I would like to change last 50 data points to another color.\nHow can I draw a custom vertical grid whenever every xx items of data are passed?\n\n", "\nYou can use the following code to draw the grid:\n\n   plot(x, y, type='n', xlab='x', ylab='')\n  axis(1, at=seq(0, 1, length.out=100), labels=c('x','y'))\n grid.text(seq(-1.5, 0.1), seq(-0.3, -0), cex=0)\n\n", "For (1), you will have to break your data into two separate lines, and assign the colors individually. PyQtGraph does not yet support multiple colors per line.\nFor (2), consider using pg.InfiniteLine or pg.VTickGroup. \n", 0.7740243673324585, 0.8330631256103516, 0.8024592995643616, 0.09999999999999999, 1.0898774101582699e-231, 203], ["I am learning the web framework Tornado. During the study of this framework, I found the class  tornado.httpserver.HTTPserver. I know how to create a constructor of this class and create instance tornado.httpserver.HTTPserver in main() function. But this class tornado.httpserver.HTTPserver  has 4 methods. I have not found how to use these methods.\n1) def close_all_connections(self):\n2)  def handle_stream(self, stream, address):\n3)  def start_request(self, server_conn, request_conn):\n4)  def on_close(self, server_conn):\nI know that 2-4 methods are inherited from the class tornado.tcpserver.TCPServer\nCan someone illustrate how to use these methods of a class tornado.httpserver.HTTPserver?\n", "\nThe class TornadoHTTPServer is a subclass of tornado_http_server and tornadohttp.HTTPServer, which are the base classes of Tornado, and the tornado http server. The class is inherited by tornado https server, tornado server and TornadoHTTPResponse. \nclass TornadoHttpServer(tornado_https_servers.TornadoHTPServers):\n\n   # Define the methods for the server\n ...\ndef __init__(server_name, port, host, timeout=None,\n#  timeout_ms=0, max_retries=3, retry_count=1,  retries_per_host=5, # default: 5\n): \n\n # Create the HTTP server instance\n tornado = tornado._TCPServer.instance(host,port,timeout=timeout,max_reuse=max(timeout_,retrycount))\n # Set the default port\n if port is None: port = 80\n else:  port=port\n...\n\n def __call__():\n try:\n\n...  if timeout is not None and timeout > 0: # timeout\n except: pass\n timeout = timeout or 0\n ret = 0\n\n if ret is 0 and ret == 0 or ret > max(ret, 0): # ret\n return tornado\n elif ret!= 0 :\n raise Exception('ret = %d, %s' % (ret, str(time())))\n time.sleep(0.1 * ret)\n def open_connection(conn_id, addr): return conn_open(addr)\n\n class connopen: def connect(connection_info): conn = None\n conn.connect(address) # connect to the address\n connection_status = conn._status\n status = connection.status\n\n conn, status, connection =  conninfo(status, conn) if connection else None\n\n return connection\n\n @tornadoweb.server\n class HTTPConnection", "These methods are used internally; you shouldn't call them yourself.  \n", 0.720919132232666, 0.840269923210144, 0.7760323882102966, 0.02816901408450704, 6.410368644657512e-232, 1370], ["This seems to be a dumb question, but how do I ssh into the kivy-remote-shell?\nI'm trying to use buildozer and seem to be able to get the application built and deployed with the command, buildozer -v android debug deploy run, which ends with the application being pushed, and displayed on my android phone, connected via USB.\nHowever, when I try ssh -p8000 admin@127.0.0.1 from a terminal on the ubuntu machine I pushed the app from I get Connection Refused.\nIt seems to me that there should be a process on the host (ubuntu) machine in order to proxy the connection, or maybe I just don't see how this works?\nAm I missing something simple, or do I need to dig in a debug a bit more?\n", "\nYou need a proxy server on your host machine. You can use the built-in proxy service in your system. For example, you can install the proxy-server package on Ubuntu. Then, run the following command: \nsudo apt-get install proxy\nThen, in the terminal, type: proxy -l\nThis will list all the available proxy servers. \n\n", "When the app is running, the GUI will tell you what IP address and port to connect to.\n", 0.8249335289001465, 0.8830162286758423, 0.8529872298240662, 0.10810810810810811, 9.358197857531086e-232, 316], ["This seems to be a dumb question, but how do I ssh into the kivy-remote-shell?\nI'm trying to use buildozer and seem to be able to get the application built and deployed with the command, buildozer -v android debug deploy run, which ends with the application being pushed, and displayed on my android phone, connected via USB.\nHowever, when I try ssh -p8000 admin@127.0.0.1 from a terminal on the ubuntu machine I pushed the app from I get Connection Refused.\nIt seems to me that there should be a process on the host (ubuntu) machine in order to proxy the connection, or maybe I just don't see how this works?\nAm I missing something simple, or do I need to dig in a debug a bit more?\n", "\nYou need a proxy server on your host machine. You can use the built-in proxy service in your system. For example, you can install the proxy-server package on Ubuntu. Then, run the following command: \nsudo apt-get install proxy\nThen, in the terminal, type: proxy -l\nThis will list all the available proxy servers. \n\n", "\n127.0.0.1\n\nThis indicates something has gone wrong - 127.0.0.1 is a standard loopback address that simply refers to localhost, i.e. it's trying to ssh into your current computer.\nIf this is the ip address suggested by kivy-remote-shell then there must be some other problem, though I don't know what - does it work on another device?\n", 0.8231277465820312, 0.8229022026062012, 0.8230149149894714, 0.09917355371900825, 9.939763718065626e-232, 316], ["I have a word, according to that i want to find out whether the text is related to that word or not using python and nltk is it possible ?\nFor example I have a word called \"phosphorous\". I would like to find out that the particular text file is related to this word or not?\nI cant use bag of words in nltk as I have only one word and no training data.\nAny Suggestions?\nThanks in Advance.\n", "You can use ngrams to do this. For example, you can do a n-gram search on the word \"Phosphorus\" and then use the nn-gram package to get the frequency of each word in the corpus. You can then do the same thing on your corpus and see if the words are related. If they are, then you know that your text contains the phrase \"PHOSPHORUS\".\nIf you want a more general approach, I suggest you look at the WordNet package. It's a very powerful tool that can be used to search for words that are similar to a given word.\n\n", "Not without a corpus, no.\nLook at it this way: can you, an intelligent being, tell whether \u5149 is related to \u90e8\u5c4b\u306b\u5165\u3063\u305f\u6642\u96fb\u6c17\u3092\u3064\u3051\u307e\u3057\u305f without asking someone or something that actually knows Japanese (assuming you don't know Japanese; if you do, try with \"svjetlo\" and \"Kad je u\u0161ao u sobu, upalio je lampu\"). If you can't, how do you expect a computer to do it?\nAnd another experiment - can you, an intelligent being, give me the algorithm by which you can teach a non-english-speaking person that \"light\" is related to \"When he entered the room, he turned on the lamp\"? Again, no.\ntl;dr: You need training data, unless you significantly restrict the meaning of \"related\" (to \"contains\", for example).\n", 0.8418226838111877, 0.798727810382843, 0.8197092413902283, 0.375, 9.248271325361215e-232, 512], ["I am running some K-Means clustering from the sklearn package.\nAlthough I am setting the parameter n_jobs = 1 as indicated in the sklearn documentation, and  although a single process is running, that process will apparently consume all the CPUs on my machine. That is, in top, I can see the python job is using, say 400% on a 4 core machine.\nTo be clear, if I set n_jobs = 2, say, then I get two python instances running, but each one uses 200% CPU, again consuming all 4 of my machine's cores.\nI believe the issue may be parallelization at the level of NumPy/SciPy.\nIs there a way to verify my assumption? Is there a way to turn off any parallelization in NumPy/SciPy, for example?\n", "\nThe documentation for the k-means function states that the default value for n-jobs is 1. \nIf you want to run multiple instances of kmean, you can set the njobs parameter to 2. This will run two instances, each using 200%.\nIn addition, the documentation states:  \n\n  nj = n - 1\n  The default is n = 10. The number of clusters to be created. If n is\nnot specified, it is set to the number that would be returned by\nkmeanext.\n\n", "Indeed BLAS, or in my case OpenBLAS, was performing the parallelization.\nThe solution was to set the environment variable OMP_NUM_THREADS to 1.\nThen all is right with the world.\n", 0.8303308486938477, 0.8542218208312988, 0.8421069383621216, 0.18181818181818182, 9.22455514866612e-232, 426], ["I have installed the latest versions of both django and python. The default \"python\" command is set to 2.7; if I want to use python 3, I have to type \"python3\". \nHaving to type \"python3\" and a django command causes problems. For example if I type: \"python3 manage.py migrate\" , I get an error. The error is:\nTraceback (most recent call last):\n  File \"manage.py\", line 8, in \n    from django.core.management import execute_from_command_line\nImportError: No module named 'django'\nDjango does not seem to recognize my python 3. How do I get around this? Your help is greatly appreciated.\n", "The answer is to install the python3 version of djangoproject. \n\n", "You need to install django for python 3, pip3 install django\n", 0.8841553330421448, 0.8912942409515381, 0.887710452079773, 0.1904761904761905, 1.3395791664433662e-231, 65], ["I have a Python script running as a daemon. At startup, it spawns 5 processes, each of which connects to a Postgres database. Now, in order to reduce the number of DB connections (which will eventually become really large), I am trying to find a way of sharing a single connection across multiple processes. And for this purpose I am looking at the multiprocessing.sharedctypes.Value API. However, I am not sure how I can pass a psycopg2.connection object using this API across processes. Can anyone tell me how it might be done?\nI'm also open to other ideas in order to solve this issue.\nThe reason why I did not consider passing the connection as part of the constructor to the 5 processes is mutual exclusion handling. I am not sure how I can prevent more than one process from accessing the connection if I follow this approach. Can someone tell me if this is the right thing to do?\n", "\nYou can use the sharedctype.Pool class to share a connection between multiple threads. The Pool class has a pool_size parameter which is a number that determines how many threads will be created. You can set this value to 0 to create a new thread for each connection. If you set the pool size to 1, then you can create one thread per connection and share the same connection among all threads, but you will have to use a shared connection object. \nHere is an example of how to set up a Pool object:  \n\n  import psyco\n  from psycos import Connection\n# Create a Connection object\nconn = Connection('dbname', 'username',  'password')\nprint conn.pool_info()\npool = psycolib.ConnectionPool(conn)\nfor i in range(pool.num_threads):\n\tpool[i].start()  # Start a thread\np = pool.get() # Get a handle to this connection\nwhile p: # Wait for the thread to finish\n print p.conn.close()\n\n print 'done'\n # Close the Connection and Pool objects\n conn = None\n pool =None\n p = 0\n while p < pool:\n\n p += 1\n if p % pool == 0: print \"done\"\n else: break\nelse: p -= 1\n\n conn\n Pool.terminate()# Close all connections\nPool.destroy()// Close Pool\nif conn: conn() // Close Connection\n\n\nA: You don't need to pass the psycog2 connection to each process. Just pass it to all processes and they will share it.\n\n", "You can't sanely share a DB connection across processes like that. You can sort-of share a connection between threads, but only if you make sure the connection is only used by one thread at a time. That won't work between processes because there's client-side state for the connection stored in the client's address space.\nIf you need large numbers of concurrent workers, but they're not using the DB all the time, you should have a group of database worker processes that handle all database access and exchange data with your other worker processes. Each database worker process has a DB connection. The other processes only talk to the database via your database workers.\nPython's multiprocessing queues, fifos, etc offer appropriate messaging features for that.\n", 0.7818702459335327, 0.8383800983428955, 0.8091397285461426, 0.29461756373937675, 7.300192564891803e-232, 1280], ["I have a log file by the name log.json.\nA simple insert in rethinkdb works perfectly.\nNow this json file get updated every second, how to make sure that the rethinkdb gets the new data automatically, is there a way to achieve this, or i have to simply use the API and insert into db as well as log in a file if i want to.\nThanks.\n", "\nYou can use a cron job to run a script every minute to update the log files. \nThe script will look like this:  \n\n  #!/bin/bash\n  while true; do\n\t  echo \"Starting crontab job\"\n# \techo \"Running crond job...\"\n #  sleep 1\ndone\necho \"\"\nsleep 1  ## to avoid the crons to be run at the same time\nrm -f log.*\nfor file in *.log;\ndo\ncat $file | sed -e's/^.*\\(.*\\)$/\\1/' > $1\nfi\nmv $2 $3\nif [ $? -eq 0 ]; then\n echo \"$file updated\"  ### this is the output of the script\nelse\n rm -rf $4\n fi\n done\nexit 0\nThis will update all the *.json files in the current directory every min.\n\n", "The process that appends new entries in your json file should probably run query to insert the same entries in RethinkDB.\nOr you can have a cron job that\n\nget the last entry saved from rethinkdb\nread your json file for new entries\ninsert new entries\n\n", 0.7736376523971558, 0.8397619128227234, 0.8053447604179382, 0.22222222222222218, 8.66005677222379e-232, 568], ["When creating a TextItem to be added to a plotItem in PyQtGraph, I know it is possible to format the text using html code, however I was wondering how to format the text (i.e. change font size) when updating the text through TextItem.setText()? or do I need to destroy/re-create a TextItem? \n", "\nYou can use the setText method to change the font of the Text Item.\nfrom PySide import QtGui, QtCore\nimport sys\nclass MyPlot(QWidget):\n   def __init__(self):\n\n  QWidget.__init_py__()\n#  self.initUI()\n\n def initUI( self ):\n self._plot = QtWidgets.QPlainTextEdit()  # create a new TextEdit\n # self_plot.textEdit.append( 'Hello World' )\nself._text = self_.plot._addItem( \"Hello\", self )\n\n self.setLayout( Qt. horizontalLayout ) \n\n # set the layout of self\n def setLayout ( self, layout ):\n\n layout.addWidget( QPlayoutItemLayout() ) # add the new layout\n layout = QVBoxLayout().addLayout([ self, self ] )# add a layout to the parent layout\n\n return self\n\n", "It is not documented, but the method you want is TextItem.setHtml().\n", 0.778386116027832, 0.8703501224517822, 0.8218033313751221, 0.058823529411764705, 8.061788991689795e-232, 652], ["I have an FTP server that hosts data files, where the date that the data is associated with is encoded into the file names. I want to write a process that can find and download all the files associated with a particular date. The complication is that different files use different encodings. (Unfortunately changing/standardising the names isn't an option.) The year can be four digits or two. The month can be two digits or three letters. Sometimes the day is represented, and the substring can be anywhere in the string.\nAt the moment, I'm creating a list of all the files on the server, then using a regular expression to determine which files are relevant, and then downloading those files.\nIs it possible to condense the first two steps? That is, is there a way to get the server to return the list of files that match the expression?\nI'm using the Python ftplib if that makes any difference.\n", "\nThe answer is yes, you can use regular expressions to find the filenames. You can do this by using regular expresions to match against the filename. For example, if you have a filename like \"2012-01-02.txt\", you could use the following regularexpression: ^\\d{4}-\\w{2}(\\.\\S+)?$\nThis will match any filename that starts with the year, followed by a 4 digit number, a 2 digit month, 2 digits day, or a 3 letter day.\n\n", "It should be fairly simple to use LIST, MLSD and NSLT to build a local index of the files on the FTP, and then use regex to filter unwanted files from the index, and then use the remainder in a batch script to download them.\n", 0.7972376346588135, 0.84592604637146, 0.8208605051040649, 0.16393442622950818, 9.285083362051119e-232, 415], ["Can you guys help me find Qt designer app on mac. I installed anaconda package and conda reports that qt, sip, and pyqt are installed. Still I couldn't find the designer app in any of the folders. My python app that uses pyqt works perfectly. I'm very new to macs and probably missing something very simple.\nI did search folder tree for anything named designer. I found QtDesigner.so (supposed to be executable?) at\n/Users/XXXX/anaconda/pkgs/pyqt-4.10.4-py27_0/lib/python2.7/site-packages/PyQt4 \nbut it won't even run saying \"cannot execute binary file\"\nanaconda/bin doesn't have it\nthere's a folder anaconda/include/QtDesigner but noting I can run\n/anaconda/pkgs/qt-4.8.5-3/bin - no designer\nI'm totally confused now.\n", "\nThe designer is in the /Users folder. You can find it by typing\n$ cd /an/ac/users/\nand then typing qmake\nYou will see the folder qdesigner\nIf you want to use the Qt Designer, you need to install it. The easiest way to do this is to run the following command:   sudo apt-get install qtdesigner  (or sudo pip install pyQtd)\nThen you can use qMake to create a new project. \n\n", "OSX Yosemite 10.10.5\nQt 5.6\nQtCreator 3.6.1\nQtDesigner is part of my QtCreator.  To use QtDesigner:\n\nLaunch QtCreator, and from the menu bar (outside QtCreator), click on:\nFile>New File or Project \nYou will be presented with a New File or Project dialog window.  In the Files And Classes section, select Qt.  In the middle pane, select QtDesigner Form.  Then click on the Choose button in the lower right corner.\nYou will be presented with a QtDesigner Form dialog window.  Then you can select Main Window or Dialog with Buttons Bottom, etc. Then click on the Continue button in the lower right corner.\nIn the Location dialog window, use a name like mainwindow1.ui, and for the path you might want to step aside and create a directory called forms, e.g. $ mkdir /Users/7stud/qt_projects/forms, then enter that as the path.\nEnter any other details and click on Done.   That will land you in QtCreator with the Design button selected (which I guess means you are in QtDesigner), and you will be able to drag and drop widgets onto your window.\nTo convert the .ui file to a .py file that you can import into your python program:\n$ pyuic5 mainwindow1.ui -o mainwindow1.py\n-o => output file (default is stdout)\n\nThat command converts the .ui file mainwindow1.ui to a .py file named mainwindow1.py.\nTo re-open the file: File>Open File or Project.  If you select a file with a .ui extension, it will be opened with QtCreator's Design button pre-selected, i.e. you will be inside QtDesigner.\n", 0.8510034680366516, 0.8099571466445923, 0.8299731016159058, 0.22807017543859642, 1.0425481295341422e-231, 373], ["I understand that I should use os.urandom() or SystemRandom in Python for 'secure' pseudo-random numbers.\nBut how does Python generate these random numbers logically? \nAlso is there a way to generate a number in Python that is 'more random' than others?\n", "he random module is a Python module that provides a set of functions for generating pseudo random values.  The random.Random module provides the following functions:  \n\n  * random_int()  - Generates a pseudo integer in the range [0, 1)\n ...\n\"\"\"\nrandom_number = random\n# random = int(random.random())\n...\nRandom = Random()\nprint(Random.randint(0))\n>>> 0\n```\nThis is the same as the function in C:\n\n\nimport random\n\n", "For \"secure\" random numbers, Python doesn't actually generate them:  it gets them from the operating system, which has a special driver that gathers entropy from various real-world sources, such as variations in timing between keystrokes and disk seeks.\n", 0.7905969619750977, 0.8309381008148193, 0.8102657198905945, 0.1568627450980392, 9.378217703508016e-232, 413], ["I've been learning Python using version 3.4. I recently started learning Web.py so have been using Python 2.7 for that, since web.py not supported in Python 3.4.  I have nose 1.3.4 module installed for both Python 3.4 and 2.7.  I need to run the nosetests command on some Python code written in 2.7 that uses the Web.py module.  However, when I type nosetests command it automatically uses Python 3.4, so is throwing an error as unable to import the Web.py module in my Python code.  Is there a way to force nosetests to use Python 2.7?  Macbook Pro running OS X Yosemite.\n", "\nYou can use the --with-python-version option to noseteests. This will tell nosetteests that you want to test your code with Python version 2, not 3, and that it should use that version of Python. For example: nosettests --python 2 --without-webpy --no-with --test-suite=test_suites/test.test\nNote that this will only work if you have the Python module Webpy installed. If you don't have it, you can install it with: pip install webpy\nIf you are using nose, then you should be able to just run nosetsests without the -v option.\n\n", "Try python2.7 -m nose instead of running nosetests. \u2013  dano 38 mins ago\n", 0.8055317401885986, 0.809244692325592, 0.8073839545249939, 0.03636363636363636, 8.319100378795605e-232, 529], ["After making changes in a course on the Teacher side, there are options to preview changes and view live version but currently both options are not working properly and we have to append base url (www.abc.com/) in URL every time to preview changes or view live version.\nWe are using edx open code for providing open(free) courses for users from all over the world.\nBut we are stuck here, please assist us so we can move forward.\nThanks in Advance, \nAnjum\n", "\nI have tried to add base URL in the URL but it is not showing any changes. I have also tried with the following URL: http://www-abc-com.edx.ac.in/courses/\nPlease help me to resolve this issue. Thanks in advance.  \n\n", "if you are in development, ie using settings=devstack ,\ncheck cms/envs/devstack.py \nwhether LMS_BASE = \"your lms url\" normally LMS_BASE = \"localhost:8000\"\nif you are in production, \ncheck /edx/app/edxapp/cms.env.json and put LMS_BASE = \"your lms url\"\n", 0.8239273428916931, 0.8022489547729492, 0.8129435777664185, 0.1379310344827586, 1.0931616654031189e-231, 216], ["So I want to implement a simple comms protocol where reads & writes are completely asynchronous. That means that client sends some data and then server may or may not respond with an answer. So I can't just call reader.read() because that blocks until at least something is returned. And I may have something more to send in the mean time.\nSo is there a way to check if reader has something to read? \n(please note that I'm talking specifically about the streams version: I'm fully aware that protocols version has separate handlers for reading and writing and does not suffer from this issue)\n", "\nYou can use the following code to do what you want:  \n\n  var reader = new StreamReader(new FileStream(filePath, FileMode.Open,  FileAccess.Read,\n  AccessControlType.Allow));\nvar response = reader\n.Result;\nif (response!= null) {\n// do something with response\n}\nreader.Close();\nreturn response;  // or null if there is no response.  This is the default behavior.\n\n", "There is no way to ask reader has incoming data or not.\nI guess to create asyncio.Task for reading data from asyncio stream reader in loop.\nIf you need to write data asynchronously feel free to call StreamWriter.write() from any task that have some outgoing data.\nI strongly dont recommend to use protocols directly -- they are low-level abstraction useful for flow control but for application code is better to use high-level streams.\n", 0.804140567779541, 0.8296611309051514, 0.8167014718055725, 0.1889763779527559, 9.601104024351565e-232, 363], ["I'm currently designing an application using the Twisted framework, and I've hit a bit of a roadblock in my planning.\nFormal Description of the Problem\nMy application has the following constraints:\n\nData arrive in-order, but asynchronously.  I cannot know when the next piece of my data will arrive\nThe order in which data arrive must be preserved throughout the lifespan of the application process.\nAdditional asynchronous operations must be mapped onto this \"stream\" of data.\n\nThe description of my problem may remind people of the Functional Reactive Programming (FRP) paradigm, and that's a fair comparison.  In fact, I think my problem is well-described in those terms and my question can be pretty accurately summarized thusly:  \"How can I leverage Twisted in such a way as to reason in terms of data streams?\"\nMore concretely, this is what I have figured out:\n\nA datum arrives and is unpacked into an instance of a custom class, henceforth referred to as \"datum instance\"\nThe newly-arrived datum instance is appended to a collections.deque object, encapsulated by a custom Stream class.\nThe Stream class exposes methods such as Stream.map that apply non-blocking computations asynchronously to:\n\nAll elements already present in the Stream instance's deque.\nAll future elements, as they arrive.\n\nResults of the operations performed in item 3 are appended to a new Stream object.  This is because it's important to preserve the original data, as it will often be necessary to map several callable's to a given stream.\n\nAt the risk of beating a dead horse, I want to insist upon the fact that the computations being mapped to a Stream instance are expected to return instances of Deferred.\nThe Question\nIncidentally, this precisely where I'm stuck:  I can implement items 1, 2 & 3 quite trivially, but I'm struggling with how to handle populating the results Stream.  The difficulty stems from the fact that I have no guarantees of stream length, so it's completely possible for data to arrive while I'm waiting for some asynchronous operations to complete.  It's also entirely possible for async operation Oi to complete after Oi + n, so I can't just add deque.append as a callback.\nSo how should I approach this problem?  Is there some nifty, hidden feature of Twisted I have yet to discover?  Do any twisty-fingered developers have any ideas or patterns I could apply?\n", " The answer is yes, there is a hidden twistiness to Twisted that you can use to solve this particular problem. The twist is that Twisted is designed to be used in a reactive way, which means that it is possible to use it to model a stream of events. In this case, the events are the asynchronous computationally", "I don't know of any neat tricks that will help you with this.  I think you probably just have to implement the re-ordering (or order-maintaining, depending on how you look at it) logic in your Stream.map implementation.\nIf operation i + 1 completes before operation i then Stream.map will probably just have to hold on to that result until operation i completes.  Then it can add results i and i + 1 to the output Stream.\nThis suggests you may also want to support back-pressure on your input.  The re-ordering requirement means you have an extra buffer in your application.  You don't want to allow that buffer to grow without bounds so when it reaches some maximum size you probably want to tell whoever is sending you inputs that you can't keep up and they should back off.  The IProducer and IConsumer interfaces in Twisted are the standard way to do this now (though something called \"tubes\" has been in development for a while to replace these interfaces with something easier to use - but I won't suggest that you should hold your breath on that).\n", 0.8452626466751099, 0.8098122477531433, 0.8271577954292297, 0.2310756972111554, 9.79628842582894e-232, 311], ["To achieve sense of nativeness in my Pyinstaller packaged Kivy Python application, I would like to create menu item in OSX menu bar. If using PyQt, I can easily achieve this. But as I am using Kivy, it seems like I have to resort to other ways.\nAny suggestion? I guess it will be in the range of PyObjc or AppleScript. Not sure though.\n", "\nI think you can achieve it using Apple Script. I think it is not possible to achieve using Python. You can use Apple script to do it. Here is an example of Apple scripting. It is a simple example. Please note that it uses Python 2.7.1. So if you are using python 3.x, you will have a problem. Also, please note, that this is just an idea. There are many ways to accomplish this task. For example, using QML. Or using Qt. And I don't think that you should use PyInstaller. The reason is that PyKivymac is using a lot of libraries. Pykivmac uses a few libraries, but it doesn't use all of them. In fact, PyMac uses only a couple of the libraries that are available in PyPy. This is why I suggest you to use Python instead of using pyinstall.py. \nHere is the Applescript code that I used to make the menu items. Note that the code is in Python, so it might not work in other languages. However, the idea is to get the same effect as using the PyMenuItem.setItem(...) method. To do this, we need to add a function to the KVMenu class. We will call this function when the user clicks on the item. Then, when we call the function, our menu will look like this:  \n\n  def menu_item_clicked(self, menu, item):\n  \"\"\"\nThis function will get called when a menu is clicked.\n\n\nArgs:\n\n - menu: The menu that was clicked\n - item: the object that is being clicked on\n\"\"\"\n# This function is called by the QMenu object. When the\ndef menuItemClicked(menu,item):\n\n # This will return the index of item that has been clicked,\n # and the text of that item\n return item['index']\nmenuItem = KvMenu(parent=self.parent)\nitem = menu.addMenu('Item', menuItems=menuItems)  # Add the items to menu\nprint item  ## This prints the list of menu objects\nfor itemIndex in item :\n print item[itemIndex]\nif item[\"index\"] == item_index :  ### This checks if the clicked item is equal to its index\n menu = item ['menu']  ##### This adds the new menu to our parent", "\nIf using PyQt, I can easily achieve this. But as I am using Kivy, it\n  seems like I have to resort to other ways\n\nI don't see why you can't just use the PyQt method. Create a separate file that runs the icon that is called by your main Kivy app.\n Like PyQt I am sure that Kivy has an exit function that you could override and have it kill the icon thread on exiting the application. \nOnly potential downside (that I can think of off the top of my head) is that it might run slower than implementing in PyObjC.\n", 0.7937097549438477, 0.8394320011138916, 0.8159308433532715, 0.25877192982456143, 6.829720890670807e-232, 1924], ["I need a way to serialize and unserialize dates that are potentially far away in the past, for instance -10000\nI first look at ISO8601, but it does not seem to support years with more than four digits. (Or at least, python libraries I tried don't.)\nThe different solutions I can think of:\n\nchange the year before serializing/deserializing, give it to the parsing/formatting library, and fix it back (sounds hacky)\ndefine my own format, like year:month:day:hour:minute:second (that is reinventing the wheel, since I have to handle timezones, etc.)\nUse a UNIX timestamp without bounds or something equivalent (may overflow in some programming languages, and still the timezone stuff)\nStore dates before -9999 (or 0) differently than those after, since there was no timezone issue/leap years/\u2026 issue at that time. (two different formats at the same place)\n\nDo you see any other way that would be better than these ones? Or recommand one of those?\n", " The best way is to use the standard library.\nYou can use datetime.datetime, datetimestamps, or datetimes.timestamp to get the date and time in a format that is compatible with the rest of the world. \nIf you want to store dates in your own custom format (which is what I would do), you can do that with datestrings.strptime.\n\n", "You could take a page from the astronomy people. Sky maps they account for long period precession of Earth's spin by establishing epochs. (The sky is different if you're looking now vs 10,000 BC.)\nCreate a new class that has an \"epoch\" number and a facade pattern of your current date class. The new class contains two private fields for epoch and internal-date. Your constructor sets epoch to (year div 10000), and instantiates the internal-date with (year modulo 10000). I hope rest of the facade pattern is as obvious as I think.\n", 0.8306914567947388, 0.8192554712295532, 0.8249338865280151, 0.25477707006369427, 9.949549637422515e-232, 326], ["I'm looking for an implementation of n-grams count vectorization that is more efficient than scikit-learn's CountVectorizer.  I've identified the CountVectorizer.transform() call as a huge bottleneck in a bit of software, and can dramatically increase model throughput if we're able to make this part of the pipeline more efficient.  Fit time is not important, we're only concerned with transform time.  The end output must be a scipy.sparse vector.  If anyone has any leads for potential alternatives it would be much appreciated.\n", "\nI've implemented a CountVectorsizer that uses the sc.signal.lognorm.logit() function to count the number of words in each document. The code is below. I have not yet tested it, but I think it should be faster than the original Count Vectorizer, which is a lot slower than I expected. It is also much more memory efficient, as it uses a single array to store the counts. If you want to try it out, you can download the code from my github page. You can also find the source code on my website.\n\n", "Have you tried HashingVectorizer? It's slightly faster (up to 2X if I remember correctly). Next step is to profile the code, strip the features of CountVectorizer or HashingVectorizer that you don't use and rewrite the remaining part in optimized Cython code (after profiling again).\nVowpal Wabbit's bare-bone feature processing that uses the hashing trick by default might give you a hint of what is achievable.\n", 0.846796989440918, 0.835238516330719, 0.8409780859947205, 0.28048780487804875, 9.32667305882306e-232, 495], ["I am a non-programmer who started to learn Python. My Mac OS X Yosemite shipped with Python 2.7.6. I installed Python 3.4.2 too. If I use pip or easy_install in the terminal to install a package, how do I know which Python I installed the package in? It seems Python 3.4.2 shipped with pip and easy_install, but I think Python 2.7.6 may also have some version of pip or easy_install. I know my system can have both versions of Python, but can it have multiple versions of pip or easy_install?\n", "\nYou can use the following command to find out which version you have installed: python -V\nIf you want to know the version that shipped in your system, you can run the command: pip --version\nThe version in pip is the one that was shipped by Apple.\nFor more information, see the documentation for pip.\n\n", "There's an easy way around it - use pip2 or pip2.7 or pip-2.7 for Python 2, and pip3 or pip3.4 or pip-3.4 for Python 3. Both version ship with easy_install, but Python 2 does not contain pip by default - you have to install it yourself.\n", 0.8455296754837036, 0.8410985469818115, 0.8433082699775696, 0.22429906542056074, 9.772182971647901e-232, 302], ["I had python2.6 on my linux box but installed python3.4 to use new modules. I installed it using sudo access. The new version was installed in /usr/local/bin. Without root access, I can use the new python3.4, both by just using python3.4 in the command line or using the shebang in the .py file #!/usr/local/bin/python3\nNow I am trying to install a module, for which I need sudo access. When I am root, and I run python3.4, it says command not found. I ran whereis python and found the path to python2.6 in /usr/bin, but whereis python3.4 as root gives, not found in /usr/bin, which is correct since it is in /usr/local/bin. Again, if I exit from root, I have no trouble using python3.4\nThis seems like a $PATH issue (not sure), can some one help me what I am doing wrong while installing the module for the new python3.4? I was able to install the module, but it was installed in the old python2.6 site-packages.\n", "I think you are using a different version of python than the one you installed. You need to uninstall the python you have installed and install python 3.5.2, then you can install your module. If you want to do it from the terminal, you need the sudo command. To install it, use: sudo apt-get install <package name>\nIf you don't want the root user to have access to the system, install sudoers. Then you will be able use sudo to run commands as the user you created.\n\n", "How about export PATH+=:/usr/local/bin, try it, maybe helpful.\n", 0.8179900646209717, 0.8229608535766602, 0.8204678893089294, 0.0196078431372549, 8.674249582128845e-232, 467], ["I had python2.6 on my linux box but installed python3.4 to use new modules. I installed it using sudo access. The new version was installed in /usr/local/bin. Without root access, I can use the new python3.4, both by just using python3.4 in the command line or using the shebang in the .py file #!/usr/local/bin/python3\nNow I am trying to install a module, for which I need sudo access. When I am root, and I run python3.4, it says command not found. I ran whereis python and found the path to python2.6 in /usr/bin, but whereis python3.4 as root gives, not found in /usr/bin, which is correct since it is in /usr/local/bin. Again, if I exit from root, I have no trouble using python3.4\nThis seems like a $PATH issue (not sure), can some one help me what I am doing wrong while installing the module for the new python3.4? I was able to install the module, but it was installed in the old python2.6 site-packages.\n", "I think you are using a different version of python than the one you installed. You need to uninstall the python you have installed and install python 3.5.2, then you can install your module. If you want to do it from the terminal, you need the sudo command. To install it, use: sudo apt-get install <package name>\nIf you don't want the root user to have access to the system, install sudoers. Then you will be able use sudo to run commands as the user you created.\n\n", "Well you could have given the location to install Py 3.4 to be in /usr/bin. \nAn easy approach could be to copy the Py 3.4 bin to /usr/bin from /usr/local/bin.\nSecondly You can also install again with the prefix params.\n", 0.8424327969551086, 0.8324686288833618, 0.8374210596084595, 0.23357664233576642, 8.849575533106213e-232, 467], ["I'm using regex to find occurrences in a string python, for example: \"cw cx\", \"cw cx av\", \"cw cx dr wt\" or \"cw cx qw hv eb\". The match can be 2, 3, 4 or 5 pairs. I'm thinking something like this \"([a-zA-Z]{2}) ([a-zA-Z]{2})\" Please help me\n", "\nYou can use the following regex: \nre.findall(r'(?<=\\w)([a\\d])(?=\\\\w)', 'cx av dr qh eb')\nThis will return: ['c', ['x', ''], ['a', None], 'dr', [''], [None, None, '']]\nIf you want to match the whole string, you can do: re.search(\nr\"(?s)^(?:(?!\\s)(?:[^\\r\\n]*\\S+))(.*)$\", 'av dr')\n\n", "You are almost there. Try this: ([a-zA-Z]{2} ){1,4}[a-zA-Z]{2}\n", 0.7871842384338379, 0.8362934589385986, 0.8109961152076721, 0.12307692307692307, 9.753281852319978e-232, 280], ["I'm sure this is a very newb question, so I apologize in advance. I'm trying to use ipython notebook for a group project. The program we are building is fairly large and pulls in a large number of external datasets. Much of the time, Ipython seems to stop working. I'll try to run a cell or multiple cells and nothing will happen (except a little asterisk * will appear in the brackets [] to the left of the cell). Even if I try to just add a new cell and execute 2+2, nothing will happen. What is going on here? How do I fix this? Thanks!\n", "\nI'm not sure what you mean by \"stopped working\". I think you are using ipykernel. If you have a problem with ipynb, you can try the following:1. Delete the ipydb folder2. Run ipythondb3. Restart ipyrb4.\nIf you still have problems, please post a question on the IPython IRC channel. You can also try posting a bug report on https://bugs.python.org/\nBest,\n--\nMichael\nhttp://www.michael-m.me/", "The asterisk next to a cell [*] indicates that the cell is currently executing. While IPython provides each notebook with it's own kernel, there is only one kernel per notebook. When that kernel is busy executing code (either a cell, or a series of cells) it cannot accept or run any further code until what it is currently doing is finished. New executions sit in a queue, until the kernel is ready.\nIf you wait long enough after trying to execute 2+2 you should find that it will eventually execute (assuming your main code ever exits).\nThe solution to this depends on your code, and how long you're willing to wait to get the results. As a general rule try the following:\n\nUse a smaller data set to test the algorithm, then scale up gradually noting the increase in time. Is it going to be feasible with your full dataset?\nIs your algorithm reading/writing to the disk? Can you avoid it, or pre-load/post-save state?\nIs it possible to split your data into batches?\nIf your algorithm is batchable, can you parallelize it to make best use of your CPU?\n\nYou can interrupt the kernel, however this will not work if the execution is currently out of the kernel's hands e.g. in external C modules (a lot of numpy for example). In these cases you may need to restart completely.\n", 0.8122649788856506, 0.8016853332519531, 0.806940495967865, 0.17532467532467533, 1.0041944042528399e-231, 390], ["I am using Ubuntu 14.04 and trying to run snoopy_auth which is a part of the snoopy-ng application I downloaded and installed from their GitHub.\nWhen running, I get an error that is documented on snoopy-ng's GitHub page, which says that it works using version 0.7.8.\nHow can I downgrade sqlalchemy to 0.7.8?\nThe error looks like:\nsnoopy_auth -l\n[+] Available drone accounts:\nTraceback (most recent call last):\n  File \"/usr/bin/snoopy_auth\", line 103, in \n    drones = auth_.manage_drone_account(\"foo\", \"list\")\n  File \"/usr/bin/snoopy_auth\", line 29, in manage_drone_account\n    self.db.create(self.drone_tbl_def )\n  File \"\", line 2, in create\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/deprecations.py\", line 106, in warned\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/interfaces.py\", line 859, in create\n    raise NotImplementedError()\nNotImplementedError\n", " The answer is to install sqlite3.  I did this by following the instructions here: https://www.sqlite.org/faq.html#faqs-installation\nI then ran the following command: sudo apt-get install libsql-dev\nThen I ran sudo pip install --upgrade sqlAlchemy==0.9.0\nAnd then I installed the latest version of snoopy-auth by running sudo easy_install snooopyauth\nNow I can run the application and it is working.\n\n", "To get passed this error I just simply ran the command:\nsudo easy_install \"SQLAlchemy==0.7.8\"\nThe virtual environments do seem like the preferred method though, so hopefully I don't run into any additional problems from downgrading system-wide.\n", 0.8514894843101501, 0.8639881610870361, 0.8576932549476624, 0.2831858407079646, 9.757773105395584e-232, 401], ["I've been installing few Library/Toolkit for Python like NLTK, SciPy and NumPy on my Ubuntu. I would like to try to use Anaconda distribution though. Should I remove my existing libraries before installing Anaconda?\n", "\nYes, you should remove your existing Python libraries.\nYou can install Anconda with the following command: \ncurl -L https://repo.continuum.io/miniconda/Minicondam-4.1.0-Linux-x86_64.sh -O- | sudo bash\nThen you can use the AniConda GUI to install the packages. \n\n", "There is no need to remove your system Python. Anaconda sits alongside it. When it installs, it adds a line to your .bashrc that adds the Anaconda directory first in your PATH. This means that whenever you type python or ipython in the terminal, it will use the Anaconda Python (and the Anaconda Python will automatically use all the Anaconda Python libraries like numpy and scipy rather than the system ones). You should leave the system Python alone, as some system tools use it. The important points are:\n\nWhichever Python is first on your PATH is what gets used when you use Python in the terminal. If you create a conda environment with conda and use source activate it will put that environment first on the PATH. \nEach Python (Anaconda or the system) will use its own libraries and not look at the others (this is not true if you set the PYTHONPATH environment variable, but I recommend that you don't).\n\n", 0.8147113919258118, 0.8203673958778381, 0.8175296187400818, 0.13526570048309178, 1.0842960393333135e-231, 263], ["I have been trying to build a prediction model using a user\u2019s data. Model\u2019s input is documents\u2019 metadata (date published, title etc) and document label is that user\u2019s preference (like/dislike). I would like to ask some questions that I have come across hoping for some answers:\n\nThere are way more liked documents than disliked. I read somewhere that if somebody train\u2019s a model using way more inputs of one label than the other this affects the performance in a bad way (model tends to classify everything to the label/outcome that has the majority of inputs\nIs there possible to have input to a ML algorithm e.g logistic regression be hybrid in terms of numbers and words and how that could be done, sth like:\ninput = [18,23,1,0,\u2019cryptography\u2019] with  label = [\u2018Like\u2019]\nAlso can we use a vector ( that represents a word, using tfidf etc) as an input feature (e.g. 50-dimensions vector) ?\nIn order to construct a prediction model using textual data the only way to do so is by deriving a dictionary out of every word mentioned in our documents and then construct a binary input that will dictate if a term is mentioned or not? Using such a version though we lose the weight of the term in the collection right?\nCan we use something as a word2vec vector as a single input in a supervised learning model?\n\nThank you for your time.\n", "Thank you very much for the answers. The first question is a bit tricky. We can use the word vector to represent a document. But we can\u201dt use it as input for a classification model. For example, if we have a collection of documents, we will have to use some kind of a feature vector that is derived from the words in each document and we need to train a classifier on that. So, the answer to your first part is yes. You can also use word vectors as features for supervised classification. However, you need a lot of training data to get a good model, and you will need some sort of regularization to prevent overfitting. In order for that to work, it is important to know what kind and what type of features you are using. If you use tf-idfs, then you can get rid of some of your features. Also, in order not to overfit, your model should be able to learn from a large amount of data, so you should use enough training examples. This is why you have the regularizer. Another way of doing it would be to take the tfidsf and use that as the input of an ML model and train it on a small amount (say, 10) of examples, but you would need more training samples to make it work. \nThe second part", "\nYou either need to under-sample the bigger class (take a small random sample to match the size of the smaller class), over-sample the smaller class (bootstrap sample), or use an algorithm that supports unbalanced data - and for that you'll need to read the documentation.\nYou need to turn your words into a word vector. Columns are all the unique words in your corpus. Rows are the documents. Cell values are one of: whether the word appears in the document, the number of times it appears, the relative frequency of its appearance, or its TFIDF score. You can then have these columns along with your other non-word columns.\n\nNow you probably have more columns than rows, meaning you'll get a singularity with matrix-based algorithms, in which case you need something like SVM or Naive Bayes.\n", 0.8295457363128662, 0.8346394896507263, 0.8320847749710083, 0.32171581769436997, 7.754795777430476e-232, 1188], ["When I use R and execute a command in the command window, I can just press up and I get the last executed command in the current command line. What key do I need to press in order to get the same in Python? \nEdit: When I asked this question, I did not know what IDLE or iPython is. I just googled \"download python\" and downloaded it. But now I know I am using IDLE.\n", "\nYou can use the following code to execute the commands in a Python script:  \n\n  import subprocess\n  subproces = subproc.Popen(['python','script.py'], stdout=subprocess.DEVNULL, stderr=None)\n#  print(subprocess.communicate())\nsubproc = None\nwhile True: # wait for the sub process to terminate\ntry: sub =subp = proces.stdout.read()\nexcept: break\nif sub!= None: print sub\nelse: raise\nprint(proce = proc.wait()) # print the process ID\nprocs = [proc for proc in sub.split('\\n') if proc!= 'exit']\nfor proc, procs in enumerate(proccs):\nproc_id = int(str(int(sys.argv[1])))\nsys_arg = sys.path[:proc] + sys_path + '\\python' + proc + '.py'+ proce\ncmd = 'python '+sys_.argvs[proc+1]+' '\npopen = open(cmd, 'w')\nstdin = popen.dup\nstderin=popt.Stderror() # get sterr from the child process\nout = stdin.write(sterrin) # write stdr to the parent process stdio\ninp=stdio.open('infile.txt', mode='r') # open the input file\nf = infile.input()  # read the file into a string\ns = f.strip() + infil.getlines()[0] # strip the first line\nn = len(s)  ## number of lines in file  (number of columns)\n\n", "If you're in IDLE, use Alt+P and Alt+N to navigate your command history.\n", 0.7295507192611694, 0.820116400718689, 0.7721871137619019, 0.06315789473684211, 7.444500602574024e-232, 1076], ["When I use R and execute a command in the command window, I can just press up and I get the last executed command in the current command line. What key do I need to press in order to get the same in Python? \nEdit: When I asked this question, I did not know what IDLE or iPython is. I just googled \"download python\" and downloaded it. But now I know I am using IDLE.\n", "\nYou can use the following code to execute the commands in a Python script:  \n\n  import subprocess\n  subproces = subproc.Popen(['python','script.py'], stdout=subprocess.DEVNULL, stderr=None)\n#  print(subprocess.communicate())\nsubproc = None\nwhile True: # wait for the sub process to terminate\ntry: sub =subp = proces.stdout.read()\nexcept: break\nif sub!= None: print sub\nelse: raise\nprint(proce = proc.wait()) # print the process ID\nprocs = [proc for proc in sub.split('\\n') if proc!= 'exit']\nfor proc, procs in enumerate(proccs):\nproc_id = int(str(int(sys.argv[1])))\nsys_arg = sys.path[:proc] + sys_path + '\\python' + proc + '.py'+ proce\ncmd = 'python '+sys_.argvs[proc+1]+' '\npopen = open(cmd, 'w')\nstdin = popen.dup\nstderin=popt.Stderror() # get sterr from the child process\nout = stdin.write(sterrin) # write stdr to the parent process stdio\ninp=stdio.open('infile.txt', mode='r') # open the input file\nf = infile.input()  # read the file into a string\ns = f.strip() + infil.getlines()[0] # strip the first line\nn = len(s)  ## number of lines in file  (number of columns)\n\n", "For anyone else who comes upon this looking for a mac solution, I found that Contrl+Option+n and Contrl+Option+n worked to cycle through previously executed lines in IDLE (Python) 2.7.10 on OS 10.8.5. \nI realize that this should work without the Option, but for months I needed to use both modifier keys to execute the shortcut properly. \n", 0.7403563261032104, 0.8131539821624756, 0.7750495076179504, 0.14285714285714285, 7.897709495635594e-232, 1076], ["The Question:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.\nWhy\nI find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  \nI can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:\n\nCall str(sympyResult)\nDo custom things with regular expressions to get this string to look like valid python code\nwrite this python code to a file\n\nI note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  \nI know of several possible but problematic ways around this problem:\n\nI know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:\n\ndependency: my code now depends on sympy to run.  This is bad.\nefficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need evalf every time.\n\nI also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.\n\nEdit: Summary\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.\nThat doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  \n", "The Answer: I think the answer is that there are no easy ways to do this.\n\n", "If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use lambdify to evaluate it.  This will be much faster than using evalf, especially if you use numpy. \nYou could also look at using the printer in sympy.printing.lambdarepr directly, which is what lambdify uses to convert an expression into a lambda function. \n", 0.866428792476654, 0.8040859699249268, 0.8340940475463867, 0.11627906976744186, 1.2751495852793276e-231, 75], ["In my app I need to know how big the canvas is in pixels.\nInstead calling canvas.size returns [100,100] no matter how many pixels the canvas is wide.\nCan you please tell me a way to get how many pixels the canvas is wide and high?\n", "\nYou can use the following code to find the size of the Canvas: \nvar canvas = document.getElementById('canvas');\ncanvasesize = canvas[0].getBoundingClientRect().width;\ndocument.body.appendChild(canvaesize);\n// or\nwindow.onresize=function(){\n   canvas=document;canvsize=(canvar.width||can.height);canvan.style.display='block';\n}\n<can id=\"can\">\n</can>\nThanks!\nPS: I'm using the latest version of Chrome.\n\n", "There is no position nor size in a Canvas. Canvas act just as a container for graphics instructions, like Fbo that draw within a Texture, so it have a size.\nIn Kivy, Canvas.size doesn't exists, but i guess you called your widget a canvas. By default, a Widget size is 100, 100. If you put it into a layout, the size will be changed, when the layout will known its own size. Mean, you need to listen to the changes of the Widget.size, or use a size you known, like Window.size.\n", 0.790550708770752, 0.8354471921920776, 0.8123791217803955, 0.2119205298013245, 9.594503055152632e-232, 403], ["I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?\nOr is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?\nThanks.\n", "\nThe doc-2-vec model is trained on the entire corpus, so it is not possible to use the pretrained vectors for the document-level vectors. However, you can use a preprocessed version of the vectors, which is available in the Gluon python package. The preprocessing step is described in this blog post.\n\n", "Note that the \"DBOW\" (dm=0) training mode doesn't require or even create word-vectors as part of the training. It merely learns document vectors that are good at predicting each word in turn (much like the word2vec skip-gram mode). \n(Before gensim 0.12.0, there was the parameter train_words mentioned in another comment, which some documentation suggested will co-train words. However, I don't believe this ever actually worked. Starting in gensim 0.12.0, there is the parameter dbow_words, which works to skip-gram train words simultaneous with DBOW doc-vectors. Note that this makes training take longer \u2013 by a factor related to window. So if you don't need word-vectors, you may still leave this off.)\nIn the \"DM\" training method (dm=1), word-vectors are inherently trained during the process along with doc-vectors, and are likely to also affect the quality of the doc-vectors. It's theoretically possible to pre-initialize the word-vectors from prior data. But I don't know any strong theoretical or experimental reason to be confident this would improve the doc-vectors. \nOne fragmentary experiment I ran along these lines suggested the doc-vector training got off to a faster start \u2013 better predictive qualities after the first few passes \u2013 but this advantage faded with more passes. Whether you hold the word vectors constant or let them continue to adjust withe the new training is also likely an important consideration... but which choice is better may depend on your goals, dataset, and the quality/relevance of the preexisting word-vectors. \n(You could repeat my experiment with the intersect_word2vec_format() method available in gensim 0.12.0, and try different levels of making preloaded vectors resistant-to-new-training via the syn0_lockf values. But remember this is experimental territory: the basic doc2vec results don't rely on, or even necessarily improve with, reused word vectors.) \n", 0.8551025390625, 0.7987203001976013, 0.8259502649307251, 0.16533333333333333, 1.0141590892147852e-231, 302], ["What is the significance of the body of the __iter__ method in a class definition in Python? \nIt is my understanding that an __iter__ method must return an iterator; then shouldn't every __iter__ method only contain a return statement which returns an iterator (defined elsewhere) ? Why is there a body in the __iter__ method in some cases?\n", "The __getitem__ and __setitem_self__ methods are not defined in __init__. They are defined as methods of a subclass of object.\nclass MyClass(object):\n   def __new__(cls, name, *args, **kwargs):  # <-- __call__\n# ...\ndef __add__(*args)\n...\n__getattr__ = __delattr_unused__  // <--- __dict__ is not a method of MyObject\nMyObject.__dict_from_class__[MyClass] = MyObj\nreturn MyObj\nThis is a very simple example, but it illustrates the point.  The __class_methods attribute of an object is defined to be a list of methods that are available to the object, and the methods in this list are called by the class. The methods defined here are __method_missing, __func__, etc. These methods can be called on any object that inherits from the Myclass class, so the list is actually a set of all the available methods. In this case, the method __my_add_1__ was called, which is an instance method. This method is called when the instance is created, not when it is used. It is also called in other cases, such as when a new instance of this class is assigned to a variable. So, in order to call this method, you must first create an actual instance, then call the my_object.myadd method on it.\n\n", "I don't quite get what you are meaning by body. I suppose you are talking about the definition of the method __iter__.\nWell sometimes you could use yield to return a generator. In this case, you could write the generating logic in the body of __iter__.\nOr returning another iterator might involve some logic. So it's not always simply a return.\n", 0.8009691834449768, 0.8475407958030701, 0.8235971331596375, 0.16541353383458648, 7.270134592908434e-232, 1183], ["From what I read the dynamic and big data rendering capabilities of Bokeh are accessible by use of the bokeh-server.\nIn Bokeh documentation there is a brief mention of embedding the bokeh-server in a Flask application using the blueprint api. This option retains all of the Bokeh's dynamic and big data rendering capabilities.\nI would like to do the same for a django1.7 application.\nHas this been done? \nIf so are there any examples?\n", "\nThe bookeph server is not available in the current version of djangorestframework. It is available as a separate package. The boKEPH server can be found here: https://github.com/bokep/bookephal/blob/master/server/src/main/java/org/openfe/fe.server\nYou can install the server using pip: pip install boakeph\nAlternatively you can use the command line: python -m boakph.Server\nThis is the code that is used to run the BookePhal server:  \n\n  from boikeph import BoakePhAlive\n  import boohp.BookePHAliveserver\n#  BoKEPServer.start()\nfrom boikphal.boakephal import Server\nserver = BoakPhALiveServer(port=8080)\nprint server.get_status()\n\n", "No need to reinvent.\n\nbokeh-server is a webserver and can listen on arbitrary port.\nyou can have your django webserver listen on some other arbitrary port.\n... and integrate the two:\n\nhave redirects from django to bokeh-server, or\nwebserver in front, e.g. nginx which does reverse-proxy.\n\n\n", 0.7881938219070435, 0.8294222354888916, 0.8082826137542725, 0.16551724137931034, 8.358311678989959e-232, 632], ["Is there an editor that I could install on the Raspbian OS to practice Regex with? If not, what about through Python? If so, is there a good python IDE out there for Raspbian that supports Regexs?\n", "\nI would recommend using the Python IDE for Regextools. It is a very good IDE and has a lot of features. I have used it for a long time and it is very easy to use.\nYou can find it here: http://regexpython.org/\nIt is free and open source. You can download it from here.\n\n", "Python itself supports regexes (via a built-in module). If you're just interested in playing around with them, import re in an interactive shell, and you have access to Python's regular expression engine.\n", 0.840140700340271, 0.8481737375259399, 0.8441380858421326, 0.15384615384615385, 1.0148707121437341e-231, 270], ["On my desktop PC I have anaconda installed, and on my laptop - to save space - I thought i'd install miniconda and be selective about the modules I install. So I installed a handful, numpy, scipy etc. I didn't install anything which isn't part of the default anaconda install, but I just realized my miniconda install is taking up more space than the anaconda install! (1.8GB vs 2.2GB). (no environments in either)\nThe bulk of the difference comes from the pkgs folder. The miniconda install seems to have the tar.bz2 of all of the installed packages as well as the exploded versions. Are these safe to delete? Will they be deleted automatically after a while? Is there an option to not cache these?\nP.S. I'm developing on both windows and mac (i've tried installed anaconda and miniconda on both mac and windows to see, and I get very similar results). \n", "\nI'm not sure if you can delete the packages from minconda, or if it's just a matter of deleting the files in the package folder, which is what I would do.\nIf you want to keep the cache, you could try to remove the cached packages, then delete them from your system. \n\n", "You can safely delete the tar.bz2 files. They are only used as a cache. The command conda clean -t will clean them automatically. \n", 0.8705700635910034, 0.8646450042724609, 0.8675974011421204, 0.23076923076923078, 9.513224649144346e-232, 269], ["Are conda's envs a wrapper around pyenv, a similar version or are they a distinct implementation?  They seem identical, but I do not know for sure.  Are they interchangeable?  \n", "  pyenvs is a python environment that wraps an existing environment.\n  condas env is an environment for condab.py. It is not a py environment,\nbut it is similar to py environ.\n\n", "Python virtualenvs (or pyvenvs) are very Python specific. The create an environment that pretends to be a distinct installation of Python, while reusing some data from the base Python. Only Python packages can be installed into a virtualenv, because that's really the only thing that makes sense. \nConda environments are not Python specific. Any kind of package can be a conda package (Python packages, C libraries, R packages, Python itself, ...), and any conda package can be installed into a conda environment. Each conda environment is essentially a completely separate installation of every file from every package (including all of Python itself). This is done efficiently using hard links. \n", 0.8600614666938782, 0.8216800689697266, 0.8404328227043152, 0.17142857142857143, 1.116863271275353e-231, 177], ["I am trying to use the timeit module to time the speed of an algorithm that analyzes data.\nThe problem is that I have to do run some setup code in order to run this algorithm. Specifically, I have to load some documents from a database, and turn it into matrix representation. \nThe timeit module does not seem to allow me to pass in the matrix object, and instead forces me to set this up all over again in the setup parameter. Unfortunately this means that the running time of my algorithm is fuzzed by the running time of pre-processing.\nIs there some way to pass in objects that were created already, to timeit in the setup parameter? Otherwise, how can I deal with situations where the setup code takes a nontrivial amount of time, and I don't want that to fuzz the code block I actually am trying to test?\nAm I approaching this the wrong way?\n", "\nI think you are approaching it the right way. I think the best way is to create a function that takes the data and the matrices, then call that function in your setup function. This way you can pass the objects you created in setup to the function, so that it can time them. You can also pass a list of objects to your function to make it easier to call it.\n\n", "The time it takes to run the setup code doesn't affect the timeit module's timing calculations.\nYou should be able to pass your matrix into the setup parameter using import, eg \n\"from __main__ import mymatrix\"\n", 0.8659003973007202, 0.8399801850318909, 0.8527433276176453, 0.25688073394495414, 9.444436160813523e-232, 360], ["I'm running Django with Postgres database. On top of application-level security checks, I'm considering adding database-level restrictions. E.g. the application code should only be able to INSERT into log tables, and not UPDATE or DELETE from them. \nI would manually create database user with appropriate grants for this. I would also need a more powerful user for running database migrations.\nMy question is, do people practice things like this? Any advice, best practices on using restricted database users with Django? \nEdit: To clarify, there's no technical problem, I'm just interested to hear other people's experiences and takeaways. One Django-specific thing is, I'll need at least two DB users: for normal operation and for running migrations. Where do I store credentials for the more privileged user? Maybe make manage.py migrate prompt for password? \nAs for the reasoning, suppose my app has a SQL injection vulnerability. With privileged user, the attacker can do things like drop all tables. With a more limited user there's slightly less damage potential and afterwards there's some evidence in insert-only log tables. \n", "\nThe best practice is to use a database with a restricted user. The user should be granted the necessary privileges to perform the desired operations. This is the best way to prevent SQL Injection. If you need to restrict the user to only perform insert operations, you can use the Django ORM to do that. You can also use Django's built-in database authentication. For example, Django has the django.contrib.auth.models.User model that can be used to authenticate users. It has an attribute called 'is_staff' that returns True if the current user is a staff member. So you could do something like:  \n\n  class User(AbstractUser):\n  is_active = models.BooleanField(default=True)\nisstaff = dj_api.django_contribute_auth_models._UserManager.get_user_model().get(\n'isStaff', False)\n\n", "Yes, this is practiced sometimes, but not commonly. The best way to do it is to grant specific privileges on user, not in django. Making such restrictions means that we should not trust application, because it might change some files / data in db in the way that we do not expect it to do so.\nSo, to sum up: create another user able to create / modify data and user another one with restrictions to use normally.\nIt's also quite common in companies to create one user to insert data and another one for employees / scripts to access it.\n", 0.8073458671569824, 0.8481736183166504, 0.8272562623023987, 0.29694323144104806, 8.390356912059446e-232, 778], ["I'm running Django with Postgres database. On top of application-level security checks, I'm considering adding database-level restrictions. E.g. the application code should only be able to INSERT into log tables, and not UPDATE or DELETE from them. \nI would manually create database user with appropriate grants for this. I would also need a more powerful user for running database migrations.\nMy question is, do people practice things like this? Any advice, best practices on using restricted database users with Django? \nEdit: To clarify, there's no technical problem, I'm just interested to hear other people's experiences and takeaways. One Django-specific thing is, I'll need at least two DB users: for normal operation and for running migrations. Where do I store credentials for the more privileged user? Maybe make manage.py migrate prompt for password? \nAs for the reasoning, suppose my app has a SQL injection vulnerability. With privileged user, the attacker can do things like drop all tables. With a more limited user there's slightly less damage potential and afterwards there's some evidence in insert-only log tables. \n", "\nThe best practice is to use a database with a restricted user. The user should be granted the necessary privileges to perform the desired operations. This is the best way to prevent SQL Injection. If you need to restrict the user to only perform insert operations, you can use the Django ORM to do that. You can also use Django's built-in database authentication. For example, Django has the django.contrib.auth.models.User model that can be used to authenticate users. It has an attribute called 'is_staff' that returns True if the current user is a staff member. So you could do something like:  \n\n  class User(AbstractUser):\n  is_active = models.BooleanField(default=True)\nisstaff = dj_api.django_contribute_auth_models._UserManager.get_user_model().get(\n'isStaff', False)\n\n", "For storing the credentials to the privileged user for management commands, when running manage.py you can use the --settings flag, which you would point to another settings file that has the other database credentials.\nExample migrate command using the new settings file:\npython manage.py migrate --settings=myapp.privileged_settings\n", 0.8183360695838928, 0.8450570106506348, 0.8314819931983948, 0.1878453038674033, 8.13961470614854e-232, 778], ["Hi I am getting the error below when going to the website url on ubuntu server 14.10 running apache 2 with mod_wsgi and python on django.\nMy django application uses python 3.4 but it seems to be defaulting to python 2.7, I am unable to import image from PIL and AES from pycrypto.\n\nImportError at /  cannot import name _imaging Request\n  Method:   GET Request URL:    Django Version:   1.7.3\n  Exception Type:   ImportError Exception Value:    cannot import\n  name _imaging Exception\n  Location: /usr/local/lib/python3.4/dist-packages/PIL/Image.py in\n  , line 63 Python Executable: /usr/bin/python Python\n  Version:  2.7.6 Python Path:  ['/var/www/blabla', \n  '/usr/local/lib/python3.4/dist-packages', \n  '/usr/lib/python2.7', \n  '/usr/lib/python2.7/plat-x86_64-linux-gnu', \n  '/usr/lib/python2.7/lib-tk',  '/usr/lib/python2.7/lib-old', \n  '/usr/lib/python2.7/lib-dynload', \n  '/usr/local/lib/python2.7/dist-packages', \n  '/usr/lib/python2.7/dist-packages',  '/var/www/blabla', \n  '/usr/local/lib/python3.4/dist-packages']\n\n", "I have tried to install the python-imaged package from the official site and it worked. I have also tried installing the package using pip install imaged and that also worked, but I don't know why.  I also have installed the pycurl package and I can import it from my python script. But I still get the same error. Any ideas?\nThanks\nEDIT: I tried the following: sudo apt-get install python3-pycredential and sudo pip3 install py3crc. It installed and installed successfully. However, when I try to run the script, it gives me the above error and the browser says \"The page cannot be displayed because the server responded with a status of 404 (Not Found)\".\nThe python version is 3, the imager version 2\nEdit2: The error is still the exact same. The browser is saying \"404 Not Found\"\nHere is the code: https://pastebin.com/X5ZYXZzF\nAnd here is my apachetop.conf file", "I believe that mod_wsgi is compiled against a specific version of python, so you need a py3.4 version of mod_wsgi. You may be able to get one from your os's package repository or you can build one without too much drama. From memory you'll need gcc and python-dev packages (python3-dev?) to build.\nOK, quick google, for ubuntu 14.10: sudo apt-get install libapache2-mod-wsgi-py3 should install a py3 version of mod_wsgi (will probably want to remove the existing py2 version).\nAdding a shebang line won't do any good as the python interpreter is already loaded before the wsgi.py script is read.\n", 0.8320622444152832, 0.8449763059616089, 0.8384695649147034, 0.24354243542435428, 8.500613312998594e-232, 865], ["So I have this little problem. When I try to install PyGame for Python 3.4 I download a .whl (wheel?) file and don't know how to use it. Some guys told me something about pip but don't know how to use/install it.\n", "You can use pip to download the wheel file. You can then use the pip command to run the installation. For example: pip install pygame\nIf you want to know more about the pygames wheel, you can check out the Pygame wheel page.\n\n", "14 y/o? Good for you! You can put the file into your python/scripts folder and run pip install *file* (where *file* is your filename).\n", 0.8669379353523254, 0.8402571678161621, 0.8533891439437866, 0.2608695652173913, 1.0176229291719042e-231, 226], ["Many spreadsheets have formulas and formatting that Python tools for reading and writing Excel files cannot faithfully reproduce. That means that any file I want to create programmatically must be something I basically create from scratch, and then other Excel files (with the aforementioned sophistication) have to refer to that file (which creates a variety of other dependency issues).\nMy understanding of Excel file 'tabs' is that they're actually just a collection of XML files. Well, is it possible to use pandas (or one of the underlying read/write engines such as xlsxwriter or openpyxl to modify just one of the tabs, leaving other tabs (with more wicked stuff in there) intact?\nEDIT: I'll try to further articulate the problem with an example.\n\nExcel Sheet test.xlsx has four tabs (aka worksheets): Sheet1, Sheet2, Sheet3, Sheet4\nI read Sheet3 into a DataFrame (let's call it df) using pandas.read_excel()\nSheet1 and Sheet2 contain formulas, graphs, and various formatting that neither openpyxl nor xlrd can successfully parse, and Sheet4 contains other data. I don't want to touch those tabs at all.\nSheet2 actually has some references to cells on Sheet3\nI make some edits to df and now want to write it back to sheet3, leaving the other sheets untouched (and the references to it from other worksheets in the workbook intact)\n\nCan I do that and, if so, how?\n", " Yes, you can. You can use the pandastatement module to manipulate the tab contents of a worksheet. The module is available from the Python Package Index. It's a bit of an overkill, but it's worth it. Here's an excerpt from that page:\nimport pandastyles\nfrom pandasta import tab_names\ntab_name = pandaste.tabname(worksheet)  # tab name\nworksheet = tabname.get_workshape()  \n# get the workshapes of all the sheets\nsheet_list = workschemes.values()[:2]\nfor sheet in sheet_List:  ## loop through all sheets in workbooks\n  works = sheet.sheetnames  ### get sheet names\nprint(tabnames(sheet))\n## get all tab names from works\nnames = []\ncount = 0\nwhile count < len(names):\nname_index = names.index(count) ## get index of tab\nindex_of_tab = name_Index[name]  // get tab index\nif index_Of_Tab == None: ## if tab is not found\ncontinue\nelse: # if it is found, get its name and index, then get a list of names and indexes\nnew_sheet", "I'm 90% confident the answer to \"can pandas do this\" is no.  Posting a negative is tough, because there always might be something clever that I've missed, but here's a case: \nPossible interface engines are xlrd/xlwt/xlutils, openpyxl, and xlsxwriter.  None will work for your purposes, as xlrd/wt don't support all formulae, xlsxwriter can't modify existing xlsx files, and openpyxl loses images and charts.\nSince I often need to do this, I've taken to only writing simple output to a separate file and then calling the win32api directly to copy the data between the workbooks while preserving all of my colleague's shiny figures.  It's annoying, because it means I have to do it under Windows instead of *nix, but it works.\nIf you're working under Windows, you could do something similar.  (I wonder if it makes sense to add a native insert option using this approach to help people in this situation, or if we should simply post a recipe.)\n\nP.S.: This very problem has annoyed me enough from time to time that I've thought of learning enough of the modern Excel format to add support for this to one of the libraries.\nP.P.S.: But since ignoring things you're not handling and returning them unmodified seems easy enough, the fact that no one seems to support it makes me think there are some headaches, and where Redmond's involved I'm willing to believe it.  @john-machin would know the details, if he's about..\n", 0.7813875079154968, 0.7993792295455933, 0.7902810573577881, 0.2248803827751196, 8.213102934857874e-232, 920], ["I have an array of graysale image read in from a color one. If I use matplotlib to imshow the grayscale image, it looks just fine. But when I io.imsave it, it's ruined (by an outrageous amount of noise). However, if I numpy.around it first before io.imsave-ing, then it's significantly better, but black and white are still all swapped (dark regions appear white, and bright regions appear dark)\nscipy.misc.imsave, on the other hand, works perfectly.\nThank you.\n", "\nI'm not sure what you mean by \"black and whites are all switched\". I think you are referring to the fact that the image is not black-and-white, rather it is a grayed-out image. The reason why this happens is because the color channels are not the same. In the case of a black image the red channel is black, the green channel has a very small amount (less than 1%) of green, etc. This is why the colors are swapped. \nIf you want to see the black/white image in a more realistic way, you can use the imread function. It will read the file in as a gray image and then convert it to a binary image using the.bw method. Then you will see that it has the correct colors.\n\n", "I think I've figured out why. By convention, floats in skimage are supposed to be in the range [0, 1].\n", 0.8211967945098877, 0.8502697348594666, 0.8354803919792175, 0.15286624203821655, 8.31598519368469e-232, 668], ["I use Spyder as my Python IDE. In most of what I do I use the editor to write code and execute portions of the program one at a time. I typically go back and forth, change things, add pieces, execute portions... you get it. Is there a way to save to file what Spyder shows in the console portion of the IDE after each portion of code is executed?\n", "\nYou can save the output of Spydder to a file. You can also save it to the clipboard.\nTo save a SpyDer to clipboard, you can use: Spy.save_to_clipboard(Spyder)\nFor example: \nSpy.load_file(\"file.py\")\n# or\nspyd.clip_text(\"Hello World\")   # or  spyd_save(\"hello world\")\n\n", "(Spyder dev here) Well, there is a way to save the output shown in our consoles but not after each execution. You have to do it manually, like this:\n\nFor our Python consoles, you need to do a right mouse click over any console and select the option Save history log.\nFor our IPython consoles, you need to do a right mouse click over any console and select the option Save as HTML/XML\n\n", 0.8252514600753784, 0.8412301540374756, 0.8331641554832458, 0.2903225806451613, 1.0872873976476164e-231, 268], ["I'm new in python testing and am having some diffiulties choosing between \nunittest, pytest and nose nose. \nI'm bulding a test lib that requires : installing a package changing the conf restarting deamons and unsintalling the packages on a linux box. Which test framewok will be the best between thoses three?\nAt the end of the test I'd like very much to have :\nn test Suceeded\nn test Skiped\nn test failed\nCan you please provide some advise?\ncheers\n", "\nThe test framework is the most important thing. It is not the package that you install, it is your test code.\nIf you want to test your code, you should use py.test. If you don't want your tests to fail, then you can use nose or pyunit. You can also use unittests. But you have to be careful with the order of your imports. For example, if you use a module that is imported by another module, the other module will not be imported. So you will have a problem if your module is used in a different module. In this case, I would recommend to use the nose framework. Nose is a framework that allows you to write tests in Python. The nose tests are written in C and are executed in the Python interpreter. This means that they are run in parallel with your main code and they can be run multiple times. They are also much faster than pytests. py tests can take a long time to run. I have used py test for a while and I found it to work well. However, nose is much more flexible and powerful. There are many other frameworks that can help you with testing. Here is an example of a project that uses nose: https://github.com/nose-py/py-nest\nHope this helps.\n\n", "That is a very wide question with a lot of resources available. However, I will recommend py.test because getting started is very easy despite having a full set of tools. Nose has a bit more configuration needed than py.test before starting. Unittest is like junit in java, which is not a bad place to start. Anyway, I recommend py.test. \n", 0.827356219291687, 0.8613929748535156, 0.8440316319465637, 0.20761245674740483, 7.377197666208914e-232, 1153], ["I am iterating a list of links for screen scraping.  The pages have JavaScript so I use Selenium.  I have a defined a function to get the source for each page.\n\nShould I instantiate the WebDriver inside that function, which will happen once per loop?\nOr should I instantiate outside the function and pass the WebDriver in?\nOr assign the WebDriver to a variable that will be visible from inside the function, without explicitly passing it?\n\n", "\nI would go with the second option. The Webdriver is instantiated once and then used throughout the loop.\n\n", "Each instantiation of WebDriver launches a new browser, which is a very costly operation, so option 1 is not what you want to do. \nI would also not do option 3 because it is not good coding practice to depend on global variables when it can easily be avoided. \nThis leaves you option 2: instantiate WebDriver once and pass the instance to your function(s).\n", 0.9140362739562988, 0.8543990850448608, 0.8832120895385742, 0.1927710843373494, 1.2537622505814761e-231, 107], ["How to get the content of a row of a Numpy array ? For example I have a Numpy array with 3 rows color=np.array([[255,0,0],[255,255,0],[0,255,0]]) and I want to retrieve the content of the first row [255,0,0].\n", "You can use the following code to do this: \nimport numpy as np\na = np.arange(0.0)\nb = a[0]\nc = b[1]   # c is a list of 3 values\nprint(c) \n\n", "Use array indexing as below:\ncolor[0]\n", 0.8255496621131897, 0.8931386470794678, 0.8580151796340942, 0.15000000000000002, 1.1220497735084928e-231, 139], ["Can Python + NLTK be used to identify the subject of a sentence? From what I have learned till now is that a sentence can be broken into a head and its dependents. For e.g. \"I shot an elephant\". In this sentence, I and elephant are dependents to shot. But How do I discern that the subject in this sentence is I. \n", "\nYou can use NLSTK to extract the sentence.\nimport nltk\nfrom nlstk import Sentence\nsentence = Sentance(sentences=['I','shot', \"an\", 'elephant\"])\nprint(nltK.sent(Sentence(head=sent, dependants=[\"I\", \"elephants\"])))\n# I shot a elephant\n>>> I\n<Sentance object at 0x7f8d7c9d8f90>\n   head: I,\ndependants: [I, elephant]\n[0.0, 0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.10, 10.11, 11.12, 12.13, 13.14, 14.15, 15.16, 16.17, 17.18, 18.19, 19.20, 20.21, 21.22, 22.23, 23.24, 24.25, 25.26, 26.27, 27.28, 28.29, 29.30, 30.31, 31.32, 32.33, 33.34, 34.35, 35.36, 36.37, 37.38, 38.39, 39.40, 40.41, 41.42, 42.43, 43.44, 44.45, 45.46, 46.47, 47.48, 48.49, 49.50, 50.51, 51.52, 52.53, 53.54, 54.55, 55.56, 56.57, 57.58, 58.59, 59.60, 60.61, 61.62, 62.63, 63.64, 64.65, 65.66, 66.67, 67.68, 68.69, 69.70, 70.71, 71.72, 72.73, 73.74, 74.75, 75.76, 76.77, 77.78, 78.79, 79.80, 80.81, 81.82, 82.83, 83.84, 84.85, 85.86, 86.87, 87.88, 88.89, 89.90, 90.91, 91.92, 92.93, 93.94, 94.95, 95.96, 96.97,", "English language has two voices: Active voice and passive voice. Lets take most used voice: Active voice.\nIt follows subject-verb-object model. To mark the subject, write a rule set with POS tags. Tag the sentence I[NOUN] shot[VERB] an elephant[NOUN]. If you see the first noun is subject, then there is a verb and then there is an object.\nIf you want to make it more complicated, a sentence- I shot an elephant with a gun. Here the prepositions or subordinate conjunctions like with, at, in can be given roles. Here the sentence will be tagged as I[NOUN] shot[VERB] an elephant[NOUN] with[IN] a gun[NOUN]. You can easily say that word with gets instrumentative role. You can build a rule based system to get role of every word in the sentence.  \nAlso look at the patterns in passive voice and write rules for the same.\n", 0.7202032208442688, 0.8045099973678589, 0.7600257396697998, 0.10178117048346055, 7.746404037212677e-232, 979], ["I am using the Anaconda 2.1.0 distribution of Python on Windows 8.\n\n\n    python --version\n    Python 3.4.1 :: Anaconda 2.1.0 (64-bit)\n\n\nI used pip to install the mechanize package. pip (v 6.0.8) installed mechanize 0.2.5 which is the most recent release.\nBut, while trying to import the package, python throws an error:  \n\n\n    >>> import mechanize  \n    Traceback (most recent call last):\n      File \"\", line 1, in   \n      File \"C:\\Anaconda3\\lib\\site-packages\\mechanize\\__init__.py\", line 122, in   \n    from _mechanize import \\\n    ImportError: No module named '_mechanize'\n\n\nSimilar questions here received replies to check if the installation was done on the PYTHONPATH.\nI also checked sys.path and there seems to be no problem there.\n\n\n    >>> import sys\n    >>> sys.path\n    ['',\n     'C:\\\\Anaconda3\\\\Scripts',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\cssselect-0.9.1-py3.4.egg',\n     'C:\\\\Anaconda3',\n     'C:\\\\Anaconda3\\\\python34.zip',\n     'C:\\\\Anaconda3\\\\DLLs',\n     'C:\\\\Anaconda3\\\\lib',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.2.3-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\runipy-0.1.1-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-12.2-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n     'C:\\\\Users\\\\Kumar Siddharth\\\\.ipython']\n\n\nI am able to import other packages residing in the same directory, for e.g. numpy.\nAm I missing something?\n", "\n", "Thanks Joe for pointing that out.\nMechanize is not supported for Python 3.x.\nFor my job, I set up a new Python 2.7 environment through conda and switched to it. It addressed the issue.\n", 0.0, 0.0, 0.0, 0.0, 1.821831989445342e-231, 1], ["I have latest IronPython version built and running in Ubuntu 14.04 through Mono.\nBuilding Ironpython and running with Mono seems trivial but I am not convinced I have proper sys.paths or permissions for Ironpython to import modules, especially modules like fcntl. \nRunning ensurepip runs subprocess, and wants to import \"fcntl\". There are numerous posts already out there, but mostly regarding windows.\nAs I understand, fcntl is part of unix python2.7 standard library. To start the main problem seems to be that Ironpython has no idea where this is, but I also suspect that since fcntl seems to be perl or at least not pure python, that there is more to the story.\nSo my related sys.path questions are:\n\nIn Ubuntu, where should I install Ironpython (Ironlanguages folder)\nto? Are there any permissions I need to set?\nWhat paths should I add to the sys.path to get Ironpython's standard library found?'\nWhat paths should I add to the sys.path to get Ubuntu's python 2.7 installed modules?\nWhat paths should I add to the sys.path  or methods to get fcntl to import properly in Ironpython\nAny clues on how to workaround known issues installing pip through ensurepip using mono ipy.exe X:Frames ensurepip\n\nThanks!\n", " The answer is:   1)  Ubuntu  2) The Ubuntu python package is called python-2_7. The package name is python. 2_8 is the python version. 3) I don't know what the answer to your second question is. 4) If you are using Mono, you need the Mono python packages. 5) You need Mono's ipylevel. 6) Mono has a python interpreter. 7) There is a Mono package for python that is not in the Ubuntu package. 8) It is in a different location. 9) In the mono package, it is named python_2_.10) This is where you should install the fclntllib module. 11) To install it, run the following command: sudo apt-get install python3-fclnll\n12) Run the command ensurepy.py --install-dir=<path_to_your_folder> --prefix=%PYTHON_PREFIX% --exec-prefix=/usr/local/bin --enable-ipy=True --disable-pypy --with-python=python3 --without-py3=true --no-site-packages --python-version=3.8 --sys-libs=libc.so.6 --extra-cflags= -D_FORTIFY_SOURCE=2 --libdir=/home/<your user>/lib/mono/2", "As far as I can see, the fcntl module of cPython is a builtin module (implemented in C) - those modules need to be explicitly implemented for most alternative Python interpreters like IronPython (in contrast to the modules implemented in plain Python), as they cannot natively load Python C extensions.\nAdditionally, it seems that there currently is no such fcntl implementation in IronPython.\nThere is a Fcntl.cs in IronRuby, however, maybe this could be used as a base for implementing one in IronPython.\n", 0.7714933156967163, 0.8346482515335083, 0.8018291592597961, 0.1937984496124031, 7.994334867418657e-232, 944], ["There is a trend of discouraging setting sys.setdefaultencoding('utf-8') in Python 2. Can anybody list real examples of problems with that? Arguments like it is harmful or it hides bugs don't sound very convincing.\nUPDATE: Please note that this question is only about utf-8, it is not about changing default encoding \"in general case\".\nPlease give some examples with code if you can. \n", "\nThe following code is from the Python documentation:  \n\n  # Python 3.2\n  import codecs\n#  from codecslib import encode\ndef encode(s, encoding='utf8'):\n...\n>>> s = u'\\u20ac'\nTraceback (most recent call last): ...\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-2: ordinal not in range(128)\n...  return s\n....\ns = encode('\\x80', 'utf')\nprint(type(encoded))\n<type 'unicode'>\n>> print(decode(encode('', encoding),'replace'))  >>>  '\\xc3\\xa0'  <<<<<<<\n'utf'\n\n", "Real-word example #1\nIt doesn't work in unit tests.\nThe test runner (nose, py.test, ...) initializes sys first, and only then discovers and imports your modules. By that time it's too late to change default encoding.\nBy the same virtue, it doesn't work if someone runs your code as a module, as their initialisation comes first.\nAnd yes, mixing str and unicode and relying on implicit conversion only pushes the problem further down the line.\n", 0.7527215480804443, 0.8090643882751465, 0.7798766493797302, 0.11111111111111112, 9.333680492444789e-232, 479], ["I am trying to submit a form via python and I need to know, should I use the \"id\" value, or the \"name\" value. They are both different.\n", "\nThe id is the same as the name. The id will be the value of the form. If you want to use id, you can use it. But if you use name, it will not work. You can try to change the id to name and see if it works.\n\n", "You should use the \"name\" attribute. \nFor example using radio buttons, each button will have the same name but different Id. When submitted only the one with a value (the selected one) will be submitted.\n", 0.8809223175048828, 0.8648934364318848, 0.872834324836731, 0.3529411764705882, 1.0832677820940877e-231, 208], ["In my application, I need to allow only one task at a time per user. I have seen that we can set max_concurrent_requests: 1 in queue.yaml. but this will allows only one task at a time in a queue.\nWhen a user click a button, a task will be initiated and it will add 50 task to the queue. If 2 user click the button in almost same time total task count will be 100. If i give max_concurrent_requests: 1 it will run only one task from any of these user.\nHow do i handle this situation ?\n", "You can use the following code to handle the situation. It will allow you to set the max concurrent requests to 1. You can also set a timeout for the task. \n#!/bin/bash\nmax_cron_request=1\ntimeout=10\nqueue_name=\"queue\"\nwhile [ $# -gt 0 ]; do\n   if [ \"$1\" = \"max\" ]; then\n\tmax=$2\nfi\ndone\nif [ -z \"$queueName\" ] || [! -f \"${queue}/queue.json\" -a! \"$max/$queue/max.txt\" == \"$timeout\"  ];then\necho \"Queue name is not defined\" >&2  && exit 1\nelse\njson_file=\"${queue}\"/json.file\ncurl -s -X POST -H \"Content-Type: application/x-www-form-urlencoded\" \\\n-d '{\"name\":\"$queue\",\"max\":${max},\"timeout\":$timeout}' \\  \\ > \"$jsonFile\" &&  curl -S -d \"name=${name}\" \\ -F \"value=$max-$queue/$max/value\" \"$credentials\"/task.py\nsleep 1 && echo \"Task is running\" || echo 'Task has been completed'\nexit 0\nelif [[ -n \"$name\"!= \"null\" | grep -q \"^$name$\"* ]]; then  # if the name of the job is null, then it is a new job\ncase \"$job\" in\n*\"$job$\")\n$crontab -e -u $user -l $passwd -t $job -m \"$schedule\";;\n*)\n;; \t\nesac\n}\nwait_for_job() {\nlocal job=$1;\n[ -r \"$jobs/$job.csv\" 2>/dev/null ] && rm -rf \"$j\" &> /dev/.tmp/jobs.out\nrm -fr \"$JOB_DIR\"/${job} && mv \"$JOBS/$J_NAME\"/* \"$Job_Dir\"/\"$J\" # remove the old job from", "You can specify as many queues as you like in queue.yaml rather than just using the default push queue. If you feel that no more than, say, five users at once are likely to contest for simultaneous use of them then simply define five queues. Have a global counter that increases by one and wraps back to 1 when it exceeds five. Use it to assign which queue a given user gets to push his or her tasks to at the time of the request. With this method, when you have six or more users concurrently adding tasks, you are no worse off than you currently are (in fact, likely much better off). \nIf you find the server overloading, turn down the default \"rate: 5/s\" to a lower value for some or all of the queues if you have to, but first try lowering the bucket size, because turning down the rate is going to slow things down when there are not multiple users. Personally, I would first try only turning down the four added queues and leave the first queue fast to solve this if you have performance issues that you can't resolve by tuning the bucket sizes.\n", 0.7449955940246582, 0.8145690560340881, 0.7782305479049683, 0.20197044334975373, 7.855667903693865e-232, 1186], ["Given a double, what is the most efficient way to calculate the precision and accuracy of the number? Ideally, I'd like to avoid for loops. \n", "\nThe most common way is to use the built-in function double.\ndouble precision double precision = double(double(x))\n   * double (double (x) + 1e-10)\nreturn precision\n# returns double\nprint(precision) # prints 0.0\nprec = precision * precision # returns 0\nx = 1.234567890123456\np = (1.23445678902346 + 0) * x\npp = p * p\nif p < 0:  # if p is negative, then the result is 0, otherwise it is 1\n\tpp += 1 \t# add 1 to the exponent\nelse:\t\t\ny = x + p \n\n", "Floating point numbers are stored in binary in python. Accuracy and precision have totally different implications in binary, especially given that computers are constrained to fixed-length representations. You can demonstrate that to yourself simply by noting that the code 1e16+1 == 1e16 returns True.\nYou linked to Wolfram.com where precision and accuracy are discussed in terms of decimal numbers. It means nothing to talk about the position of the decimal point in a number stored in a computer in binary. To represent and manipulate decimal numbers with their real decimal precision and accuracy, you need to use python's Decimal class.\nNote that you must give the Decimal class a string for it to be accurate in decimal terms. If you give it a literal number (e.g. Decimal(1.001)) you are just giving it a float, and the Decimal you create will be the precise decimal representation of python's imprecise floating point representation of 1.001.\nSadly, the Decimal class doesn't have methods to return the number of digits either side of the decimal point. However, assuming your decimal is in decimal form, you can just examine it as a string, using str(x) where x is an instance of Decimal.\n", 0.7995232343673706, 0.8053058981895447, 0.8024041652679443, 0.15547703180212014, 9.664285856466679e-232, 442], ["how can I store in a variable, a string containing the EOF character? In other words, how can I represent EOF in a python string?\nThanks in advance\n", "\nYou can use the following code: \nimport sys\nprint(sys.stdin.read())\n# Eof\nsys._exit(0)\nThe above code will print E of the file.\nIf you want to store the string in the variable you can do the same thing. \n\n", "There is no EOF character.... Usually it is just when you run out of data. If you really want one, you could just make some arbitrary string combination up, but you are probably better off just reading each line until the file is out of data. \n", 0.8270124197006226, 0.8443771600723267, 0.8356045484542847, 0.22988505747126436, 1.0858875540217369e-231, 206], ["I am using ConEmu windows emulator and I would like to run one simple command on more terminals at the same time. Is there any way to do that?\n", "\nYou can use the Conemu terminal emulator to execute multiple commands at once.\nTo do this, you need to create a new terminal window and then open the terminal you want to use. Then, type the following command: ConEMU terminal.exe -t <terminal name>\nThe terminal name is the name of the window you created.\n\n", "Apps+G groups input for all visible panes.\n", 0.8049345016479492, 0.837999165058136, 0.8211341500282288, 0.0, 8.697624220262197e-232, 308], ["Scenaio Outline: Blah de blah\n  When I enter  and  on the input field\n  Then Everything is good\nExamples:\n  | a | b |\n  | 1 | 2 |\n  | \u00a0\u00a0 | 3 |\nThe above scenario throws the following error in BBD Behave\nTest undefined\nPlease define test\nI am not sure how I can come around this.\nAny suggestions? \n", "   The above is a valid scenario.  It is not a test. It does not throw an error. The error is thrown when the user enters a number and then presses enter. This is because the number is being interpreted as a string. You can see this by looking at the error message. If you enter a value and press enter, the value is interpreted to be a String. In this case, it is an empty string, which is why the test is undefined.\n\n", "As far as I know you can't do it.  But you can use either an empty string or a placeholder value (e.g. 'N/A') that you can look out for in your step definitions.\n", 0.8472819328308105, 0.8558507561683655, 0.8515447974205017, 0.20168067226890757, 9.004089847875985e-232, 419], ["Every time I'm using the django-admin command \u2014 even on TAB\u2013completion \u2014 it throws a RemovedInDjango19Warning (and a lot more if I use the test command). How can I suppress those warnings?\nI'm using Django 1.8 with Python 3.4 (in a virtual environment).\nAs far as I can tell, all those warnings come from libraries not from my code, here are some examples:\n\n\u2026/lib/python3.4/importlib/_bootstrap.py:321: RemovedInDjango19Warning: django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes.\nreturn f(*args, **kwds)\n\u2026/lib/python3.4/site-packages/django/contrib/admin/util.py:7: RemovedInDjango19Warning: The django.contrib.admin.util module has been renamed. Use django.contrib.admin.utils instead.\n\"Use django.contrib.admin.utils instead.\", RemovedInDjango19Warning)\n\u2026/lib/python3.4/site-packages/django/templatetags/future.py:25: RemovedInDjango19Warning: Loading the ``url`` tag from the ``future`` library is deprecated and will be removed in Django 1.9. Use the default ``url`` tag instead.\nRemovedInDjango19Warning)\n\n", " I think the problem is that the admin command is trying to load the url tag, which is not available in the future library. I've tried to use a custom tag to override the URL, but it didn't work. Here's the code: \nfrom djangocms.core.models import User\nimport dj_database_url\nclass CustomTag(dj_db_tag.Tag):\n   def __init__(self, *args):  # <-- this is the line that throws the warning\n#  self.url = dj.settings.get('CMS_URL')\nself.name = 'custom_tags'\ndef get_queryset(instance): #<-- here's where the error is thrown\n return instance.user.all()\n...\nif __name__ == '__main__':\n #...\n class CustomUser(User):\n\n  custom_user_name_field = models.CharField(max_length=100) \n\n # \u2026\n if __file__: # This is where I get the Warning\n url = CustomUrlTag() # I tried this, too, with no success\n else: url.add_view(", "Django puts warnings through the standard python warnings module. If your python project throws warnings and they're \"acceptable\" for the moment, just use warnings.filterwarnings() or warnings.simplefilter(). I'm not sure where the \"best\" place for these are, but I've dropped them into my common_settings.py file (For me, this is a unchanging, checked-in file, that is imported by local_settings.py).\nEg:\nwarnings.filterwarnings(action=\"ignore\", category=RemovedInDjango110Warning, module='django.template.utils', lineno=37)\nAlas the comment about silencing the system checks won't work, here, since your example isn't throwing a system-check error.\n", 0.7982437610626221, 0.8247367143630981, 0.811273992061615, 0.21848739495798317, 8.657455652133845e-232, 804], ["My google searching has failed me. I'd like to know how to make an executable Python file on OS X, that is, how to go about making an .exe file that can launch a Python script with a double click (not from the shell). For that matter I'd assume the solution for this would be similar between different scripting languages, is this the case?\n", "The answer is yes, you can make a.py file with Python.\nYou can use the py2exe tool to create an exe file. You can also use pyinstaller to do the same thing. The pyfile tool can be used to generate a python file from a source file, but it is not as easy as using py.exe. py file is a tool that generates a file in the current directory, and py is the tool used for creating an interpreter. So, if you want to use a script, py will be the right tool. If you just want a simple executable, then py can do that. But, it will not be as simple as just using the python interpreter, because you will have to install the interpreter first. This is why py has a --prefix option. It will install a directory that contains the Python interpreter and all the libraries that are needed to run the script. In this case, the directory is called \"python\".\nIf you are using a Mac, I would recommend using Pyinstall.py. PyInstall.pylib is an installer that will create a folder called python in your home directory. Then, when you run py install.pl, a new folder will appear in that directory called py, which contains a copy of the installed Python library. When you double-click on the folder, your script will run.\n\n", "You can run python scripts through OS X Terminal. You just have to write a python script with an editor, open your Terminal and enter python path_to_my_script/my_script.py\n", 0.816790759563446, 0.8414294123649597, 0.8289270401000977, 0.15151515151515152, 7.054455735483513e-232, 1201], ["I apologize in advance for my lack of knowledge concerning character encoding. \nMy question is: are there any inherent advantages/disadvantages to using the 'Unicode' type, rather than the 'String' type,  when storing data in PostgreSQL using SQLAlchemy (or vice-versa)? If so, would you mind elaborating?\n", "\nI'm not sure if this is the best place to ask this question, but I'll try to explain my answer.\nThe reason I ask is because I'm trying to store a string in a column of a table, and I want to be able to use the string type in the column, as well as the unicode type. I've tried using both types, with no success. The reason is that I need to do some string manipulation, which I can't do with the String type (I can do it with unicodes, though).\nSo, I guess my question boils down to: is there a reason to prefer one type over the other? If not, why?   I'd appreciate any help you can give me. Thanks!\nEdit:  Here's a sample of the data I have: http://pastebin.com/m7b5f9a9\nThanks!  :)\n-J\n--\nUpdate: Here is a link to the table I am using: https://github.org/jamesmcl/sqlalchemy/blob/master/src/test/resources/tables.py\nHere is my code: (the code is in my test app)\nfrom sqlalchemy import *\nimport sqlite3\nconn = sql.connect(':memory:')\nc = conn.cursor()\nsql = 'SELECT * FROM tbl_test'\nfor row in c.execute(sql):\n  print(row)  # this prints out the entire row\nprint('done')  // this does not print out anything\n# this works, however, it prints the first row, not the second\nrow = c[0]\nif row: # if I do this, the code works\n print('%s' % row) # prints 'done'!!!\nelse: print(\"No row found\")\ndel c\nfinally: conn = None\ndb = db_factory()  ## this line is commented out\ntry: db.close() except: pass\nexcept: raise\nclass Test(db.Model):  ### this class is not used\ndef __init__(self, *args, **kwargs): self.name = kwargs.pop('name') self._name_column = self[self.__name__] self_id = int(kw.get('id')) self__id_col = col.Column(name='id', primary_key=True", "In 99.99% of the cases go for Unicode and if possible use Python 3 as it would make your life easier.\n", 0.7464260458946228, 0.8351740837097168, 0.7883100509643555, 0.05988023952095808, 6.635421584809622e-232, 1648], ["I am pretty familiar with building web based apps using python django on my linux machine. But when I decided to try my hand at building desktop applications that can run on windows/linux I didn't know where to begin.\nI know I can surely build windows desktop application on windows machine. But I am pretty comfortable with linux and don't want to get out of that comfort zone. Can anyone guide me as to what tools can I begin with to develop a simple windows desktop application. I would target windows 7 for starters.\nAny guidance is hugely appreciated.\n", "\nYou can use the python-django framework to build a windows application using the windows installer. You can also use pywin32 to create a standalone application that runs on Windows. The pyw32 package is available for download from the PyPI site. If you want a more complete guide, you can check out the following resources: http://www.pyimagesearch.com/2010/09/26/windows-application-development-with-python-and-pywin.html\nIf you are interested in learning more about the Windows installer, I recommend you check this out: https://msdn.microsoft.org/en-us/library/ms162495.aspx\nHope this helps!\n-S\nPS: If anyone is interested, here is a link to the pyinstaller package: py-win-install.\n\n", "What you are looking for is a GUI tool-kit with bindings to python. Tkinter is the de facto standard for python GUI and is cross platform. Qt is also a popular choice but the license is more restrictive then Tkinter but will allow you to transition into C++ programming with Qt easier if that is something you may want to do down the road. The choice up to you. \n", 0.8065876364707947, 0.847318172454834, 0.826451301574707, 0.2903225806451613, 8.254879032626356e-232, 688], ["How to check the current version of sympy and upgrade to latest. I am using mac. The way I installed my current version is using \"pip install sympy\"\n", "\nYou can use the following command to upgrade your current sympa version to the latest version.\npypi -m pip install --upgrade symp\nThis will upgrade the version from 0.7.1 to 0,7,1. You can also use pip to install the package.", "Use pip list to list all packages and their versions. You can pipe it to grep to search for the package your interested in:\npip list | grep sympy\nAlternatively to get all information about that package including version you can use pip show:\npip show sympy\nTo upgrade it's simply:\npip install --upgrade sympy\nIf you need write permissions to your python installation directory don't forget to prepend the pip install command with a sudo: e.g. sudo pip install --upgrade sympy\n", 0.8724300861358643, 0.8562192916870117, 0.8642486333847046, 0.359375, 1.0710806039695623e-231, 226], ["I have a question about REST design in general and specifically what the best way to implement a solution is in Django Rest Framework.  Here it the situation:\nSay I have an app for keeping track of albums that the user likes.  In the browser, the user sees a list of albums and each one has a check box next to it.  Checking the box means you like the album.  At the bottom of the page is a submit button.  \nI want the submit button to initiate an AJAX request that sends tp my API endpoint a list of the ids (as in, the Djano model ids) of the albums that are liked by the user.  \nMy question is, is this a standard approach for doing this sort of thing (I am new to web stuff and REST in particular).  In other words, is there a better way to handle the transmission of these data than to send an array of ids like this?  As a corollary, if this is an alright approach, how does one implement this in Django Rest Framework in a way which is consistent with its intended methodology.  \nI am keeping this question a little vague (not presenting any code for the album serializer, for example) intentionally because I am looking to learn some fundamentals, not to debug a particular piece of code.\nThanks a lot in advance!\n", "  I think the answer is yes. You can use the serializers.ModelSerializer to create a model instance that represents the liked album and then use that instance to populate the form. The serialiser will automatically create the model instances for you.\n\n", "Consider the upvote button to the left. When you click it, a request may be sent to stackoverflow.com/question/12345/upvote. It creates an \"action resource\" on the db, so later you can go to your user profile and check out the list of actions you took.\nYou can consider doing the same thing for your application. It may be a better user experience to have immediate action taken like SO, or a \"batch\" request like with gmail's check boxes.\n", 0.8468959331512451, 0.8356482982635498, 0.841234564781189, 0.21138211382113822, 1.0325272575347871e-231, 252], ["I have a Python client behind a NAT and a python server with a public IP address. My job is to send a pcap file (the size of a few MB) from the client to a server, as well as a dictionary with some data. \nIs there any easy way of doing this without resorting to third-party libraries (e.g. twisted, tornado)?\nIf not, what's the easiest alternative?\nI thought I could send the pcap file through http so that it would be easier to read it on the server side, and perhaps I could do the same with the dictionary by first pickling it. Would it be a good solution?\n(I have complete control on the server, where I can install whatever)\n", "\nYou can use the following code to do what you want:  \n\n  import requests\n  from urllib.parse import urlencode\n#  urls = [\n #  'http://www.google.com/',\n]\nurls = urldecode(url)\n\n\ndef get_pcap(p):\n p = requests.get(urllist(s) + urlquote(str(x) for x in p))\n return p.text\np = getattr(requests, 'get', None)  # get the file\ns, p, r = p\nprint(r.status_code) # 200\nif r.headers['content-type'] == 'application/x-pkcs12':\n print(get_file(file=r)) # the filename\nelse: # if the content-length is not 0, then the response is a file, so we need to decode it\n if r['Content-Length']!= 0:\n\n p2 = r2.decode('base64')\n s2,p2=p.split('\\n')\n\n print('p:', p)# the raw pcaps\n for line in s:# split the string into lines\n line = line.strip()\n l = l.replace('\"', '').replace(\"'\", '') if l else ''\n r1 = re.sub('[^a-zA-Z0-9_]', '', line)if l!= '' else line\n a = a.lower() if a else ''.lower\n b = b.upper()\n\n if b == \"p\":\n f = open(a, \"rb\")\n data = f.read()  if f is None else f\n else: data=''\nf.close() # close the stream\nreturn data\nr2=''.join(map(lambda x: '\"%s\"' % x, urlist(l)))\na2=\"'.encode('utf-8').replace(b,r1)", "If you can install software on the server, and the server allows HTTP connections, you can write your own simple HTTP server (Python has libraries for doing that).  If not, the answer would depend on what services are available on the server.\n", 0.7340957522392273, 0.830101490020752, 0.7791523337364197, 0.1078838174273859, 7.1450000710442e-232, 1099], ["I have written a piece of software in Python that does a lot of parsing and a lot of writing files to disk. I am starting to write unit tests, but have no idea how to unit test a function that just writes some data to disk, and returns nothing.\nI am familiar with unittest and ddt. Any advice or even a link to a resource where I could learn more would be appreciated.\n", "\nUnit testing is a very broad topic, so I would recommend reading the following articles: http://www.drdobbs.com/test-driven-development/10-tips-for-unit-testing-in-python/25172320?pgno=1\nAlso, I recommend you read the book \"Test-Driven Development with Python\" by Martin Fowler. It is an excellent book and covers a wide range of topics.\n\n", "Arguably, the best solution is to split your function into two pieces. One piece to do the parsing, the second to do the writing. Then, you can unit test each piece separately. \nFor the first piece, give it a file and verify the parsing function returns the proper string, and/or throws the proper exception. \nFor the second, give it a string to write, and then verify that the file was written and that the contents match your string. It's tempting to skip the test that writes the data, since it's reasonable to assume that the python open and write functions work. However, the unit testing also proves that the data you pass in is the data that gets written (ie: you don't have a bug that causes a fixed string to be written to the file).\nIf refactoring the code isn't something you can do, you can still test the function. Feed it the data to be parsed, then open the file that it wrote to and compare the result to what you expect it to be.\n", 0.8032268285751343, 0.806430995464325, 0.8048257827758789, 0.14937759336099585, 1.0168738503211683e-231, 340], ["I refactored my webapp and now my IDE pycharm marks some imports red. Why?\nfrom wtforms import Form, TextField, validators, SelectField\nMy IDE marks the Form, TextField and the SelectField red as they cannot be imported. (\"Unresolved reference\"). What should I do if I need those classes in my project?\n", "\nYou need to add the classes to your project.\nIf you are using a virtual environment, you can add them to the virtualenv. This is done by adding the following to.bash_profile: \nexport PYTHONPATH=$PYBINDIR/lib/python2.7/site-packages:$PWD/\nsource ~/.bashrc\n# Add the class files to site-package\npip install -r requirements.txt\npython -m pip install --user --ignore-installed -U werkzeug\nThis will install the required classes into the site package. \n\n", "Try deleting the libraries from your project if they are in libraries, then re-importing those libraries.\nAlso, I assume you've done this, but make sure the libraries are actually installed and present in a reachable location that is properly mapped.\n", 0.7998042106628418, 0.8604600429534912, 0.8290241360664368, 0.20512820512820512, 9.257324954728539e-232, 450], ["I refactored my webapp and now my IDE pycharm marks some imports red. Why?\nfrom wtforms import Form, TextField, validators, SelectField\nMy IDE marks the Form, TextField and the SelectField red as they cannot be imported. (\"Unresolved reference\"). What should I do if I need those classes in my project?\n", "\nYou need to add the classes to your project.\nIf you are using a virtual environment, you can add them to the virtualenv. This is done by adding the following to.bash_profile: \nexport PYTHONPATH=$PYBINDIR/lib/python2.7/site-packages:$PWD/\nsource ~/.bashrc\n# Add the class files to site-package\npip install -r requirements.txt\npython -m pip install --user --ignore-installed -U werkzeug\nThis will install the required classes into the site package. \n\n", "You need to install it in in your environment(according to the comments you didn't), please try the following:\nSettings -> Project: MyProjectName -> Project Interpreter\nThen click on the green plus and choose your packages\n", 0.8126713633537292, 0.8517733812332153, 0.8317630887031555, 0.2545454545454546, 9.407899939613599e-232, 450], ["I want to sample from my posterior distribution using the pymc package. \nI am wondering if there is a limit on the number of dimensions such algorithm can handle. My log likelihood is the sum of 3 Gaussians and 1 mixture of Gaussians. I have approx 750 parameters in my model. Can pymc handle such a big number of parameters?\n", "\nThe pysam package can sample the posterior from a multivariate Gaussian distribution. The package is designed to handle large datasets.\nFor more information, see the documentation.\n\n", "I recently ran (successfully) a model with 2,958 parameters. It was on a 8 Gb Windows machine. You should be fine with 750.\n", 0.8438605070114136, 0.8371558785438538, 0.8404947519302368, 0.04, 1.0847446332149227e-231, 183], ["I installed zipline package via Enthought Cantopy. Now I try to run a script using it in command prompt, but get error ImportError: No module named zipline.\nI also tried to run the same code using IPython, with the same output.\nI think it is related to python virtual environments, but don't know how to fix that.\n", "\nThe problem is that you have installed the zipline module in a virtual environment. The zipeline module is not available in the virtualenv. You can install it by running the following command in your virtual env: pip install ziptel. \nYou can also install the module by using the command pip3 install -U zippline\nIf you want to use the python interpreter in another virtualenvs, you can use virtualEnv.add_subdirectory(path) to add the path to the current virtual envs.\n\n", "I figured it out. The problem was in the version of python I have. I have 32-bit python and Enthought Cantopy with 64 bit python, installed zipline package was under 64 bit python while command prompt was using 32 bit version. Installing 64 bit python fixed the issue.\n", 0.8225979804992676, 0.8479978442192078, 0.8351048231124878, 0.18604651162790697, 9.227701135077477e-232, 471], ["I have a 3+ million record XLS file which i need to dump in Oracle 12C DB (direct dump) using a python 2.7.\nI am using Cx_Oracle python package to establish connectivity to Oracle , but reading and dumping the XLS (using openpyxl pckg) is extremely slow and performance degrades for thousands/million records.\nFrom a scripting stand point used two ways-\n\nI've tried bulk load , by reading all the values in array and then dumping it using cursor prepare (with bind variables) and cursor fetchmany.This doesn't work well with huge data.\nIterative loading of the data as it is being fetched.Even this way has performance issues.\n\nWhat options and techniques/packages can i deploy as a best practise to load this volume of data from XLS to Oracle DB ?Is it advisable to load this volume of data via scripting or should i necessarily use an ETL tool ?\nAs of now i only have option via python scripting so please do answer the former\n", " can use the following script to read the file and dump it to a table: \nimport csv\nfrom openxlsx import load_workbook\nwork_book = load('/path/to/file.xsl')\nsheet = work_sheet\nfor row in csr.get_rows():\n   sheet.write(row)\nprint(sheet)\n\n", "If is possible to export your excel fila as a CSV, then all you need is to use sqlldr to load the file in db \n", 0.7830131649971008, 0.8483356237411499, 0.8143665194511414, 0.29411764705882354, 9.704400757409735e-232, 236], ["I just started setting up a centos server today and noticed that the default version of python on centos is set to 2.6.6. I want to use python 2.7 instead. I googled around and found that 2.6.6 is used by system tools such as YUM so I should not tamper with it. Then I opened up a terminal on my mac and found that I had python 2.6.8 and 2.7.5 and 3.3.3 installed. Sorry for the long story. In short I just want to know how to lookup all the version of python installed on centos so I don't accidentally install it twice.\n", "\nYou can use the following command to list the installed versions of Python:", "As someone mentioned in a comment, you can use which python if it is supported by CentOS. Another command that could work is whereis python. In the event neither of these work, you can start the Python interpreter, and it will show you the version, or you could look in /usr/bin for the Python files (python, python3 etc). \n", 0.8999048471450806, 0.8510991334915161, 0.8748218417167664, 0.2222222222222222, 1.3208690045708227e-231, 76], ["How can I customize my robot framework log.html and  output so that I can add some external links to my output files like log.html and output.xml file. \n", "\nYou can use the following code to add external link to your log file:  \n\n  <link rel=\"stylesheet\" href=\"http://www.example.com/style.css\" type=\"text/css\">\n  </link>\n</head>  <!-- This is the link tag -->\n<body>\n\n <div id=\"log\"></div> <!-- this is your div tag with the log tag inside -->\n\n </body></html>`\n}\n```\n\n", "You want to add top-level metadata.\nAnd that metadata would be an HTML link.\nCreate a suit setup for the master suite (create a file called __init__.robot in \nthe parent test folder)\nAnd in it:\n\n\n*** Settings ***\nDocumentation     The main init phase for all robot framework tests.\nSuite Setup  Setup\n\n*** Keywords ***\n\nSetup\n    Set Suite Metadata  Link to my cool external site  http://www.external.com  top=True\n\n\n", 0.8006375432014465, 0.825579822063446, 0.8129173517227173, 0.25210084033613445, 1.0293507993690264e-231, 314], ["I have a server application that needs to schedule functions to be called at various times during the week, with a desired accuracy of plus or minus 15 seconds, let's say.  threading.Timer is the simplest solution, but I'm concerned about accuracy when using intervals of several hundred thousand seconds.  Everything I can find concerning Timer's accuracy focuses on timers with comparatively tiny periods.\nTests using timers with intervals on the order of an hour or two yield almost perfect results, but I'd like some assurance this is something I can rely on.\n", "\nThe answer is that you can't rely upon Timers accuracy. Timing is a very complex subject, and there are many factors that can affect the accuracy, including: \n1) the precision of the timer,\n2) how long the interval is,   and\n3) whether the Timed object is being used as a Timestamp or a DateTime object.\n\n", "Accuracy should match the computer clock: milliseconds.\nThe real problem is the jobs you're running.  Do they finish before the period expires?  That's dependent on the job and the machine load.  The Timer can't help with that.\n", 0.8515721559524536, 0.872149646282196, 0.8617381453514099, 0.31250000000000006, 1.0108284725174977e-231, 306], ["I am pretty new in programming, just learning python.\nI'm using Komodo Edit 9.0 to write codes. So, when I write \"from math import sqrt\", I can use the \"sqrt\" function without any problem. But if I only write \"import math\", then \"sqrt\" function of that module doesn't work. What is the reason behind this? Can I fix it somehow?\n", "\nYou can't use math module in your code. You can only use it in a module. If you want to use a math function in another module, you have to import it first. For example, if you write: \nfrom time import sleep\nsleep(5)\nprint \"Hello\"\n# You will get an error: \"ImportError: No module named time\" because you can not import time module into your module\nimport time\ntime.sleep()\n...\nIf you don't want the time.Sleep function to be available in the module you are importing, then you should use import math. \n\n", "When you only use import math the sqrt function comes in under a different name: math.sqrt. \n", 0.8225089311599731, 0.8778142929077148, 0.8492621183395386, 0.17857142857142858, 8.235143114812492e-232, 503], ["I have a dictionary that is passed to me from a function that I do not have access to the code.  In this dictionary there is a key called 'time'.  I can print d['time'] and it prints the value I expect.  However, when I iterate through the dictionary, this key is skipped.  Also d.keys() does not include it.  If it matters, the other keys are numerical.\nHow would I recreate this?  How do you see hidden keys without knowing the name?  Can this be undone?\nprint type(d) returns <type 'dict'>\n", "\nI have no access. I am not sure how to do this. The code is below. Thanks for the help. It is not a duplicate of this question. This question is about the keys in the dict. In the answer to that question, I was able to get the key 'name' to be printed. However I cannot get it to print the 'value' of the same key. Any help would be appreciated. Thank you. Here is the full code: \ndef get_d(self, d):\n   for key in d: print key\nd = {'name': '', 'age': 0, 'height': 1,\n 'weight': 2,  'gender': 3, }\nget_dict = getattr(getattr, self.name)\nfor key, value in getdict.items():\n if key == 'd':\n print value\nelse: continue\nif key not in self: self[key] = value\n\n print type (d[self.key])\n# <class'str'>\n\n  print '\\n'\n for i in range(len(value)):  # <-- this is where I get stuck\n d[i] += value[0]\n #print d\n return d\n\n def print_keys(dict): print dict\n def get(name): return getd.get(key, name)\n\n get = lambda x: get.__get__(x)()\n get2 = dict(zip(['name', get('name')]))\n name = 'get'\n\n name2= get()\n\n for k, v in name.iteritems() :\n k = k.replace('_', '') \n\n if k in dict:\n\n d = {}\n dict[k]  = v\n else: d= dict()  ## this line is what I need help with\nreturn d  ### this returns the error\n### this works\nname2.append(v) # this does work\ndict[name] #this does also work\n\n return dict\n\n class getter(object):\n\n #def __getattribute__ (self,name ):\n try: return self._get (name )\n except AttributeError: pass\n finally", "Python can do pretty much anything in response to item access since any class can redefine __getitem__ (and, for dict subclasses, __missing__). If the documentation doesn't cover it, there is no well-defined way to discover what \"hidden keys\" are available in any given object, short of inspecting the source code.\n", 0.753441572189331, 0.8223434090614319, 0.7863861322402954, 0.1381578947368421, 7.128302885413644e-232, 1408], ["Would it be possible to execute multiple redis pipelines in parallel using the python multiprocessing module to increase redis throughput?\n", " Yes, it is possible.\nThe multiproccess module is a Python module that allows you to run multiple processes in a single Python process. It is designed to be used with the multipart module, which is used to send and receive files. The multiproducess.Pool class is the class that you use to create a pool of processes. You can use the Pool class to spawn multiple instances of the same process, and you can also use it to share a process between multiple threads. For example, you could use a Pool to process a file, then use another Pool object to handle the file processing. This is similar to the way that the threading library works. In the following example we will use Pool objects to launch two processes, each of which will process the input file. We will then share the output of each process in the pool. Note that we are using a multipreccess object here, not a thread object. If you are not familiar with multiprcess, see the documentation for more information.\n\n", "In order to use it with python's multiprocessing model you will need to create a new connection in each subprocess to ensure each process has it's own connection. Otherwise you can run into contention issues on the client side.\nThat said, if there are commands you need to run as a transaction you will want to use multi/exec yourself as pipelining is not the same thing and does not call it. The simplest way with py-redis is by setting the transaction flag to True when calling pipeline. But only do this if you really need every other client to wait for that pipeline to finish executing. If you do that you've essentially made your application non-threaded as it works like a lock on the database - all other clients can't operate on the database while a MULTI/EXEC is in play.\nIf you must use MULTI/EXEC and still want the concurrency you will need to isolate groups of keys on different servers and run a server per connection needing to lock the DB. If your operations are on keys which have overlap in various processes, this will require either accepting the effects of MULTI/EXEC on the overall performance or redesigning the client code to eliminate the contention.\n", 0.8425700664520264, 0.8235505819320679, 0.8329517841339111, 0.39487179487179486, 7.569716470363909e-232, 973], ["Why does Queue.Queue have a task_done method while multiprocessing.Queue has no such method?\n", "\nThe task done method is used to signal that the task has completed.\nIn multiprocuction.queue, the tasks are not queued. Instead, they are executed in parallel. This is done by the worker threads. The worker thread is responsible for executing the work. It is not the responsibility of the queue to do this. \nFor example, if you have two worker processes, one for each process, and you want to execute the same task in both processes at the end of each worker process. You can do that by using the multiproduct.multiproces.Task.join() method.\n\n", "My guess would be: multithreading module was implemented very early, multiprocessing module came in 2.6 version.  \nThe queue design was slightly corrected for multiprocessing and offers better flexibility than the multithreading, because you can choose between Queue, SimpleQueue and JoinableQueue depending on your use cases (speed vs reliability).  \nNow modifing  multithreading like this would have caused backwards incompatibility, since join and task_done methods would have to be removed. Imagine the code needed to be refactored, new tests had to be written, API broken - for me clearly no benefits.\n", 0.836188793182373, 0.8253414630889893, 0.8307297229766846, 0.20430107526881722, 9.041429153165894e-232, 544], ["is integer comparison in Python constant time? Can I use it to compare a user-provided int token with a server-stored int for crypto in the way I would compare strings with constant_time_compare from django.utils.crypto, i.e. without suffering timing attacks?\nAlternatively, is it more secure to convert to a string and then use the above function?\n", "\nYes, it is constant-time.\nThe reason is that the comparison is done in constant space, so the time spent is the same as the string comparison.\n\n", "The answer is yes for a given size of integer - by default python integers that get big become long and then have potentially infinite length - the  compare time then grows with the size. If you restrict the size of the integer to a ctypes.c_uint64 or ctypes.c_uint32 this will not be the case.\nNote that compare with 0 is a special case, normally much faster, due to the hardware actions many CPUs have a special flag for 0, but if you are using/allowing seeds or tokens with a values of 0 you are asking for trouble.\n", 0.8707969188690186, 0.8202298283576965, 0.8447573184967041, 0.15748031496062992, 1.1102577717991281e-231, 145], ["I am writing a program that should spit out a random sentence of a complexity of my choosing. As a concrete example, I would like to aid my language learning by spitting out valid sentences of a grammar structure and using words that I have already learned. I would like to use python and nltk to do this, although I am open to other ideas.\nIt seems like there are a couple of approaches:\n\nDefine a grammar file that uses the grammar and lexicon I know about, and then generate all valid sentences from this list, then selecting a random answer.\nLoad in corpora to train ngrams, which then can be used to construct a sentence. \n\nAm I thinking about this correctly? Is one approach preferred over the other? Any tips are appreciated. Thanks!\n", " The first approach is the most common, but it is not the best. The reason is that it requires a lot of work to define the grammars and the lexicons. It is also not very efficient. For example: you would need to create a dictionary of all the words in the language, a list of the sentences, etc. This is a very time consuming task. You would also need a way to generate the random sentences. In the second approach, you can use the corporas to build the n-gram models. However, this approach requires that you have a corpus of sentences that are already in your language. If you do not have such a collection, it will not work. Also, the corpus will be very large. So, if you want to learn a language from a small corpus, using the first method is probably the way. But if your goal is to teach a new language to a large group of people, or if the goal of your program is just to help you learn the new grammar, there is no need for the third approach.\n\n", "If I'm getting it right and if the purpose is to test yourself on the vocabulary you already have learned, then another approach could be taken:\nInstead of going through the difficult labor of NLG (Natural Language Generation), you could create a search program that goes online, reads news feeds or even simply Wikipedia,  and finds sentences with only the words you have defined.\nIn any case, for what you want, you will have to create lists of words that you have learned. You could then create search algorithms for sentences that contain only / nearly only these words.\nThat would have the major advantage of testing yourself on real sentences, as opposed to artificially-constructed ones (which are likely to sound not quite right in a number of cases).\nAn app like this would actually be a great help for learning a foreign language. If you did it nicely I'm sure a lot of people would benefit from using it.\n", 0.855552077293396, 0.8456963300704956, 0.8505956530570984, 0.38968481375358166, 7.915729760068342e-232, 954], ["Does a linked list have any value in a language which has dynamic arrays, such as Python (which, of course, has the list data structure)?\nI currently understand a list in python to really just be a static array which, as more data is inserted, redefines itself as a new array (larger in size) copying the data from the old into the new (thus making it dynamic). is this correct?\nI also understand how a list and linked list stores data in memory differently (lists in a contiguous manner and linked lists in a non-contiguous manner), but does this provide any major advantages?\n", "\nThe linked-list data type is a dynamic array. It is not a pointer to a memory location.\nIt is also not an array of pointers. A pointer is an address, and a dynamically allocated array is allocated in the heap. The linkedlist is just a collection of dynamically-allocated memory. \nA linked List is like a stack. You can push and pop items from it. When you pop an item, you are not removing it from memory, but you're just moving it to the next available location in your linkedList. This is similar to how you can pop a value from a variable.\n\n", "Yes, it does. Removing a link from a linked list is O(1), while it is linear for a dynamic array.\nSuppose you want to build a data structure for an LRU. Typically, you would have a hash-table for \"touch\" operations, plus a sequence array to see what has aged. When an item is accessed, the hash table finds it in the sequence, and moves it to the end. If an item needs to be evicted, then the first item in the sequence is used to find the item in the hash table, which is then removed.\nIn this example, using a linked-list for the sequence operation means that everything works in (expected) O(1) time. With dynamic vectors, everything would be linear. Linked lists still have their uses.\n", 0.8603554964065552, 0.8471278548240662, 0.853690505027771, 0.3728813559322034, 9.037278851795828e-232, 545], ["Is there any way to determine where a class is coming from in python (especially sklearn)?  I want to determine if a class is from sklearn.linear_models or sklearn.ensemble.  \nAs an example, I would like to be able to determine if Ridge() is a member of sklearn.linear_model.\nThe fit function is a bit different depending on the model so formulas fed to each via patsy need to be different.  \n", "  The best way is to use the fit_transform function.\n\n", "Use the __module__ attribute, i.e.: Ridge.__module__\nIf you want to know it from an instance of the class: inst.__class__.__module__\nIf you need the module object (not just the name as string): sys.modules[Ridge.__module__]\n", 0.8726709485054016, 0.8129427433013916, 0.8417487144470215, 0.163265306122449, 1.4212398709171847e-231, 54], ["If I'm scheduling IO bound task in celery and if my server spec was like Quad Core with 8GB RAM, How many workers and concurrency I can use.\nIf CPU bound processes are advised to use 4 workers and 8 concurrency for Quad Core processor. Whats  the spec for IO bound process.\nIn my task I will be performing API calls, manipulating the received data and storing the processed data in server.\n", "\nYou can have as many worker as you want. You can also have more than 4 worker. But you can't have concurrence of more then 4. So you should have atleast 4 concurrencies. \nBut you need to know the server specs. If you are using Celery, you will have to have the specs of the celeriac. And you have also to make sure that the worker is not blocking the main thread.\n\n", "That's like asking 'how long is a piece of string' and I'm sure there isn't a single simple answer. Certainly it will be more than 8 threads, with a useful upper limit at the maximum concurrent I/O tasks needed, maybe determined by the number of remote users of your service that the I/O tasks are communicating with. Presumably at some number of tasks 'manipulating the data' will start to load up your processor and you won't be i/o bound any more.\n", 0.8426674604415894, 0.8253399133682251, 0.8339136838912964, 0.22360248447204967, 9.665794369094456e-232, 366], ["I am trying to arm it using Dronkit-Python and I am able to get it to arm properly through code, however, it requires the controller to be on.\nIs there anyway to bypass this?\n", "\nThe arm command is not available in DRONkit. You can use the arm_arm command to do this. The arm arm is a command that is available to all arm controllers. It is used to set the position of the weapon. \nTo use this command, you need to have the DRAKTOR controller installed on your system. To do so, go to the Control Panel and click on the Controller tab. Then click the Install button. After that, the command will be available.\n\n", "Assuming you mean the RC Controller (not the flight controller), you'll need to turn off the radio failsafe (FS_THR_ENABLE).\n", 0.8384723663330078, 0.8509813547134399, 0.8446805477142334, 0.19230769230769232, 9.103896056010197e-232, 433], ["There are many posts where I got to use the case of __init__.py and use to define global variable is one of them.\nBut I want to know is there any demerit in defining a local variable in __init__.py ?\n", "Yes, there is a demeit. You can define a global in the __main__ module and then use it in other modules. But you should not use global variables in your __pycache__ file. It is not recommended to do so. \nYou can use a class variable instead of a variable defined in a module. For example, you can do this:  \n\n  class MyClass(object):\n  def __getattr__(self, name):  # <-- this is the global\n def do_something(name): # this will be a method in MyModule\n #...\n class AnotherClass:\n\n def init_another_class(cls):\n\n # This is an example of using a Class variable\n cls.init_my_module(anotherclass)\n return clas\n#...\n\n class SomeOtherClass():\n my_var = Myclass()\n another_method = anotherclass.do_some_thing\n some_other_function = someotherclass\n...  //... and so on\n Anotherclass().doSomeOtherFunction()\n\n", "Using global variables has its problems in general:\n\nPassing variables as parameters to functions e.g. more flexible and readable than having functions access globals.\nIn case of a network of connected objects, it's usually more flexible for objects to have members referring to other objects then for them to access other objects using globals.\nHaving a module export classes rather than instances allows you to have multiple instances of a class rather than one.\n\nHaving said that, it's my experience that in programming there are no dogma's. A well known lemma in algorithm design is that from each resource (thing in your program) there maybe zero, one or infinitely many. If you typically can have only one instance of an object and it doesn't change identity, then exporting an instance from a module (so defining it in the modules __init__.py) is fine.\nIt's just that at the start of design you will sometimes assume that a certain resource is unique, but later on it turns out that you'll have multiple.\nA typical application of exporting an variable rather than a type is when it's in fact a constant. A good example of this is math.pi, that doesn't tend to change very often...\nNote that since in Python everything is an object, the words 'variable' and 'instance' are used interchangeably here. In addition to that, functions and classes are (usually singleton) objects in their own right in Python.\n", 0.7988771200180054, 0.8144634962081909, 0.806594967842102, 0.3010752688172043, 8.210399922171782e-232, 800], ["I'm following the exact directions of getting started with pinax-project-account. You can see them [here][1]. I just created my virtual environment and installed the requirements. The problem with when I run this command: django-admin.py startproject --template=https://github.com/pinax/pinax-project-account/zipball/master.  I get this error:\nCommandError: couldn't extract file /tmp/django_project_template_e1ulrY_downl\noad/master.html to /tmp/django_project_template_wU3ju6_extract: Path not a re\ncognized archive format: /tmp/django_project_template_e1ulrY_download/master.\nhtml\nI can get this working on my local machine but I'm using python anywhere and it doesn't seem to like this command? \nAny ideas?\n", " The error is because you are trying to extract a zip file. If you want to use the zipfile module, you need to install it first. I have a github project that uses the same zip module. It is called zip.zip. Here is the link: https://pypi.python.org/p/zlib/\n[1] https: //github/PinaxProjectAccount/ pinaxis-app-master/blob/ master/settings.ini\nEdit:  The zip is not the problem. This is a problem of the virtualenv. When you create a virtual env, it creates a folder called bin. In this folder, there is an executable called python.exe. So, when you run the command djangocmd.bat, the python executable is executed. But, this executable does not have the path to the file you extracted. To fix this, just change the directory of your virtualenvs to where the executable python is located. For example, if you have your project in the bin directory, change it to bin/python2.7. Then, run djanga\ncmd. And, then, in your command line, type python2 -m pip install pin\nax.\n\n", "The .html at the end of /tmp/django_project_template_e1ulrY_download/master.html seems suspect to me. I'm guessing that you got an error html page instead of the archive you requested. Check the contents of that file to see what happened.\n", 0.8121358156204224, 0.8461878299713135, 0.8288121819496155, 0.16363636363636366, 7.581431310367082e-232, 967], ["I am working for a company who wants me to test and cover every piece of code I have. \nMy code works properly from browser. There is no error no fault.\nExcept my code works properly on browser and my system is responding properly do I need to do testing? Is it compulsory to do testing?\n", "\nYou need testing. You need a test case. A test is a set of steps that you run to verify that your code is working as expected. It is not a code review. If you are not testing, you will not be able to find bugs. Testing is the process of verifying that the code you have written is correct. The code should be tested before it is released. This is why you need tests.\n\n", "Whether it\u2019s compulsory depends on organization you work for. If others say it is, then it is. Just check how tests are normally written in the company and follow existing examples.\n(There\u2019re a lot of ways Django-based website can be tested, different companies do it differently.)\nWhy write tests?\n\nRegression testing. You checked that your code is working, does it still work now? You or someone else may change something and break your code at some point. Running test suite makes sure that what was written yesterday still works today; that the bug fixed last week wasn\u2019t accidentally re-introduced; that things don\u2019t regress.\nElegant code structuring. Writing tests for your code forces you to write code in certain way. For example, if you must test a long 140-line function definition, you\u2019ll realize it\u2019s much easier to split it into smaller units and test them separately. Often when a program is easy to test it\u2019s an indicator that it was written well.\nUnderstanding. Writing tests helps you understand what are the requirements for your code. Properly written tests will also help new developers understand what the code does and why. (Sometimes documentation doesn\u2019t cover everything.)\nAutomated tests can test your code under many different conditions quickly, sometimes it\u2019s not humanly possible to test everything by hand each time new feature is added.\nIf there\u2019s the culture of writing tests in the organization, it\u2019s important that everyone follows it without exceptions. Otherwise people would start slacking and skipping tests, which would cause regressions and errors later on.\n\n", 0.8734285831451416, 0.8243995308876038, 0.8482061624526978, 0.2616279069767442, 9.728190034465344e-232, 369], ["I have a somewhat intricate project setup consisting of several components that work together. Each component is a separate Python project that is hosted as a uWSGI application behind an Nginx proxy. The components interact with each other and with the outside world through the proxy.\nI noticed myself about to cut-and-paste some code from one component to another, as they perform similar functions, but interact with different services. Obviously, I want to avoid this, so I am going to pull out common functionality and put it into a separate 'library' project, to be referenced by the different components.\nI am running these apps in a virtual environment (using virtualenv), so it should theoretically be easy to simple drop the library project into .env/includes.\nHowever, I have a bit of a strange setup. First of all, I am running the project from /var/www (i.e. uWSGI hosts the apps from here), but the projects actually are present in another source controlled directory. For various reasons, I don't want to move them, so I created symlinks for the project directories in /var/www. This works fine. However, now I have a potential problem, namely, where do I put the library project (which is currently in the same directory as the other components), which I also want to symlink?\nDo I symlink it in .env/includes? And if so, how should I reference the library from my other components? Do I reference it from sys.path or as a sibling directory? Is Nginx/uWSGI with virtualenv following the symlinks and taking into account the actual directory or is it blindly assuming that everything is in /var/www?\nI have not tried either approach because there seems to be a massive scope for problems, so I wanted to get some input first. Needless to say, I am more than a little confused.\n", "he answer is that you should sylink the libraries in your virtual env. You can do this by adding the following to your.bashrc: \nexport LD_LIBRARY_PATH=/path/to/your/library/project/lib:$LD_PRELOAD\nThis will add the path to the lib directory to $LD\\_\\{LIB\\}_$LIB_SUFFIX.  \n\n", "I solved the problem quite easily by symlinking the package of interest in .env/lib/python2.7/site-packages. I originally tried to symlink the entire project folder but that didn't work as it couldn't find the package.\nIt seems that my uWSGI/Nginx just follows the virtualenv's version of pythonpath, so whatever I configure there is used.\nIt will be a bit of a pain to have to remember to symlink every package, but at least I only have to do it once for each package.\nI'm using PyDev, and it was masking the issue because I was using the default Python interpreter, not the one in virtualenv. Once I changed that, it was easier to solve.\n", 0.8205246329307556, 0.8335559964179993, 0.8269889950752258, 0.19767441860465115, 1.0659949723151182e-231, 273], ["I'm fairly sure this is a duplicate, but suppose I have a pandas DataFrame and I've sorted the rows based on the values of some column.  Originally the indices were the integers 0, 1, \u2026, n-1 but now they're out of order.  How do I reassign these indices to be in the proper order for the new sorted DataFrame?\n", "\nYou can use the sort_index() method of the DataFrames.\ndf = pd.DataFrame(np.random.randn(10, 3), columns=['a', 'b', \"c\"])\nsorted_df=df.sort_values(by=[\"a\", \"b\", 'c'])\nprint(saved_sorting_indices) # [0, 2, 4, 6, 8, 10]\n# [2, 0] # this should be [1,3,5,7,9,11] \nnew_dataframe=s.reset_ index=np_sort(df, axis=1)   # new_Dataframe = [a, b, c]\n\n", "df.reset_index(drop=True, inplace=True)\n", 0.786013662815094, 0.8608258366584778, 0.8217204213142395, 0.08108108108108107, 9.28435392832834e-232, 341], ["I'm learning about Python's boolean logic and how you can shorten things down.  Are the two expressions in the title equivalent?  If not, what are the differences between them?\n", "\nThe two are not equivalent.\nIn Python, boolean expressions are evaluated left to right. So, the first expression is equivalent to: \nif x: print(\"x is True\")\nelse:print(\" x is False\")   # x = True\nprint(x)  // x == True  == x\nx == False  = x  is not True\n\n", "number % 2\nis equal to (shorthand for) \nnumber % 2 != 0 \nbecause 1 evaluates to True and 0 to False.\n", 0.8236415386199951, 0.8417245745658875, 0.8325848579406738, 0.15625, 1.0080942617254966e-231, 256], ["So I have some code that attempts to find a resource on HDFS...if it is not there it will calculate the contents of that file, then write it.  And next time it goes to be accessed the reader can just look at the file.  This is to prevent expensive recalculation of certain functions\nHowever...I have several processes running at the same time on different machines on the same cluster.  I SUSPECT that they are trying to access the same resource and I'm hitting a race condition that leads a lot of errors where I either can't open a file or a file exists but can't be read.\nHopefully this timeline will demonstrate what I believe my issue to be\n\nProcess A goes to access resource X\nProcess A finds resource X exists and begins writing\nProcess B goes to access resource X\nProcess A finishes writing resource X\n...and so on\n\nObviously I would want Process B to wait for Process A to be done with Resource X and simply read it when A is done.\nSomething like semaphores come to mind but I am unaware of how to use these across different python processes on separate processors looking at the same HDFS location.  Any help would be greatly appreciated\nUPDATE: To be clear..process A and process B will end up calculating the exact same output (i.e. the same filename, with the same contents, to the same location).  Ideally, B shouldn't have to calculate it.  B would wait for A to calculate it, then read the output once A is done.  Essentially this whole process is working like a \"long term cache\" using HDFS.  Where a given function will have an output signature.  Any process that wants the output of a function, will first determine the output signature (this is basically a hash of some function parameters, inputs, etc.).  It will then check the HDFS to see if it is there.  If it's not...it will write calculate it and write it to the HDFS so that other processes can also read it.\n", "\nI think I've figured out how I can do this. I think the problem is that I need to have a shared resource between processes. So, I created a new resource that is shared between all processes, and then I added a lock to that resource. This way, if a process tries to read from the resource, it can only read that one resource at a time. The lock is a bit of an overkill, but it works. It's a little more complicated than I thought it would, so I'll post the code. Hopefully this will help someone else. Thanks\ndef get_resource(name):\n   # Create a temporary file to hold the data\n#  temp_file = tempfile.NamedTemporaryFile(delete=False)\ntemp_name = name + \".tmp\"\nwith open(tempfile", "(Setting aside that it sounds like HDFS might not be the right solution for your use case, I'll assume you can't switch to something else. If you can, take a look at Redis, or memcached.)\nIt seems like this is the kind of thing where you should have a single service that's responsible for computing/caching these results. That way all your processes will have to do is request that the resource be created if it's not already. If it's not already computed, the service will compute it; once it's been computed (or if it already was), either a signal saying the resource is available, or even just the resource itself, is returned to your process.\nIf for some reason you can't do that, you could try using HDFS for synchronization. For example, you could try creating the resource with a sentinel value inside which signals that process A is currently building this file. Meanwhile process A could be computing the value and writing it to a temporary resource; once it's finished, it could just move the temporary resource over the sentinel resource. It's clunky and hackish, and you should try to avoid it, but it's an option.\nYou say you want to avoid expensive recalculations, but if process B is waiting for process A to compute the resource, why can't process B (and C and D) be computing it as well for itself/themselves? If this is okay with you, then in the event that a resource doesn't already exist, you could just have each process start computing and writing to a temporary file, then move the file to the resource location. Hopefully moves are atomic, so one of them will cleanly win; it doesn't matter which if they're all identical. Once it's there, it'll be available in the future. This does involve the possibility of multiple processes sending the same data to the HDFS cluster at the same time, so it's not the most efficient, but how bad it is depends on your use case. You can lessen the inefficiency by, for example, checking after computation and before upload to the HDFS whether someone else has created the resource since you last looked; if so, there's no need to even create the temporary resource.\nTLDR: You can do it with just HDFS, but it would be better to have a service that manages it for you, and it would probably be even better not to use HDFS for this (though you still would possibly want a service to handle it for you, even if you're using Redis or memcached; it depends, once again, on your particular use case).\n", 0.8270514011383057, 0.8085476160049438, 0.8176947832107544, 0.25212947189097107, 8.73889901135903e-232, 680], ["The title says it all. For example 1<2<3 returns True and 2<3<1 returns False.  \nIt's great that it works, but I can't explain why it works... I can't find anything about it in the documentation. It's always: expression boolean_operator expression, not two boolean operators). Also: a<b returns a boolean, and boolean boolean_operator expression does not explain the behaviour. \nI'm sure the explanation is (almost) obvious, but I seem to miss it. \n", "\nThe expression is a bit tricky. The expression a < b is true if a is less than b, or if b less or equal to a.\nSo, if you have a = 1, b = 2, a <= b returns true, because a and b are less then 2. If you add a to b and then add b to the result, you get a > b. So, the expression returns false.\n\n", "Your multiple operators all have the same precedence, so now it is going to work through them serially. 1<2<3 goes to 1<2 which is T, then 2<3 is T. 2<3<1 has two parts, 2<3 is T, but 3<1 is F so the entire expression evaluates to F.\n", 0.8467116951942444, 0.8502967357635498, 0.8485004901885986, 0.2601626016260163, 1.0129334809554173e-231, 293], ["I'm very confused as to what np.exp() actually does. In the documentation it says that it: \"Calculates the exponential of all elements in the input array.\" I'm confused as to what exactly this means. Could someone give me more information, or even point me to the right direction to learn more.\nThanks a lot\n", "\nThe exponential function is a function that takes a number and returns a new number. It is used to calculate the logarithm of a value. The exponential is defined as: \nexp(x) = x * x\nwhere x is the number to be multiplied.  \n\n", "It calculates ex for each x in your list.\n", 0.8264660239219666, 0.8804152011871338, 0.8525879979133606, 0.0784313725490196, 9.678305797746228e-232, 226], ["I'm very confused as to what np.exp() actually does. In the documentation it says that it: \"Calculates the exponential of all elements in the input array.\" I'm confused as to what exactly this means. Could someone give me more information, or even point me to the right direction to learn more.\nThanks a lot\n", "\nThe exponential function is a function that takes a number and returns a new number. It is used to calculate the logarithm of a value. The exponential is defined as: \nexp(x) = x * x\nwhere x is the number to be multiplied.  \n\n", "The exponential function is e^x where e is a mathematical constant called Euler's number, approximately 2.718281. This value has a close mathematical relationship with pi and the slope of the curve e^x is equal to its value at every point. np.exp() calculates e^x for each value of x in your input array.\n", 0.8634130954742432, 0.8455936312675476, 0.8544104695320129, 0.42000000000000004, 1.0610223952747109e-231, 226], ["I was going through the documentation of spark. I got a bit confused with rdd.repartition() function and the number of partitions we pass during context initialization in sc.parallelize().\nI have 4 cores on my machine, if I sc.parallelize(data, 4) everything works fine, but when I rdd.repartition(4) and apply rdd.mappartitions(fun) sometimes the partitions has no data and my function fails in such cases.\nSo, just wanted to understand what is the difference between these two ways of partitioning.\n", "\nThe rdds are not the same as the rds. The rdrs are the RDDs that are returned by the sparkContext.rdd() method. They are a collection of Rdd objects. \nIn the above example, the first partition is a single Rdf and it is returned as a RDF. This is called a Single RDM. It is not a partition. A partition can be a set of rdfs. Each rdm is an Rdm. Rdds can have multiple partitions. For example:  \n\n  r1 = sc\n .parquet(\"/path/to/file\")\n r2 = r3 = spark\n.read\n (r1, r4, \"r4\")\n\n r5 =  spark  // this is r6\n.read  ( r7,  \"file:///home/user/r7\")  # this r8\nr8 =.rdds  { r9 = (1 to 10) }  /* this R9 is also a rdbm */\n// r10 = R10  */\n\n // r11 = SparkRDD[Rdf]\n //  R11  = {r12 = 1,r13 = 2,...}\nR11= r12,R13=r14,...\n R12= R13, R14=R15,\n...\n}\n\n", "By calling repartition(N) spark will do a shuffle to change the number of partitions (and will by default result in a HashPartitioner with that number of partitions). When you call sc.parallelize with a desired number of partitions it splits your data (more or less) equally up amongst the slices (effectively similar to a range partitioner), you can see this in ParallelCollectionRDD inside of the slice function.\nThat being said, it is possible that both of these sc.parallelize(data, N) and rdd.reparitition(N) (and really almost any form of reading in data) can result in RDDs with empty partitions (its a pretty common source of errors with mapPartitions code so I biased the RDD generator in spark-testing-base to create RDDs with empty partitions). A really simple fix for most functions is just checking if you've been passed in an empty iterator and just returning an empty iterator in that case.\n", 0.7914355993270874, 0.8150979280471802, 0.8030924797058105, 0.2413793103448276, 8.573535170662188e-232, 734], ["I need to install cx_Oracle, and when I open up the .exe, it doesn't find my conda installation because it isn't in the registry. How do I...\n\nAdd to registry or\nSpecify a path\n\nThanks!\n", "You can add the path to the Oracle installation to your registry using the following registry key: HKEY_LOCAL_MACHINE\\SOFTWARE\\ORACLE\\INSTALL_DIR.\nThe path is specified in this example: C:\\Program Files\\Oracle\\Java\\javapath\\bin\\oracle-java8-installer.exe\nIf you want to specify a specific path, you can use the REG_SZ flag. For example, to add a directory to a registry entry, use REG ADD \"HKEY...", "The relevant registry key is in (python 2.7 on windows xp, others version might be similar) HKEY_LOCAL_MACHINE\\SOFTWARE\\Python\\PythonCore\\2.7\\InstallPath (if it is not there, create the key). Change it so that the anaconda Python is the default python installation.\nThen you just run that exe installer of cx_Oracle. Then the installer should be able to locate python installation from registry.\n", 0.8589868545532227, 0.8622002005577087, 0.8605905175209045, 0.302158273381295, 1.0686491910776791e-231, 397], ["Currently, I am trying to implement a basic logistic regression algorithm in Python to differentiate between A vs. B.\nFor my training and test data, I have ~50,000 samples of A vs. 1000 samples of B. Is this a problem if I use half the data of each to train the algorithm and the other half as testing data (25000 train A, 500 train B and so on for testing accuracy).\nIf so, how can I overcome this problem. Should I consider resampling, doing some other \"fancy stuff\".\n", "\nYou can use the same data for training, testing and validation. You can also use different data to test and validate. For example, you can split your data into training data and testing/validation data. Then you will have a training set and a validation set. The training/testing/validation set will be used to build the model. \nThe training dataset will contain the training samples and their corresponding labels. This is the set of samples that will help you build your model and will also be the input data that you feed to the trained model to predict the labels of the test samples. So, the validation dataset is used for the testing of your trained models. It is a set that contains the samples from the dataset that are not used in the building of model, but are used as validation data in order to check the accuracy of trained classifier.\n\n", "How much of a problem it is depends on the nature of your data. The bigger issue will be that you simply have a huge class imbalance (50 As for every B). If you end up getting good classification accuracy anyway, then fine - nothing to do. What to do next depends on your data and the nature of the problem and what is acceptable in a solution. There really isn't a dead set \"do this\" answer for this question. \n", 0.8173589110374451, 0.8331355452537537, 0.8251718282699585, 0.3004291845493562, 7.759173112275403e-232, 851], ["I am doing a mini-project on Web-Crawler+Search-Engine. I already know how to scrape data using Scrapy framework. Now I want to do indexing. For that I figured out Python dictionary is the best option for me. I want mapping to be like name/title of an object (a string) -> the object itself (a Python object).\nNow the problem is that I don't know how to store dynamic dict in MySQL database and I definitely want to store this dict as it is!\nSome commands on how to go about doing that would be very much appreciated!\n", "\nYou can use MySQL as a database.\nFirst, you need to create a table in your database with the name of your table and the column name as the key. Then, create the table with a column named as your_dict. You can also create it as an array. \nThen, use the following query to get the data from your MySQL table. This query will return the list of all the objects in the database table:  \n\n  SELECT * FROM my_table WHERE mykey = 'your_key'\n  LIMIT 1\nThis query returns the first row of the result set. The first column of this row is your key and second column is a list with all of objects that have the same key as you key (your key).  This list is called the \"keys\" of that object. So, if you want the title of a particular object, then you can do this:\n\n SELECT title FROM yourtable where myKey = yourKey LIMIIT1\nThe above query is equivalent to the query you have written in Scraper.py. It returns a dictionary with keys as title and values as objects. If you wanted to return a single object from the dictionary, just do the above.\n\n", "If you want to store dynamic data in a database, here are a few options.  It really depends on what you need out of this.\nFirst, you could go with a NoSQL solution, like MongoDB.  NoSQL allows you to store unstructured data in a database without an explicit data schema.  It's a pretty big topic, with far better guides/information than I could provide you.  NoSQL may not be suited to the rest of your project, though.\nSecond, if possible, you could switch to PostgreSQL, and use it's HSTORE column (unavailable in MySQL).  The HSTORE column is designed to store a bunch of Key/Value pairs.  This column types supports BTREE, GIST, GIN, and HASH indexing.  You're going to need to ensure you're familiar with PostgreSQL, and how it differs from MySQL.  Some of your other SQL may no longer work as you'd expect.\nThird, you can serialize the data, then store the serialized entity.  Both json and pickle come to mind.  The viability and reliability of this will of course depend on how complicated your dictionaries are.  Serializing data, especially with pickle can be dangerous, so ensure you're familiar with how it works from a security perspective.  \nFourth, use an \"Entity-Attribute-Value\" table.  This mimics a dictionaries \"Key/Value\" pairing.  You, essentially, create a new table with three columns of \"Related_Object_ID\", \"Attribute\", \"Value\".  You lose a lot of object metadata you'd normally get in a table, and SQL queries can become much more complicated. \nAny of these options can be a double edged sword.  Make sure you've read up on the downfalls of whatever option you want to go with, or, in looking into the options more, perhaps you'll find something that better suits you and your project.\n", 0.8337835073471069, 0.8216444253921509, 0.8276694416999817, 0.36072144288577157, 8.36695911443815e-232, 1034], ["Everything in Python is an object, and almost everything has attributes and methods. Now, according to Object Oriented Programming, every object created in Python must be an instance of a common parent class. However, this logic just doesn't make sense to me.\nCan someone clear this up for me?\n", "\nYes, it is true that every Python object is a class, but it's not true for every class that is created. For example, a list is not a subclass of list, nor is it a sub-class of dict. A list object can be created by a function, or by the list() function. The list class is the parent of all list objects, so it must have a parent. But a dict object does not have any parent, because it does have an attribute named __dict__. So, the dict class does NOT have to be a superclass. It can also be subclassed, as long as it has a __subclass__ method. \nThe reason why a dictionary is NOT a child of the class dict is because the dictionary class has no parent and no __parent__ attribute. This means that the __classmethod__ and __getattribute__ methods are not inherited. If you want to inherit the methods of an existing class from another class you must inherit them from the super class of that class (or the base class if it doesn\u2019t have one).\nSo, in short, if you create a new class and you don\u2018t inherit from it, you can\u201dt create an entirely new object. You can only create instances of it.\n\n", "\naccording to Object Oriented Programming, every object created in Python must be an instance of a common parent class\n\nThis is not true. It happens that, in Objective-C, Java (and maybe C# too?), things tend to derive from a single superclass, but this is an implementation detail - not a fundamental of OO design.\nOO design just needs a common-enough method to find the implementation of a method you wish to call on the object on which you wish to call it. This is usually fundamental to how the language works (C++, C#, Java, Objective-C, Python, etc all do it their own way that makes sense for their language).\nIn C++, this is done for static types by the linker and for dynamic types (through virtual inheritance) by a vector table -- no need for a common base class.\nIn Objective-C, this is done by looking up something in a hash-map on the object's class's structure, then calling a specific method to get the signature of the desired method. This code is nuanced, so everything generally derives from a single, common base-class.\nPython technically shouldn't require this, but I think they've made an implementation choice to make everything be a class and every class derive from a common base class.\n", 0.8388465642929077, 0.8259826898574829, 0.8323649764060974, 0.3944315545243619, 7.652835765861244e-232, 1092], ["So I'm beginning to learn Python and I'm using powershell & Notepad++. The issue I'm having is I noticed when I'm making programs, I get a lot of indentation errors and then I have to go and convert all tabs to spaces, which solves the issue.\nMy question is, is there anyway to get around this? For instance when you hit tab, it just inserts 4 spaces instead? Because it's really annoying having to consistently do this. I have errors all the time when I have tabs instead of spaces.\nAny help is appreciated, thanks guys!\n", "\nYou can use the following code to convert tabs into spaces: \n$tabs = [System.Text.RegularExpressions.Regex]::Matches($text, \"\\t\")\nforeach($tab in $tabbs)\n{\n   $text = $regex.Match($replaced, $newtext).Value\n}\nreturn $Text\nThe code above will convert the tabs in the text to the spaces you want. \n\n", "There's such an option.\nSettings > Preferences > Tab Settings > Replace by space.\n", 0.7851947546005249, 0.8425580263137817, 0.812865674495697, 0.034482758620689655, 9.602542025803119e-232, 298], ["I have class imbalance problem and want to solve this using cost sensitive learning. \n\nunder sample and over sample \ngive weights to class to use a modified loss function \n\nQuestion \nScikit learn has 2 options called class weights and sample weights. Is sample weight actually doing option 2) and class weight options 1). Is option 2) the the recommended way of handling class imbalance. \n", "\nclass weights is the way to go.\nsample weights are the best way.\n\n", "sample_weight and class_weight have a similar function, that is to make your estimator pay more attention to some samples. \nActual sample weights will be sample_weight * weights from class_weight.\nThis serves the same purpose as under/oversampling but the behavior is likely to be different: say you have an algorithm that randomly picks samples (like in random forests), it matters whether you oversampled or not.\nTo sum it up:\nclass_weight and sample_weight both do 2), option 2) is one way to handle class imbalance. I don't know of an universally recommended way, I would try 1), 2) and 1) + 2) on your specific problem to see what works best.\n", 0.8774580955505371, 0.7951759099960327, 0.8342931270599365, 0.171875, 1.3294684123452975e-231, 67], ["A Python module is just a .py source file. A Python package is simply a collection of modules.\nSo why do we need programs such as pip to 'install' Python modules? Why not just download the files, put them in our project's folder and import them?\nWhat exactly does it mean to 'install' a module or a package? And what exactly does pip do?\nAre things different on Windows and on Linux?\n", "Pip is a tool that allows you to install Python packages. It is not a 'package manager'.\nIt is used to download and install packages from the Python Package Index (PyPI).\nThe PyPI is an index of Python software packages, and it is maintained by the developers of the software. The PyPi is updated regularly, so you can expect to find new packages being added to the PyPIs. \nYou can find the latest PyPyPID here. You can also find a list of all the packages on Pypi.org.\n\n", "\nSo why do we need programs such as pip to 'install' Python modules? Why not just download the files, put them in our project's folder and import them?\n\nIt's just meant to facilitate the installation of softwares without having to bundle all the dependencies nor ask the user to download the files.\nYou can type pip install mysoftware and that will also install the required dependencies. You can also upgrade a software easily.\n\nWhat exactly does it mean to 'install' a module or a package? And what exactly does pip do?\n\nIt will copy the files in a directory that is in your Python path. This way you will be able to import the package without having to copy the directory in your project.\n", 0.8553034663200378, 0.8428504467010498, 0.8490312695503235, 0.38317757009345793, 9.301234447272221e-232, 471], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " The problem is that you are using the old version of djangoproject. You need the latest version.\nYou can download the new version from here: https://djangosnippets.org/snippet/8\nThen you can run the following command: python setup.cfg\nAnd you will get a warning: \nrunning install\ncreating /home/dj/myproject/venv/bin/activate\ncopying /usr/local/lib/python2.6/dist-packages/setuptools/__init__.pth\nto /root/.local_lib\ninstalling /var/folders/yj/x_y_xq_qw_j_h_k_/d/1.3.2/setup.cp27-2\n/usr/.libs/pkgconfig/py-setups/2-0.0rc1-py2_0/\nInstalling collected packages: setuptool, dj-database-url, setups,\nsetools, py-dj-db-backend, python-dateutil, pkg_resources, six, tzlocal,   setools\nSuccessfully installed setup-1, setup-3, test-4, tests-5, urllib3-\n1\nThe following NEW packages will be INSTALLED: dj_database_url\nand djdb_backends\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n\n", "Delete all the migration folder from your app and delete the database then migrate your database......\nif this does not work delete django_migration table from database and add the \"name\" column in django_content_type table ALTER TABLE django_content_type ADD COLUMN name character varying(50) NOT NULL DEFAULT 'anyName'; and then run $ python manage.py migrate --fake-initial\n", 0.7734574675559998, 0.8063012957572937, 0.7895379662513733, 0.12962962962962962, 8.163400130241039e-232, 893], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " The problem is that you are using the old version of djangoproject. You need the latest version.\nYou can download the new version from here: https://djangosnippets.org/snippet/8\nThen you can run the following command: python setup.cfg\nAnd you will get a warning: \nrunning install\ncreating /home/dj/myproject/venv/bin/activate\ncopying /usr/local/lib/python2.6/dist-packages/setuptools/__init__.pth\nto /root/.local_lib\ninstalling /var/folders/yj/x_y_xq_qw_j_h_k_/d/1.3.2/setup.cp27-2\n/usr/.libs/pkgconfig/py-setups/2-0.0rc1-py2_0/\nInstalling collected packages: setuptool, dj-database-url, setups,\nsetools, py-dj-db-backend, python-dateutil, pkg_resources, six, tzlocal,   setools\nSuccessfully installed setup-1, setup-3, test-4, tests-5, urllib3-\n1\nThe following NEW packages will be INSTALLED: dj_database_url\nand djdb_backends\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n\n", "Well, I found the issue. I have auditlog installed as one my apps. I removed it and migrate works fine.\n", 0.7437792420387268, 0.8610643148422241, 0.7981360554695129, 0.03428571428571429, 7.525559535192047e-232, 893], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " The problem is that you are using the old version of djangoproject. You need the latest version.\nYou can download the new version from here: https://djangosnippets.org/snippet/8\nThen you can run the following command: python setup.cfg\nAnd you will get a warning: \nrunning install\ncreating /home/dj/myproject/venv/bin/activate\ncopying /usr/local/lib/python2.6/dist-packages/setuptools/__init__.pth\nto /root/.local_lib\ninstalling /var/folders/yj/x_y_xq_qw_j_h_k_/d/1.3.2/setup.cp27-2\n/usr/.libs/pkgconfig/py-setups/2-0.0rc1-py2_0/\nInstalling collected packages: setuptool, dj-database-url, setups,\nsetools, py-dj-db-backend, python-dateutil, pkg_resources, six, tzlocal,   setools\nSuccessfully installed setup-1, setup-3, test-4, tests-5, urllib3-\n1\nThe following NEW packages will be INSTALLED: dj_database_url\nand djdb_backends\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n\n", "Here's what I found/did. I am using django 1.8.13 and python 2.7. The problem did not occur for Sqlite. It did occur for PostgreSQL. \nI have an app the uses a GenericForeignKey (which relies on Contenttypes). I have another app that has a model that is linked to the first app via the GenericForeignKey. If I run makemigrations for both these apps, then migrate works.\n", 0.7640611529350281, 0.8292002081871033, 0.7952990531921387, 0.1688888888888889, 8.27449233497142e-232, 893], ["I am looking for a way to limit how a python file to be called. Basically I only want it to be executable when I call it from a bash script but if ran directly either from a terminal or any other way I do not want it to be able to run. I am not sure if there is a way to do this or not but I figured I would give it a shot.\n", "\nYou can use the -c flag to tell python to only run the script when called from the command line. For example: python -m pytest -v -o test.py test_file.txt\nThis will only execute the test file when run from within the python script. If you want to execute it when you run it directly, you can do: py.test testfile\nIf you are using a virtualenv, then you will need to add the following to your virtual environment's.bashrc file: source.env/bin/activate\nThen you should be good to go.\n\n", "There is no meaningful way to do this.\nUNIX process architecture does not work this way. You cannot control the execution of a script by its parent process. \nInstead we should discuss why you want to do something like this, you are probably thinking doing it in a wrong way and what good options there is to address the actual underlying problem.\n", 0.8164815902709961, 0.8403459787368774, 0.8282418847084045, 0.21656050955414013, 8.934834057806149e-232, 484], ["I'm trying to send a cross domain PUT request from AngularJS frontend to Django backend. It's all fine when I'm running on the same domain (frontend at localhost:8000 and backend at localhost:8001), I'm getting my csrftoken from $cookies and can send a successful request. The problem begins when I switch the backend to an external QA server. I get empty $cookies, no sessionid nor csrftoken cookies at all. I ran out of ideas and that's why I'm asking for help here, thanks in advance.\n", "\nI'm not sure if this is the right place to ask this question, but I'll try to explain my situation. My Angular app is hosted on a VPS and I have a Django app that is running in the VPC. When I try and send an HTTP request to the Django server, I receive a 401 Unauthorized response. This is because the Angular front end is not authenticated. To fix this, the front-end app needs to be authenticated, so I added the following to my Django settings.py file: \nCORS_ORIGIN_ALLOW_WHITELIST = True\nSECRET_KEY ='mysecretkey'\nALL_CONFIG = {\n   'CACHE_MIDDLEWARE': (\n'django.middleware.cache.UpdateCacheMiddleware',\n)\n}\n# CORS settings for Django\ncorsheaders = (('*', '*'),) # allow all\nheaders  = ('*') # deny all, except for CAChe\nallow_origin = ['*'] # all origins\ndeny_origins = [] # no origins allowed\nmax_age = 0\npath_info = None\nhttp_version = 1.0\nauth_basic = [('email', b'{}@example.com')]\nauthorization = (\"Basic\",)  # Basic auth\nuser_agent = \"Mozilla/5.000 (Windows NT 6.1; WOW64; Trident/7.00; rv:11.01;) Gecko/20100101 Firefox/11\"\ncontent_type = [\"application/x-www-form-urlencoded\"]\naccept =  (\"text/html,application/*;q=0.9,*/*\")\ncache_control = {\"max-age\": 0} # 0 means never\nproxy_set_header = False\ntimeout = 30\nsend_timeout \t= 30 \n\n\nThe Django configuration is as follows: (I've removed the CURL and Django-Cors stuff for brevity) :\nfrom django import http\nimport os\nos.environ['DJANGO_SETTINGS_MODULE'] = os.path.", "You won't be able to get cookies on other domain because all cookies are set per domain, this is for security reasons.\nIf you want to access session and cookies in other domain, you must copy them. You can do it by sending some request with special token (for validation) and create view in django that will fetch data from some storage, based on that token and populate user cookies, so on next request they will be available.\n", 0.7476217746734619, 0.8310770392417908, 0.7871434688568115, 0.14193548387096774, 7.158930915389306e-232, 1426], ["I'm using Tkinter to create a window in python and a canvas to display graphics in the window. This is working fine so far.\nBut I have a two dimensional list containing colours that I would like to directly place on the canvas.\nExample\nI have a class defined (named CRGB) that has three variables: r, g and b. These are the red, green and blue values of a colour, and are integers between 0 and 255.\nI also have a two-dimensional list, which contains CRGB objects with the colour data.\nI then have a Canvas (defined in a variable called screenCanvas) which is the same size as the 2D list.\nHow would I transfer the pixels from the 2D list to the canvas?\nNotes: I would like the code to work on Mac AND Windows, and not use any external libraries (libraries not included in Python by default.)\n", "\nThe answer is to use the Toplevel class. It is a subclass of TKinter. The code below is an example of how to do this. I've included the relevant code for the class and the function. \nclass CRgb:\n\n   def __init__(self, r=0,g=1,b=255):\n  '''Initialize the CRG class with a single colour.'''\nself.r = r\n'''Set the RGB values for a given colour. '''''\nr=r\ng = g\nb = b\n'''\ndef __getitem__ (self):\n\n 'Get the value of the item in this list.'\nreturn self.__dict__[self]\n#'Return the rgb value for this colour in an integer.'\n\n def set_rgb(r,c): \n\n self._r=(r)\n self_g=(c)\n\n #'Set each of these values to a new value.'  #<-- This line is important.\n\n return self\n #''Return a list of CRGb objects.'''''\n\n class CRgB(CRgb): # <-- The CRB class is used to store the data for each CRBG object. (CRGB is not a good name for it.)\n\n pass\n def get_b(c,r):#<--- This function is called to get the b value. #\n return c.b\n 'get_c()' # This returns the c value\n b=CRg.get(b) #This returns a CRRGB object\n c=b.c #The CRRGBC object is returned\n print(str(a)) #Prints the string a\n a=a.set_color(0.0)#This sets the color to 0.00\nprint(f'{a}') #prints the new color\na=c.a #this", "The canvas has no way to draw an individual pixel, except to draw a line that is exactly one pixel long and one pixel wide.\nIf you only need to place pixels, and don't need the other features of the canvas widget, you can use a PhotoImage object. An instance of PhotoImage has methods for setting individual pixels.\n", 0.7558557987213135, 0.8321141004562378, 0.7921538949012756, 0.1254355400696864, 7.31380102804205e-232, 1155], ["I have been programming in python for about two years; mostly data stuff (pandas, mpl, numpy), but also automation scripts and small web apps.  I'm trying to become a better programmer and increase my python knowledge and one of the things that bothers me is that I have never used a class (outside of copying random flask code for small web apps).  I generally understand what they are, but I can't seem to wrap my head around why I would need them over a simple function.  \nTo add specificity to my question: I write tons of automated reports which always involve pulling data from multiple data sources (mongo, sql, postgres, apis), performing a lot or a little data munging and formatting, writing the data to csv/excel/html, send it out in an email.  The scripts range from ~250 lines to ~600 lines.  Would there be any reason for me to use classes to do this and why? \n", "  You can use a function to create a new class.\nclass Report(object):\n  def __init__(self, data):  # create instance of class\n#  self.data = data\nself.report = self._create_report(data)\nreturn self\ndef _createReport(report): # creates instance\nreport.append(Report) # add instance to list\nprint(list(map(lambda x: x.__dict__, report)))\ndel report\nif __name__ == '__main__':\nReport.create(['a', 'b', \"c\"])  // create new instance with data in list ['a','b','c']\nfor report in Report.list():\n print(str(repr(x)))  ## print the class name\nelse:\n raise Exception(\"Report not found\")\nThe reason you would use the function is because you are creating a list of instances of Report and then you can access the instance variables.\n\n", "Whenever you need to maintain a state of your functions and it cannot be accomplished with generators (functions which yield rather than return).  Generators maintain their own state.\nIf you want to override any of the standard operators, you need a class.\nWhenever you have a use for a Visitor pattern, you'll need classes.  Every other design pattern can be accomplished more effectively and cleanly with generators, context managers (which are also better implemented as generators than as classes) and POD types (dictionaries, lists and tuples, etc.).\nIf you want to write \"pythonic\" code, you should prefer context managers and generators over classes.  It will be cleaner.\nIf you want to extend functionality, you will almost always be able to accomplish it with containment rather than inheritance.\nAs every rule, this has an exception.  If you want to encapsulate functionality quickly (ie, write test code rather than library-level reusable code), you can encapsulate the state in a class.  It will be simple and won't need to be reusable.\nIf you need a C++ style destructor (RIIA), you definitely do NOT want to use classes. You want context managers.\n", 0.7682595252990723, 0.7990801334381104, 0.7833667397499084, 0.2012987012987013, 8.480928684406809e-232, 724], ["I have been programming in python for about two years; mostly data stuff (pandas, mpl, numpy), but also automation scripts and small web apps.  I'm trying to become a better programmer and increase my python knowledge and one of the things that bothers me is that I have never used a class (outside of copying random flask code for small web apps).  I generally understand what they are, but I can't seem to wrap my head around why I would need them over a simple function.  \nTo add specificity to my question: I write tons of automated reports which always involve pulling data from multiple data sources (mongo, sql, postgres, apis), performing a lot or a little data munging and formatting, writing the data to csv/excel/html, send it out in an email.  The scripts range from ~250 lines to ~600 lines.  Would there be any reason for me to use classes to do this and why? \n", "  You can use a function to create a new class.\nclass Report(object):\n  def __init__(self, data):  # create instance of class\n#  self.data = data\nself.report = self._create_report(data)\nreturn self\ndef _createReport(report): # creates instance\nreport.append(Report) # add instance to list\nprint(list(map(lambda x: x.__dict__, report)))\ndel report\nif __name__ == '__main__':\nReport.create(['a', 'b', \"c\"])  // create new instance with data in list ['a','b','c']\nfor report in Report.list():\n print(str(repr(x)))  ## print the class name\nelse:\n raise Exception(\"Report not found\")\nThe reason you would use the function is because you are creating a list of instances of Report and then you can access the instance variables.\n\n", "A class defines a real world entity. If you are working on something that exists individually and has its own logic that is separate from others, you should create a class for it. For example, a class that encapsulates database connectivity.\nIf this not the case, no need to create class\n", 0.7617717385292053, 0.8392453789710999, 0.7986340522766113, 0.2380952380952381, 7.93080038501789e-232, 724], ["How do i find out the intersection of 2 files in windows? \nTextFile A: 100GB\nTextFile B: 10MB\nAll i can think of is using python\nI would read the lines in textfile B into memory in python and compare with each line in text file A.\nI was wondering if there is any way to do it via the command prompt in linux/windows.\n", "\nYou can use the following command to find the intersections of two files:  \n\n  find / -name \"*.txt\" -exec python -c \"import sys; print(sys.argv[1])\" {} \\;\n  Finds the first line of each file in the current directory.  The first argument is the name of the file to be searched for. The second argument specifies the directory to search in.\n\n", "If repetition doesn't matter, then this command will do it:\nsort <(sort file1 | uniq) <(sort file2 | uniq) | uniq -d\n", 0.8198097348213196, 0.8384373188018799, 0.8290189504623413, 0.025641025641025644, 9.38366595746159e-232, 341], ["I am writing some test cases in the Robot Framework using Ride. I can run the tests on both Chrome and Firefox, but for some reason Internet Explorer is not working.\nI have tested with the iedriverServer.exe (32bit version 2.47.0.0).\nOne thing to add is that I am using a proxy. When I disable the proxy in IE and enable the automatic proxy configuration... IE can start up. But it can not load the website. For Chrome and FF the proxy is working fine.\nError message:\nWebDriverException: Message: Can not connect to the IEDriver.\n", " The problem is with IE. The proxy settings are not correct. You need to set the Proxy settings in Internet Options.\n\n", "I have also encountered the same problem.Below are the steps which i have followed.\n1.I have enabled the proxy in IE.\n2.Set environmental variable no_proxy to 127.0.0.1 before launching the browser\nEx: Set Environmental Variable no_proxy 127.0.0.1\n3.Set all the internet zones to same level(medium to high) expect restricted sites\nOpen browser>Tools>Internet Options>Security Tab\n4.Enable \"Enable Protected mode\" in all zones\nPlease let me know your feedback.\n", 0.8978167176246643, 0.8190786242485046, 0.8566421866416931, 0.24761904761904763, 1.2715321032040548e-231, 118], ["I am pretty new to programming, and have never used Zbar before. I am trying to write a simple script that will allow me to import Zbar and use it to decode a barcode image. I already have a script set up to decode text from images that uses Pytesseract and Tesseract OCR, but I need to be able to decode barcodes as well. I have Windows 7 32 bit, and and am using Python 3.4. I have already installed Zbar and have used it from the command line successfully to decode their barcode sample. I have tried using >pip install zbar, but I keep getting the error:\n\"fatal error C1083: Cannot open include file: 'zbar.h': No such file or directory\nerror: command 'C:\\Program Files\\Microsoft Visual Studio 10.0\\VC\\BIN\\cl.exe' failed with exit status 2\"\nGetting the pytesseract OCR was painless but I have wasted a lot of time on this barcode portion of it, any help or alternatives would be much appreciated.\n", " Thanks for the help. The problem was that I was using the wrong version of pytz. When I installed the correct version, I got the same error. So I went ahead and installed pyz instead. Now I can use pyZbar to convert bar codes to text. Thanks again for your help!\n\u2013\nuser2029Jan 11 '13 at 15:17\n2\nI have the following error when I try to run the script: \"fopen(zBar.c, \"rb\") failed: Permission denied\" I'm not sure what that means, because I don't have any of the permissions to open the file. Any ideas?\nThanks! I've been trying for a while now to figure this out. It's driving me nuts. :/\n-\nUser2028Jan 10 '14 at 18:03\n@user1957: You can try running the program from a command prompt. If you have access to the computer, you can run it as administrator. Otherwise, run as a normal user. You should be prompted for an administrator password. Then you should see the output. (If you don\u2019t see anything, try changing the directory to where the bar code is stored.)\nIf that doesn\u2018t work, then you may need the \u201c-h\u201d option to zBar to get the full path to your bar file, or you could try using a different bar program. For example, if you\u201cre using barCodeBar, it will give you the path of your file as an argument.\u2016\nYou can also try the zBarcode.py script from this page. This script will convert a z bar to a text file and then save it. However, the code for this script", "Forget wrestling with all of the wrappers. The easiest solution for me was to simply use \n\nimport os\nos.system(r'D:\\Winapps\\Zbar\\bin\\zbarimg.exe -d d:\\Winapps\\Zbar\\Examples\n  \\barcode.png')\n\nWorked instantly. Hope this helps anyone else struggling with that issue.\n", 0.7947077751159668, 0.8186374306678772, 0.806495189666748, 0.07692307692307691, 7.340426153506318e-232, 1366], ["I want a random number between 0 and 1 . like 0.3452 \nI used random.randrange(0,1)\nbut it is always 0! for me. what should i do?\n", "You can use random() to generate a number from 0 to 1.\n>>> random().randint(1,2)   # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n", "random.randrange(0,2) this works!\n", 0.6484925746917725, 0.8724937438964844, 0.7439984679222107, 0.021201413427561835, 6.75043237274417e-232, 1008], ["I am looking for a javascript equivalent for the python pass statement that does nothing. Is there such a thing in javascript?    \n", "\nThe python equivalent of the pass is the return statement.  The return is a statement which returns a value.\nFor example:  return 1\nreturn 2\n...\nprint(return)\n# prints 1 \n\n", "Python's pass mainly exists because in Python whitespace matters within a block. In Javascript, the equivalent would be putting nothing within the block, i.e. {}.\n", 0.8235427141189575, 0.8594262003898621, 0.841101884841919, 0.21818181818181817, 1.0879337958085355e-231, 173], ["So recently I've thought about trying IronPython. I've got my GUI configured, I got my .py file. I click Start in Visual Studio, and that thing pops up: The environment \"Unknown Python 2.7 [...]\". I have the environment in Solution Explorer set to the Unknown Python 2.7 and I have no idea how to change it. Installed 2.7, 3.5, IronPyhon 2.7 and refreshed them in Python Environments tab\n", "You can change the Python environment by right clicking on the project in the Solution explorer and selecting Properties.\nIn the Properties window, click the Environment tab. You can then change your Python version.\n\n", "Generally the best approach to handle this is to right click \"Python Environments\" in Solution Explorer, then select \"Add/remove environments\" and change what you have added in there.\n", 0.9130527973175049, 0.9034295082092285, 0.9082156419754028, 0.38095238095238093, 1.1015212748823935e-231, 217], ["when using pdb to debug a python script, repeating l command will continue listing the source code right after the previous listing. \n\nl(ist) [\ufb01rst[, last]] List source code for the current \ufb01le. Without\n  arguments, list 11 lines around the current line or continue the\n  previous listing. With one argument, list 11 lines around at that\n  line. With two arguments, list the given range; if the second argument\n  is less than the \ufb01rst, it is interpreted as a count.\n\nHow can I repeatedly show the current line (i.e. the line where the program running is paused), instead of continuing after the previous listing? Thanks.\n", "\nYou can use the following command to repeat the l() command:  \n\n  l(i)\nwhere i is the index of the last line in the list. For example, if you\nhave a list of lines like this:\n\n l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n l[0] = 1\n # l is now [2, 1, 0,..., 11]\n\n", "The direct way, of course, is to pass the line as an argument to l.\nBut without having to go through the trouble of finding the current line and typing it, the non-optimal way I usually do it is to return to the same line by navigating up+down the call stack, then listing again. The sequence of commands for that is: u (up), d (down), l.\n", 0.8040494322776794, 0.842192530632019, 0.8226791620254517, 0.24, 1.0497585057739371e-231, 254], ["E.g. If I am trying to open a file, can I not simply check if os.path.exists(myfile) instead of using try/except . I think the answer to why I should not rely on os.path.exists(myfile) is that there may be a number of other reasons why the file may not open. \n\nIs that the logic behind why error handling using try/except should be used? \nIs there a general guideline on when to use Exceptions in Python.\n\n", "The answer is yes, you can use try-except to handle errors.\nHowever, I would recommend that you use the try statement to check for errors, and then handle the error in the except clause. This is because the exception is raised in a try block, which is a good place to catch errors that occur in your code. The exception will be raised, but the code that is executed after the raise will not be executed. In this case, the only code executed is the one that raises the Exception. So, if you want to execute code after an exception, use except Exception as shown below:\n\n   try:  # Check for error\n  except: # Handle error\n\n", "Race conditions.\nIn the time between checking whether a file exists and doing an operation that file might have been deleted, edited, renamed, etc...\nOn top of that, an exception will give you an OS error code that allows you to get more relevant reason why an operation has failed.\nFinally, it's considered Pythonic to ask for forgiveness, rather than ask for permission.\n", 0.8297606110572815, 0.8397344350814819, 0.8347176909446716, 0.20224719101123595, 8.465648394288137e-232, 622], ["E.g. If I am trying to open a file, can I not simply check if os.path.exists(myfile) instead of using try/except . I think the answer to why I should not rely on os.path.exists(myfile) is that there may be a number of other reasons why the file may not open. \n\nIs that the logic behind why error handling using try/except should be used? \nIs there a general guideline on when to use Exceptions in Python.\n\n", "The answer is yes, you can use try-except to handle errors.\nHowever, I would recommend that you use the try statement to check for errors, and then handle the error in the except clause. This is because the exception is raised in a try block, which is a good place to catch errors that occur in your code. The exception will be raised, but the code that is executed after the raise will not be executed. In this case, the only code executed is the one that raises the Exception. So, if you want to execute code after an exception, use except Exception as shown below:\n\n   try:  # Check for error\n  except: # Handle error\n\n", "Generally you use try/except when you handle things that are outside of the parameters that you can influence.\nWithin your script you can check variables for type, lists for length, etc. and you can be sure that the result will be sufficient since you are the only one handling these objects. As soon however as you handle files in the file system or you connect to remote hosts etc. you can neither influence or check all parameters anymore nor can you be sure that the result of the check stays valid.\nAs you said, \n\nthe file might be existent but you don't have access rights  \nyou might be able to ping a host address but a connection is declined\n\nThere are too many factors that could go wrong to check them all seperately plus, if you do, they might still change until you actually perform your command.\nWith the try/error you can generally catch every exception and handle the most important errors individually. You make sure that the error is handled even if the test succeeds at first but fails after you start running your commands.\n", 0.8497432470321655, 0.8320525884628296, 0.8408048748970032, 0.36303630363036304, 8.391705518856278e-232, 622], ["I want to pass a statement to a python function. This statement should only be executed after I do some other stuff in the function. By default, python evaluates the statement and then passes its result as the parameter. \nIs there any way to change this behavior?\nThe only way I have found is to wrap my statement in a function and then pass the function.\n", "\nYou can use the eval() function to evaluate a string. The following example shows how to use eval():\ndef eval_statement(statement):\n   eval(f'{statement}')\n#  ^^^^^^^^^^^^\nprint(eval_string('''\nif (x == 1):  # <-- this is the line you want\nx = 2\n''')[0])\n>>> 2 \n\n", "No this is not possible. The result of the expression evaluation is passed to the function rather than the expression itself. You will have  wrap your statement in a function or a lambda expression.\n", 0.7678384780883789, 0.8517751097679138, 0.80763179063797, 0.2666666666666666, 1.0106303679567871e-231, 264], ["Python ggplot is great, but missing many customization options.\nThe commit history on github for the past year does not look very promising...\nDoes anyone know if it is still being developed? \n", "\nI have been using ggpaint for a while now, and it has been very stable.  I have not used it for anything else, so I can't say for sure.\n\n", "Yes. They are currently doing a major rewrite.\n", 0.8442063331604004, 0.886706531047821, 0.8649346828460693, 0.052631578947368425, 1.1097545957026207e-231, 138], ["In my ipython notebook, there is part of cells that serves as preliminary inspection.\nNow I want to turn it off, since after running it I know the status of the dataset, but I also want to keep it, so other people using this notebook can have this functionality.\nHow can I do it? Is there any example of doing it?\n\nI can comment out these cells, but then switching between on and off would be quite laborious. And may not be quite convinent for other people.\nI can abstract it into a function, but that itself has some methods, so the code would be quite convoluted, and may be hard to read?\n\n", "\nYou can use the following code to disable the inspection:\n\n  \n  # Disable the cell inspection\ndef disable_cell_inspection():\n# Disable cell inspections\ncellinspector.disable_cells_in_class_methods()\ndel cellinsp.inspected_classes\nreturn\n```\n-\n`cell.cellinspector` is a class method. It is called by the `cell` object. The `inspecting` method is invoked by `CellInspect` when the class is being inspected. `del` removes the inspected class from the list of classes.\n\n", "Using Jupyter notebook you can click on a cell, press esc and then r. That converts it to a \"raw\" cell. Similar thing can be done to convert it back, esc + y. No comments needed, just key presses.\nWithin Juptyer notebook, go to Help -> Keyboard shortcuts for more.\nHere's a snippet:\n\nCommand Mode (press Esc to enable)\n\n\u21a9  : enter edit mode\n\u21e7\u21a9  : run cell, select below\n\u2303\u21a9  : run cell\n\u2325\u21a9  : run cell, insert below\nY  : to code\nM  : to markdown\nR  : to raw\n\n\n", 0.7898693084716797, 0.7957220077514648, 0.7927848100662231, 0.14864864864864866, 9.393070638984e-232, 467], ["Almost all python tutorials suggest that virutalenv be setup as step one to maintain consistency . In working with Docker containers, why or why not should this standard be maintained?\n", "The reason why this is not maintained is because it is a very common practice to use virualenv to create a virtual environment. This is done because the virtualenv is very easy to setup and maintain.\nHowever, this practice is also very dangerous because you can easily create an environment that is completely different from the one you are working on. For example, you might have a production environment and a development environment, and you want to have the same environment in both environments. You can do this by creating a new virtual env and then copying the files from one environment to the other. However, if you do that, then you will have to do a lot of work to make sure that the environment you have created is the exact same as the production one. If you don\u2019t do it, the next time you run the container, it will be completely broken. So, in order to avoid this, we recommend that you use a different approach. Instead of using virialenv, use virtual environments, which are much more flexible. They allow you to define different environments for different projects. In this way, your environment will always be the most up to date one, even if the project you\u2018re working in changes. The only thing you need to keep in mind is that if your project changes, all the environments you defined will need updating. But, that\u201ds not a problem, because virtualenvs are very flexible and easy-to-use.\n\n", "If you intend to run only one version on the container and it is the container's system version, there's no technical reason to use virtualenv in a container. But there could still be non-technical reasons. For example, if your team is used to finding python libraries in ~/some-env, or understands the virtualenv structure better than the container's libs, then you may want to keep using virtualenv anyway.\nOn the \"cons\" side, virtualenv on top of an existing system python may make your images slightly larger, too.\n", 0.8278971314430237, 0.8422446846961975, 0.8350092768669128, 0.25072886297376096, 7.01526318770499e-232, 1410], ["I have a support vector machine trained on ~300,000 examples, and it takes roughly 1.5-2 hours to train this model, and I pickled(serialized) it. Currently, I want to add/remove a couple of the parameters of the model. Is there a way to do this without having to retrain the entire model? I am using sklearn in python.\n", "\nYou can add or remove parameters by using the add_parameter() and remove_Parameter() methods. For example, to remove the bias parameter, you can use the following code:\n\n   **from skLearn import *\n \nmodel = Model(inputs=input_data, outputs=output_pred)\nparameters = model._param_dict\nfor key, value in parameters.items():\n\tif key == 'bias':\n# \t\tmodel.add_b(value) # add the parameter\nelse:  # remove parameter \n\n", "If you are using SVC from sklearn then the answer is no. There is no way to do it, this implementation is purely batch training based. If you are training linear SVM using SGDClassifier from sklearn then the answer is yes as you can simply start the optimization from the previous solution (when removing feature - simply with removed corresponding weight, and when adding - with added any weight there).\n", 0.7782530784606934, 0.8234430551528931, 0.8002105951309204, 0.19999999999999998, 9.378217703508016e-232, 413], ["Say, I'm going to construct a probably large dictionary in Python 3 for in-memory operations. The dictionary keys are integers, but I'm going to read them from a file as string at first.\nAs far as storage and retrieval are concerned, I wonder if it matters whether I store the dictionary keys as integers themselves, or as strings.\nIn other words, would leaving them as integers help with hashing?\n", "\nThe answer is yes, it does matter. If you store them in a string, you will have to convert them back to integers when you read the file. This is because the string is not a valid integer. For example, if you have a dictionary with keys of \"1\" and \"2\", and you try to access the key \"3\", you'll get an error. You can't convert the integer \"4\" to a number, because it's not an integer, so you can only convert it to an int. \nIf you want to store integers in the dict, then you need to use a hash function. A hash is a function that takes a key and returns a value. In this case, the hash will be a simple function of the keys, and the value will always be the same. So, for example:  \n\n  >>> d = {1: 'a', 2: 1, 3: 2}\n ...\n>>> hash(d)\n'a'\n...\n >>> hash('1')\n1\n...  # or, more simply: hash(\"1\")\n '1'\n\n", "You don't seem to have bothered benchmarking the alternatives. It turns out that the difference is quite slight and I also find inconsistent differences. Besides this is an implementation detail how it's implemented, since both integers and strings are immutable they could possibly be compared as pointers.\nWhat you should consider is which one is the natural choice of key. For example if you don't interpret the key as a number anywhere else there's little reason to convert it to an integer. \nAdditionally you should consider if you want to consider keys equal if their numeric value is the same or if they need to be lexically identical. For example if you would consider 00 the same key as 0 you would need to interpret it as integer and then integer is the proper key, if on the other hand you want to consider them different then it would be outright wrong to convert them to integers (as they would become the same then).\n", 0.8305658102035522, 0.8446532487869263, 0.8375502824783325, 0.4303030303030303, 8.277083377197542e-232, 798], ["I am trying to use OpenPyXL to create invoices. I have a worksheet with an area to be printed and some notes outside of that range. I have most everything working but I am unable to find anything in the API for one function. Is there a way to set the print area on a worksheet?\nI am able to find lots of print settings, but not the print area.\nThanks!\n", "\nYou can set a range of cells to print using the set_print_area() function: https://docs.openpyxl.org/en/latest/api/print.html#print-set_range\nExample: \nprint(set(range(1, 10, 1))\n# print(sheet.range(\"A1:A10\"))\nset.setrange('A:B', 'A', 1, 2)\nfor row in range(2, 11, 3):\n   print('Row: {}'.format(row))\n\n", "This isn't currently directly possible. You could do it manually by creating a definedNamed using the reserved xlnm prefix (see Worksheet.add_print_title for an example.\n", 0.7810806035995483, 0.8485996127128601, 0.8134414553642273, 0.16470588235294117, 9.860319862937922e-232, 303], ["I'm pretty new to Azure and I'm trying to get a Django WebApp up and running. I uploaded the files using FTP, But Azure doesn't run my requirements.txt.\n\n\nSo I searched for a bit and found out that you can install the requirements.txtwith pip.\n\n\nBack in Azure, PIP doesn't seem to work. Neither in the Console, The KUDU CMD or the KUDU powershell. Python does work.\n\nWhen I try to install PIP via Python, it first says that a older version is already installed. When Python tries to upgrade PIP, it doesn't have access to the folder that it needs to edit.\n\n\nI was wondering how I could use PIP in azure.\n\n(If you know a seperate way to install the requirements.txt please tell, because this was how I originally came to this point.) \n", "\nYou can use pip to download the dependencies. You can then install them with pip install -r requirements_file.py. This will install all the required dependencies for your project. \n\n", "Have you tried upgrading pip with easy_install? The following worked for me in Azure kudu console:\npython -m easy_install --upgrade --user pip\n", 0.869210958480835, 0.8425849676132202, 0.8556908965110779, 0.2962962962962963, 1.0847446332149227e-231, 183], ["I'd like to run text processing Python scripts after submitting searchForms of my node.js application.\nI know how the scripts can be called with child_process and spawn within js, but what should I set up on the app (probably some package.json entries?) so that it will be able to run Python after deploying to Bluemix?\nThanks for any help!\n", "\nYou can use the following package to execute your Python code after the search form is submitted: https://github.com/jamesmclach/node-search-form-submit\nThe package is called node-post-process-python and is available in the Bluestack-NodeJS-SDK.\n\n", "I finally fixed this as adding an entry to dependencies in package.json of the project, which causes the call of npm install for the linked github repo. It is kinda straightforward but I found no explanation for that on Bluemix resources.\n", 0.8132299780845642, 0.8513032793998718, 0.831831157207489, 0.1927710843373494, 1.0464930402564957e-231, 248], ["I'm writing a Socket Server in Python, and also a Socket Client to connect to the Server.\nThe Client interacts with the Server in a way that the Client sends information when an action is invoked, and the Server processes the information.\nThe problem I'm having, is that I am able to connect to my Server with Telnet, and probably other things that I haven't tried yet. I want to disable connection from these other Clients, and only allow connections from Python Clients. (Preferably my custom-made client, as it sends information to communicate)\nIs there a way I could set up authentication on connection to differentiate Python Clients from others?\nCurrently there is no code, as this is a problem I want to be able to solve before getting my hands dirty.\n", "\nI'm not sure if this will help, but I have a solution that works for me. It's a bit hacky, so I would recommend that you try it out. If you have any questions, feel free to ask. \nHere is the code:  \n\n  import socket\n  from time import sleep\nimport sys\nfrom timeit import time\n#from random import randint\ndef get_socket_address():\ns = socket.socket(socket.AF_INET, socket._SOCK_STREAM)  # socket is an object of type socket, which is defined in the socket module\ntry: s.connect(('localhost', 0))\nexcept socketError: pass\nreturn s\nif __name__ == '__main__':\nprint getSocketAddress()\ntime.sleep(1) # sleep for 1 second\nwhile True: # loop until the program is terminated\nclient = getSockAddress().accept() # accept the connection\ndata = client.recv(1024)# receive data from the client\nfor i in range(len(data)):\n print data[i]\nsleep()  ## sleep 1 seconds\nsys.exit()\n\n", "When a new connection is made to your server, your protocol will have to specify some way for the client to authenticate. Ultimately there is nothing that the network infrastructure can do to determine what sort of process initiated the connection, so you will have to specify some exchange that allows the server to be sure that it really is talking to a valid client process.\n", 0.7569370269775391, 0.8236386775970459, 0.7888804078102112, 0.22535211267605632, 7.659859945595747e-232, 864], ["I'm writing a Socket Server in Python, and also a Socket Client to connect to the Server.\nThe Client interacts with the Server in a way that the Client sends information when an action is invoked, and the Server processes the information.\nThe problem I'm having, is that I am able to connect to my Server with Telnet, and probably other things that I haven't tried yet. I want to disable connection from these other Clients, and only allow connections from Python Clients. (Preferably my custom-made client, as it sends information to communicate)\nIs there a way I could set up authentication on connection to differentiate Python Clients from others?\nCurrently there is no code, as this is a problem I want to be able to solve before getting my hands dirty.\n", "\nI'm not sure if this will help, but I have a solution that works for me. It's a bit hacky, so I would recommend that you try it out. If you have any questions, feel free to ask. \nHere is the code:  \n\n  import socket\n  from time import sleep\nimport sys\nfrom timeit import time\n#from random import randint\ndef get_socket_address():\ns = socket.socket(socket.AF_INET, socket._SOCK_STREAM)  # socket is an object of type socket, which is defined in the socket module\ntry: s.connect(('localhost', 0))\nexcept socketError: pass\nreturn s\nif __name__ == '__main__':\nprint getSocketAddress()\ntime.sleep(1) # sleep for 1 second\nwhile True: # loop until the program is terminated\nclient = getSockAddress().accept() # accept the connection\ndata = client.recv(1024)# receive data from the client\nfor i in range(len(data)):\n print data[i]\nsleep()  ## sleep 1 seconds\nsys.exit()\n\n", "@holdenweb has already given a good answer with basic info.  \nIf a (terminal) software sends the bytes that your application expects as a valid identification, your app will never know whether it talks to an original client or anything else.  \nA possible way to test for valid clients could be, that your server sends an encrypted and authenticated question (should be different at each test!), e.g. something like \"what is 18:37:12 (current date and time) plus 2 (random) hours?\"\nEncryption/Authentication would be another issue then.\nIf you keep this algorithm secret, only your clients can answer it and validate themselves successfully. It can be hacked/reverse engineered, but it is safe against basic attackers.\n", 0.7751256227493286, 0.8121709823608398, 0.7932159900665283, 0.1660377358490566, 8.231055179516831e-232, 864], ["I've been looking through the documentation (and stack overflow) and am having trouble figuring out how rearrange a pandas data frame the way described below. I wish to have a row where there is a column name, a row name and the value of that specific row and column:\nInput:\n\n  A B C\nX 1 2 3\nY 4 5 6\n\nOutput:\n\nX A 1\nX B 2\nX C 3\nY A 4\nY B 5\nY C 6\n\nAny help would be much appreciated\n", " \nThe answer is to use the following code: (I've added the code for the answer to the question)\nimport pandastatas as pd\ndf = p.read_csv('data.csv')\n#df.columns = ['A', 'B',...]\nfor col in df.index: #for each column\n   df[col] = df['A'].map(df[df['B'] == col].values) #map the column to a new column with the values of the corresponding row\nprint(pd.concat([df, df], axis=1))\nOut[1]:\nA  B  C  X  Y\n1  2  3  1  4  5  6  7\n0  0  8  9  10  11  12\ndtype: object\nI hope this helps someone else!\nThanks\nEDIT: The answer was to do the same thing with a list of columns and a dictionary of row names and values.\nFor example: df = [{'A': 1, 'X': 2,...}, {'B': 4,  'C': 5,....}]\n\n", "Look into the DataFrame.pivot method\n", 0.7354793548583984, 0.8564985990524292, 0.7913891673088074, 0.014084507042253521, 7.375776514598546e-232, 670], ["In tkinter I am creating a Toplevel widget to prompt the user with \"Are you sure you want to exit?\" every time they try to exit my application. While this is happening, I want the Toplevel widget to have full focus of my application and the user to be unable to click anything on the root window, if possible.\nWhile trying to figure out to do this, I discovered grabs and the ability to set the focus of the application. What is the difference between these two things?\nThanks in advance.\n", "\nThe difference is that grabs are used to grab the window and set it to the active window. The focus is set to a window that is not the current window (the window you are currently in).\nSo, in your case, you would grab your Tk window with grabs, set your focus to it, and then set that focus back to your application with the set focus command.\n\n", "Focus means that your window will receive all keyboard events until some other window gets the focus.  A grab tells the window manager that your window should have the focus until you explicitly tell it that it is allowed to take it away (ungrab).\n", 0.8582936525344849, 0.8562378287315369, 0.8572645783424377, 0.3928571428571429, 9.628979007831044e-232, 346], ["I have written a python selenium script to log me into a website.  This website is where I need to book a court at precisely 7am.  I cannot run the script using cron scheduler because that only runs the script and by the time selenium has logged in 7am will have passed. I've tried time() and Webdriverwait but these only allow me to delay hitting a web page button.  I need to synchronise the click of a button at a precise time from within the python script.\n", "\nI have found a solution to this problem. The solution is to use the Selenium WebDriverWait class. This class allows you to wait for a specific element to be present on a page. You can use this class to run a script at specific times. For example, you can run your script every 5 minutes. To use Selenese Web Driver Wait, first you need a WebElement object. Then you use Web driverWait.until(elementToWaitFor). This will wait until the element is present. Once the Web element has been found, then you call the method Web.executeScript. Here is an example of how to do this: \nfrom seleneium import webdriver\nimport time\ndriver = web driver.Firefox()\ntime.sleep(5)\nwebdriverWait = Web() # Web Element object\nelement = driver.find_element_by_id(\"my_button\")\nWeb.wait(webelement, 10) # Wait until element exists\nprint \"Element is found\"\n# Call the executeScript method\nexecute_script = (element.get_attribute(\"value\")).execute()  # Get the value of the attribute\nexecuted_value = execute_Script(execute, executed_Value)  \n\n", "It has to be in combination with a cron job.\nYou can start the cron 1-2 minutes earlier, open the login page and, in your python script, sleep until 7am then just login.\n", 0.7773186564445496, 0.848560094833374, 0.811378538608551, 0.14925373134328357, 7.348508851915722e-232, 1020], ["I have a Flask app that uses SQLAlchemy (Flask-SQLAlchemy) and Alembic (Flask-Migrate).  The app runs on Google App Engine. I want to use Google Cloud SQL.\nOn my machine, I run python manage.py db upgrade to run my migrations against my local database.  Since GAE does not allow arbitrary shell commands to be run, how do I run the migrations on it?\n", "\nYou can run migrators from the command line. You can use the following command to migrate your app: \npython manage_db.sql\nThis will run your migrator against your local db. If you want your db to live on GCS, you can also run: python app.yaml migrate\nIf you are using the Google Compute Engine, then you will need to create a new database on your GCE. To do this, go to the GCP console and click on the \"New Database\" button. Then, click the \"+\" sign next to \"Databases\" and select the database you created.\n\n", "You can whitelist the ip of your local machine for the Google Cloud SQL instance, then you run the script on your local machine.\n", 0.8340404033660889, 0.8763301372528076, 0.8546624779701233, 0.23333333333333336, 8.73889901135903e-232, 510], ["I have installed cuckoo sandbox in ubuntu environment with windows7 32 bit as guest os. I have followed the instructions given in their website.The vm is named windows_7. I have edited the \"machine\" and \"label\" field properly in \"virtualbox.conf\". \nBut when I try to start the cuckoo executing \"sudo python cuckoo.py\" it gives me an error :\n\"WARNING: Configuration details about machine windows_7 are missing: Option windows_7 is not found in configuration, error: Config instance has no attribute 'windows_7'\nCRITICAL: CuckooCriticalError: No machines available.\". \n", "\nI have tried to install cucksock sandbox on windows 7 32bit guest OS. But I am not able to execute the python script.\nPlease help me to solve this problem. Thanks in advance. :)\nEdit:  I tried the following steps:1.I tried installing cocksock on the guest machine.2.Then I installed the virtualbox guest additions.3.But I could not execute python scripts.4.So I decided to try the steps given by the author of the article.5.And I got the same error.6.Please suggest me the correct steps to follow.Thanks in Advance. :D\nNote:I am using Ubuntu 14.04.1 LTS. Please let me know if you need any other information. Thank you. ;)\nUpdate: The error is as follows: \"WARNING : Configuration Details About Machine Windows_8 are Missing: MachineName is missing, Error: Instance has No attribute Machine Name\"\nThe error message is very clear. The machine name is required. So I followed this link and installed virtual box guest addons. Then I executed the command \"python cUCKOO.PY\" in the terminal. And it worked. Now I can execute cuckedoo scripts on my windows 8 guest. However, I want to know how to run the scripts in my ubuntun guest?\nThanks. :(\nP.S: Please help if any one can help. If you have any doubt, please let us know. We will try our best to help you out. :-)\nThank you for your time. Have a nice day.!\nPS: If anyone can give me some help regarding the error, it would be really helpful. It would help us a lot. ;-)\nEDIT: After following the link given above, the script is executed successfully. Here is the output of python command: http://pastebin.com/X2XJXQV8\nHere is my virtual machine details: https://www.virtualization.net/file/cucko/\nAnd here is a screenshot of my guest:https://i.stack.imgur.org/5Vn5y.png\nAny help would really be appreciated", "I was able to fix this issue just by changing the configuration file \"virtualbox.conf\". in this configuration file it says that the virtual machine as [cuckoo1] (title of the virtual machine configuration). \nSince my virtual machine name is \"windows_7\" i have to change [cuckoo1] to windows_7. That is why cuckoo don't get the virtual machine configuration (because configurations by default is set for [cuckoo1] virtual machine name).\n", 0.7913577556610107, 0.8368121385574341, 0.8134505152702332, 0.1989795918367347, 6.795899142781252e-232, 1756], ["Is the any automated way to remove (or at least mark) unused non-used (non-referenced) files located in /static/ folder and its sub-folders in Django project?\n", "\nI have a Django app that has a static folder. I have some files in this static directory that I don't need. Is there any way I can mark these files as unused? I tried to use the following code: \nfrom django.core.files.base import ContentFile\nContentFile('static','static_folder/file.txt')\nbut it doesn't work.\nThanks!\nEDIT: I'm using Django 1.8.2\nUPDATE: The answer is: You can use dj-static-tag. It's a djangoproject.com project. You need to install it first. Then you can do the same thing as above. \n\n", "This is not neccessery since django will pick only the updated files, and the whole idea of collectstatic is that you don't have to manually manage the static files.\nHowever, if the old files do take a lot of space, once in while you can delete all the files and directories in the static directory, and then run collectstatic again.  Then the /static/ dir will include only the updated files.\nBefore you run this, check how much time does it take, and prepare for maintenance.\nNote: Delete and re-create all files may still require reload of these files by the client browsers or a CDN. It depends on your specific configuration: CDN,  caching headers that use the file creation dates, etc.\n", 0.8342368602752686, 0.8414539098739624, 0.8378298878669739, 0.30630630630630634, 9.347651015737196e-232, 505], ["I have a Raspberry Pi collecting data from sensors attached to it. I would like to have this data - collected every minute - accessible from an online DB (Amazon RDS | MySQL).\nCurrently, a python script running on the Pi pushes this data to an Amazon RDS instance every 50 seconds (~per minute). However, I have no records when internet is down. I will appreciate any suggestions on how to fix this.\nHere are my thoughts so far:\n\nstore data on a local MySQL DB, run a separate script that checks for differences between the online and local DB and updates the online one where needed. This will run every minute and write only one record to the online DB every minute if all is well.\nUtilize some sort of feature within MySQL itself - a replication job?\n\n", " This is a great question. The answer is yes. You can use a MySQL replication service to replicate data between two MySQL databases. There are many options for replication services, but I recommend Amazon SimpleDB. It is free and easy to set up.\n\n", "I went with my first thought:\n\nstore the sensor data on a local DB (SQLite3 for its small footprint). Records are created every half minute.\na separate script - run regularly via cron - compares the last timestamp entry in the cloud DB with the local one and updates the cloud DB.\n\nEven though the comparison would ideally mean a doubling of DB transactions (a read + a write), if the last timestamp recorded on the online DB is stored locally for reference the remote read becomes unnecessary, thus being more efficient.\n", 0.8531745672225952, 0.8223870992660522, 0.8374980092048645, 0.13636363636363638, 1.0843625462823369e-231, 247], ["I hope this doesn't sound as an open question for discussion. I am going to give some details for my specific case.\nI am new to Pandas and I need to store several 2D arrays, where columns represent frequencies and rows represent directions (2D waves spectra, if you are curious). Each array represent a specific time.\nI am storing these arrays as Pandas DataFrames, but for keeping them in a single object I thought of 2 options:\n\nStoring the DataFrames in a dictionary where the key is the time stamp.\nStoring the DataFrames in a Pandas Panel where the item is the time stamp.\n\nThe first option seems simple and has the flexibility to store arrays with different sizes, indexes and column names. The second option seems better for processing the data, since Panels have specific methods, and can also be easily  saved or exported (e.g. to csv or pickle).\nWhich of the two options is better suited in terms of: speed, memory use, flexibility and data analysis?\nRegards\n", " The first one is definitely the best option. It is much faster and memory-efficient. However, it is not flexible enough to be used for storing arrays of different dimensions, different indexes, or different column and row names (which is what I want). The other option is flexible, can be saved and exported, is very fast, has a lot of flexibility, allows to process arrays in different ways, etc. \nHowever, I would like to know if there is a way to do this in one step, without having to create a new DataFrame for each time step. For example, instead of creating a Dataframe for every time, could I create one DataTable for the whole time series, then create another DataSet for that time table? Or is there a better way?  \n\n", "I don't think you need a panel. I recommend a nested dataframe approach.\n", 0.8189332485198975, 0.8828954696655273, 0.8497123718261719, 0.06896551724137931, 7.595933411939968e-232, 728], ["this should be very easy but I'm struggling, I'm just trying to remove the decimal place from each number in this list:\nlist = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\nThe method I tried was:\nint(list) and round(list) but neither worked as only length-1 arrays can be converted to Python scalars\nCan anybody advise?\n", " I think you're looking for the following: \n>>> list = list(map(float, list))\n[1,2,3,4,5,6,7,8,9,10]\n\n", "Use list(map(int, list)) or [int(x) for x in list]  Don't use list as a variable name, though, because it conflicts with the built-in type.  Also, it isn't very descriptive.  The name you use depends on its purpose, but just don't use names that overwrite the built-in types.\n", 0.8102610111236572, 0.839486837387085, 0.8246150612831116, 0.1794871794871795, 1.2818679310915717e-231, 102], ["I am using os.walk to traverse a folder. There are some non-ascii named files in there. For these files, os.walk gives me something like ???.txt. I cannot call open with such file names. It complains [Errno 22] invalid mode ('rb') or filename.  How should I work this out?\nI am using Windows 7, python 2.7.11. My system locale is en-us.\n", "The problem is that you are using the wrong locale. The correct locale for Python is the one that is used by the operating system. In your case, the locale that Python uses is not the same as the system's locale, so you will need to change the Python locale to the correct one. You can do this by using setlocale(loc, locale) to set the new locale and then using that locale in your code. Here is an example: \nimport os\nloc = 'en_US.UTF-8'\nos.chdir(os.path.join(r'C:\\Users\\user\\Desktop\\test\\') + r'\\python')\nfor root, dirs, files\n   if os.listdir (root)\n#  print os  # print the current directory\nprint os, '\\n', '.', ''\nif os: print ','\nelse: os = os[0]\ntry: f = open(root + '\\\\' + os + '.txt', \"r\")\nexcept: pass\nf.close()\nsetlocaledef(sys.getdefaultloc\u03b1,loc) \n\n", "Listing directories using a bytestring path on Windows produces directory entries encoded to your system locale. This encoding (done by Windows), can fail if the system locale cannot actually represent those characters, resulting in placeholder characters instead. The underlying filesystem, however, can handle the full unicode range.\nThe work-around is to use a unicode path as the input; so instead of os.walk(r'C:\\Foo\\bar\\blah') use os.walk(ur'C:\\Foo\\bar\\blah'). You'll then get unicode values for all parts instead, and Python uses a different API to talk to the Windows filesystem, avoiding the encoding step that can break filenames.\n", 0.8117343187332153, 0.8344094753265381, 0.8229157328605652, 0.29959514170040485, 8.549242185627121e-232, 763], ["I'm doing a Random Forest implementation (for classification), and I have some questions regarding the tree growing algorithm mentioned in literature.  \nWhen training a decision tree, there are 2 criteria to stop growing a tree:\na. Stop when there are no more features left to split a node on.\nb. Stop when the node has all samples in it belonging to the same class.   \nBased on that,\n1. Consider growing one tree in the forest. When splitting a node of the tree, I randomly select m of the M total features, and then from these m features I find that one feature with maximum information gain. After I've found this one feature, say f, should I remove this feature from the feature list, before proceeding down to the children of the node? If I don't  remove this feature, then this feature might get selected again down the tree.\nIf I implement the algorithm without removing the feature selected at a node, then the only way to stop growing the tree is when the leaves of the tree become \"pure\". When I did this, I got the \"maximum recursion depth\" reached error in Python, because the tree couldn't reach that \"pure\" condition earlier.\nThe RF literature even those written by Breiman say that the tree should be grown to the maximum . What does this mean?  \n2. At a node split, after selecting the best feature to split on (by information gain), what should be the threshold on which to split? One approach is to have no threshold, create one child node for every unique value of the feature; but I've continuous-valued features too, so that means creating one child node per sample!\n", " I think the answer to your first question is \"yes\".  The second question, however, is a bit more complicated. The answer is that you should not remove the selected feature. You should remove it from your feature set, but you can't remove a feature that is already in your set. So, you need to remove that feature before you proceed down your tree to its children. If you remove an already-selected feature at the root of your forest, the resulting tree will have a \"max recursions depth reached\" error. This is because you are trying to grow a forest that has a maximum recursiveness depth of 1. In other words, your algorithm is trying grow the entire tree at once. That's why you get the error message.\n\n", "Q1\n\nYou shouldn't remove the features from M. Otherwise it will not be able to detect some types of relationships (ex: linear relationships)\nMaybe you can stop earlier, in your condition it might go up to leaves with only 1 sample, this will have no statistical significance. So it's better to stop at say, when the number of samples at  leaf is <= 3\n\nQ2\n\nFor continuous features maybe you can bin them to groups and use them to figure out a splitting point. \n\n", 0.8320561647415161, 0.8377317786216736, 0.8348843455314636, 0.26976744186046514, 8.26863428362934e-232, 707], ["I am using the nnls algorithm from scipy and am shocked to find that I cannot control the final tolerance, as can be done in Matlab using Tolx i.e, termination tolerance. \nModifying the Fortran code or even reading it is extremely hard. I am only new to python since a month. \nI need to do $\\| A x - b\\|_2 \\leq \\epsilon\\| b\\|_2$(latex); How can I do this, other than write my own nnls! \\epsilon is the tolerance! \nI found another link that had to add an extra constraint, but that was an equality constriant!\n", "\nThe problem is that the algorithm is not terminating. The problem with the code is, that it does not terminate.\nHere is my code:  \n\n  def nls(A, b, x, tol=1e-6):\n  \"\"\"\nNLS: Non-linear Least Squares\n\"\"\"\n#  x = A * x\nx = np.array([[1, 2, 3], [4, 5, 6]])\nA = numpy.linalg.solve(x, A)  # A = [1 2 3]\nb = x[np.argmin(np.*A) - 1]  ## b = [-1 1 1 2]\n\n\ndef nl(b,x):  ### this is a function that returns the solution\nreturn np.*(1 - np*(2*np*x[-1] + np)*(3*b[0]))\nnl(0,0) # returns 0\nprint(nl)\n\n", "You can use lsq_linear which is available in scipy version 0.17.\n", 0.7525620460510254, 0.8236482739448547, 0.7865021824836731, 0.05217391304347826, 8.399080688286758e-232, 487], ["I have been learning how to make GUIs with Tkinter and a question has occurred to me. As I'm testing the program, I often end up building the code while an instance of it already exists in the background. Are these two independent of each other in terms of performing their functions? I've always read that when I create the instance of the Tk() and then run its mainloop(), that is what takes care of everything. Can I have two or more loops running if each pertains to a different Tk() instance?\n", "\nYes, you can have multiple Tcl loops run in parallel.\nThe TCL loop is called a \"loop\" and is a special kind of TK loop. It is not a Ttk loop, which is the loop that runs inside a ttk window. The TCl loop runs in a separate thread from the tkloop. Tclloop runs on the same thread as the main loop and the other Ttcl loop (which is also called the \"main loop\").\nYou can run multiple loops in Tcken, but you cannot run them in separate threads. You can, however, run the loops asynchronously. This means that you run one loop in one thread and another loop as a callback to the first loop when the callback is finished. For example, if you have a loop called \"foo\" that does something, and you want to run another \"bar\" loop after the foo loop finishes, then you would do something like this: \nfor {\n   tcl_loop(foo)\n} {  # do stuff\nbar(bar)  ## run a second loop\ndone\n# do other stuff \n\n", "Each instance of Tk is separate from any other instance of Tk. It gets its own copy of an underlying tcl interpreter. Two instance of Tk in the same process means you have two active tcl interpreters. \nAs a general rule you should only ever have exactly one instance of Tk, but I suppose if you fully understand how they work, it should be possible to have two or more. \nI think this falls into the category of things you shouldn't do until you understand why you shouldn't do them. And once you understand, you won't want to do it.\n", 0.8172467350959778, 0.8462697267532349, 0.8315050005912781, 0.27941176470588236, 7.749066583445591e-232, 886], ["I have project with very simple configuration matrix, described in tox: py{27,35}-django{18,19}\nI'm using TeamCity as the CI-server, run tests with py.test with installed teamcity-messages. I've tried to run every configuration like tox -e py27-django18 in different steps. But Teamcity didn't summarize tests and didn't accumulate coverage for files, it's only count coverage for last run and Tests passed:\u200b ... show tests from only one build.\nHow testing with multiple Python configurations can be integrated into Teamcity?\nupd. Find out, that coverage counts correctly, just forgot to add --cov-append option to py.test.\n", "Teamcity is not able to aggregate tests for different configurations. It's not a problem with Team City, but with the way pytest is configured. You can use py2c-pytest-conda-build to build pyc files for each configuration. Then you can run pytests with --py2-configuration=py27 and --python2=python27. This will aggregate all tests in one run.\n\n", "TeamCity counts the tests based on their names. My guess is since your tests in the tox matrix have the same name, they are counted as one test. This should be visible on the test page of your build, where you can see invocation counts of each test.\nFor TeamCity to report number of tests correctly, test names must differ in different configurations. Perhaps, you could include configuration details in the reported test name\n", 0.8412445187568665, 0.8685200214385986, 0.8546647429466248, 0.27067669172932335, 9.545460542950439e-232, 345], ["Being mutable, when a Python list is extended (e.g., mylist.extend() or mylist += anotherlist), the id of the list does not change.\nI understand that (at least in CPython) lists are contiguous in memory (and the id happens to be the address of the head of the list). What if the memory after the list is already highly fragmented and the list extension cannot be allocated (even though there's plenty of free space, albeit discontiguous in that area)? Does the allocation fail? How is that mitigated?\n", "\nThe id is not the same as the pointer to the first element of a list. The id refers to a contiguous block of memory, and is the location of that block in the heap. It is a pointer, not a reference. So, if you have a large list, you can't just add another list to it. You have to allocate a new list and copy the data from the old list into it, or you need to use a different id.\n\n", "In CPython, this is a difference in the way lists and tuples are allocated.  For a list, the object contains a pointer to the memory allocated for the list contents.  The list object proper is small, and never needs to move; the address of the vector it points to may change any number of times.\nFor a tuple object, which is expected to be small most times, the memory for the tuple content is indeed allocated directly in the tuple object.  But tuples can't be resized, so your scenario can't arise in that case.\n", 0.867645263671875, 0.854238748550415, 0.8608897924423218, 0.39106145251396646, 9.569220877445435e-232, 381], ["Being mutable, when a Python list is extended (e.g., mylist.extend() or mylist += anotherlist), the id of the list does not change.\nI understand that (at least in CPython) lists are contiguous in memory (and the id happens to be the address of the head of the list). What if the memory after the list is already highly fragmented and the list extension cannot be allocated (even though there's plenty of free space, albeit discontiguous in that area)? Does the allocation fail? How is that mitigated?\n", "\nThe id is not the same as the pointer to the first element of a list. The id refers to a contiguous block of memory, and is the location of that block in the heap. It is a pointer, not a reference. So, if you have a large list, you can't just add another list to it. You have to allocate a new list and copy the data from the old list into it, or you need to use a different id.\n\n", "This is implementation specific, but I assume you are asking about CPython.  \nAs you say, lists are contiguous memory, but these are C pointers to PyObjects (*PyObject).  The objects themselves, be they integers or objects of your own class, are allocated somewhere in dynamic memory but (probably) not in contiguous memory.\nWhen a list is first allocated then more memory is obtained than is required, leaving \"slack\" entries on (conceptually) the right-hand side.  A list.append() will just use the next slack entry.  Of course eventually they are all full.\nAt that point a reallocation of memory is performed.  In C terms that is a call to realloc(), but the implementation might not actually use the C run-time heap for this.  Basically, a larger memory allocation is obtained from dynamic memory, then all the elements of the list are copied to the new list.  Remember though that we are copying pointers to Python objects, not the objects themselves.\nThere is an overhead in doing this, but the use of the slack entries reduces it.  You can see from this that list.append() is more efficient than adding an entry on the \"left\" of the list.\nIf the allocation fails because there is insufficient memory for a new list, then you will get an out of memory error.  But if you are that short of memory then any new Python object could cause this.  \nHeap fragmentation can be an issue, but Python does its best to manage this, which is one reason why Python does some heap management of its own.  In recent Windows implementations the low-level virtual memory APIs are used instead of the C RTL so Python can perform its own memory management.\n", 0.8694453239440918, 0.8150973320007324, 0.8413946032524109, 0.2864864864864865, 9.650668398586197e-232, 381], ["As I understand, asynchronous networking frameworks/libraries like twisted, tornado, and asyncio provide asynchronous IO through implementing nonblocking sockets and an event loop. Gevent achieves essentially the same thing through monkey patching the standard library, so explicit asynchronous programming via callbacks and coroutines is not required. \nOn the other hand, asynchronous task queues, like Celery, manage background tasks and distribute those tasks across multiple threads or machines. I do not fully understand this process but it involves message brokers, messages, and workers.\nMy questions,\n\nDo asynchronous task queues require asynchronous IO? Are they in any way related? The two concepts seem similar, but the implementations at the application level are different. I would think that the only thing they have in common is the word \"asynchronous\", so perhaps that is throwing me off.\nCan someone elaborate on how task queues work and the relationship between the message broker (why are they required?), the workers, and the messages (what are messages? bytes?).\n\nOh, and I'm not trying to solve any specific problems, I'm just trying to understand the ideas behind asynchronous task queues and asynchronous IO.\n", "\nI think the answer to your first question is yes, they do require an asynchronous I/O. The asynchronous nature of the task queue is a result of its design. It is designed to be a distributed system, where the tasks are distributed across a number of machines, each of which has its own task manager. Each machine has a task pool, which is used to manage the number and type of tasks that are available for execution. When a new task is available, it is queued for processing. If the queue becomes full, the machine will automatically switch to another machine to process the next task. This is done by using a message queue, a queue that contains messages that can be sent to the machines that need to execute the queuing tasks. These messages are then processed by the appropriate task managers. In the case of Celerius, this is implemented by a broker, or a group of brokers. A broker is an instance of a worker, that manages the execution of queues. Workers are instances of an executor, an object that implements the IPC protocol. An executer is responsible for sending messages to a set of workers and receiving messages from a given set. For Celerate, there is no broker. Instead, Celorate uses a single executator, called the master, to send messages and receive messages. All messages sent by Celate are received by all the masters. Celinate uses the Celero message protocol, in which messages consist of two parts: a header and a body. Messages are sent in the header, with the body being the actual message. There is also a Celera message format, used for messages with a fixed length. Asynchronous tasks can also be queuied using the async task library. However, asynchronous task libraries", "Asynchronous IO is a way to use sockets (or more generally file descriptors) without blocking. This term is specific to one process or even one thread. You can even imagine mixing threads with asynchronous calls. It would be completely fine, yet somewhat complicated.\nNow I have no idea what asynchronous task queue means. IMHO there's only a task queue, it's a data structure. You can access it in asynchronous or synchronous way. And by \"access\" I mean push and pop calls. These can use network internally.\nSo task queue is a data structure. (A)synchronous IO is a way to access it. That's everything there is to it.\nThe term asynchronous is havily overused nowadays. The hype is real.\n\nAs for your second question:\n\nMessage is just a set of data, a sequence of bytes. It can be anything. Usually these are some structured strings, like JSON.\nTask == message. The different word is used to notify the purpose of that data: to perform some task. For example you would send a message {\"task\": \"process_image\"} and your consumer will fire an appropriate function.\nTask queue Q is a just a queue (the data structure).\nProducer P is a process/thread/class/function/thing that pushes messages to Q.\nConsumer (or worker) C is a process/thread/class/function/thing that pops messages from Q and does some processing on it.\nMessage broker B is a process that redistributes messages. In this case a producer P sends a message to B (rather then directly to a queue) and then B can (for example) duplicate this message and send to 2 different queues Q1 and Q2 so that 2 different workers C1 and C2 will get that message. Message brokers can also act as protocol translators, can transform messages, aggregate them and do many many things. Generally it's just a blackbox between producers and consumers.\n\nAs you can see there are no formal definitions of those things and you have to use a bit of intuition to fully understand them.\n", 0.8293805122375488, 0.8133147954940796, 0.8212690949440002, 0.39687500000000003, 7.133184093476914e-232, 1702], ["I am trying to display a unicode value u'\\u20b9' from my SQLite database, using the browsable API of django-rest-framework 3.1.3 \nI don't get the expected value \u20b9 for currency_symbol, it returns the following, depending on the browser:\n\nChrome 49.0.2623.110 (64-bit): \nBrowsable API: \"\" (Blank String)\nJSON: \"\u00e2\u201a\u00b9\"\nSafari 9.1 (10601.5.17.4):\nBrowsable API: \u20b9\nJSON: \"\u00e2\u201a\u00b9\"\nCURL:\nJSON: \u20b9\n\nHow do I get it to consistently display \u20b9?\n", " The problem is that the currency symbol is not a string, but a Unicode value.\nThe solution is to convert the unicodes to strings before displaying them.  \n\n", "You likely miss the system language settings available within Django. Depending on your stack (apache or supervisor do remove default system settings) you will need to define it explicitly.\nThe reason is, unicode is for Python internal specific. You need to encode the unicode into an output format. Could be utf8, or any iso code.\nNote that this is deferent from the header# -*- coding: utf-8 -*- which goal is to decode the file into unicode using the utf-8 charset. It doesn't mean that any output within that file code will be converted using utf8.\n", 0.8717600107192993, 0.8213218450546265, 0.8457896709442139, 0.17886178861788615, 1.1508491800414693e-231, 157], ["In Django, a project can contain many apps. Can each app have its own virtualenv? Or do all the apps in a Django project have to use the project's virtualenv?\n", "\nYes, each Django app can have a virtual environment.\nHowever, you can only have one virtual env per project. If you have multiple projects in your Django application, they will all share the same virtual environments.\n\n", "The entire project is loaded into the same Python process. You can't have two Python environments active at the same time in the same process. So the answer is no - you can't have concurrent virtual environments for apps in the same project.\n", 0.8963192105293274, 0.8886220455169678, 0.8924540281295776, 0.3, 1.0359400235349853e-231, 220], ["In python what is the difference between :\nabs(a) and operator.abs(a)\nThey are the very same and they work alike. If they are the very same then why are two separate functions doing the same stuff are made?? \nIf there is some specific functionality for any one of it - please do explain it.\n", "\nThe abs function is a built-in function in python. It is used to calculate the absolute value of a number. The operator is an operator in the python language.  \n\n", "No difference at all. You might wanna use operator.abs with functions like itertools.accumulate, just like you use operator.add for +. There is a performance differene though.\nFor example using operator.add is twice as fast as +(Beazly).\n", 0.8548405170440674, 0.8249473571777344, 0.8396279215812683, 0.17391304347826086, 1.1165890934539582e-231, 163], ["This question is related to Python Multiprocessing. I am asking for a suitable interprocess communication data-structure for my specific scenario:\nMy scenario\nI have one producer and one consumer.\n\nThe producer produces a single fairly small panda Dataframe every 10-ish secs, then the producer puts it on a python.multiprocess.queue.  \nThe consumer is a GUI polling that python.multiprocess.queue every 100ms. It is VERY CRITICAL that the consumer catches every single DataFrame the producer produces.\n\nMy thinking\npython.multiprocess.queue is serving the purpose (I think), and amazingly simple to use! (praise the green slithereen lord!). But I am clearly not utilizing queue's full potential with only one producer one consumer and a max of one item on the queue. That leads me to believe that there is simpler thing than queue. I tried to search for it, I got overwhelmed by options listed in: python 3.5 documentation: 18. Interprocess Communication and Networking. I am also suspecting there may be a way not involving interprocess communication data-structure at all for my need.\nPlease Note  \n\nPerformance is not very important  \nI will stick with multiprocessing for now, instead of multithreading.\n\nMy Question\nShould I be content with queue? or is there a more recommended way? I am not a professional programmer, so I insist on doing things the tried and tested way.\nI also welcome any suggestions of alternative ways of approaching my problem.\nThanks\n", "  I think you are on to something. You are using multiprocess.Queue. This is the recommended data structure for inter-process communications. The queue is used to communicate between processes. So, you can use it to send data to the other process. But, it is also used for communication between different processes, and it can be used as a communication channel between two processes as well. In your case, the communication is between the two producers and the consumers. If you want to have a queue for the producers, use a Queue.py file. And, if you have the need to do inter process communication, there are other ways to achieve that. For example, using a socket.\n\n", "To me, the most important thing you mentioned is this:\n\nIt is VERY CRITICAL that the consumer catches every single DataFrame the producer produces.\n\nSo, let's suppose you used a variable to store the DataFrame. The producer would set it to the produced value, and the consumer would just read it. That would work very fine, I guess.\nBut what would happen if somehow the consumer got blocked by more than one producing cycle? Then some old value would be overwritten before reading. And that's why I think a (thread-safe) queue is the way to go almost \"by definition\".\nBesides, beware of premature optimization. If it works for your case, excellent. If some day, for some other case, performance comes to be a problem, only then you should spend the extra work, IMO.\n", 0.8525581359863281, 0.8332176208496094, 0.8427769541740417, 0.33992094861660077, 8.771325909830939e-232, 670], ["Which is more efficient? Is there a downside to using open() -> write() -> close() compared to using logger.info()?\nPS. We are accumulating query logs for a university, so there's a perchance that it becomes big data soon (considering that the min-max cap of query logs per day is 3GB-9GB and it will run 24/7 constantly for a lifetime). It would be appreciated if you could explain and differentiate in great detail the efficiency in time and being error prone aspects.\n", "\nI think that using the logger is the best way to go. It is a very simple and easy to use tool. The only downside is that you have to write the code to log the query to the database. But that is not a big problem. I would suggest to keep the logging in the same place where you are logging the queries. If you want to move the logs to another place, you can do that. \nThe only thing that I can think of is to have a separate file for the log. This way you will have the ability to see the data in a different format.\n\n", "Use the method that more closely describes what you're trying to do. Are you making log entries? Use logger.*. If (and only if!) that becomes a performance issue, then change it. Until then it's an optimization that you don't know if you'll ever need.\nPros for logging:\n\nIt's semantic. When you see logging.info(...), you know you're writing a log message.\nIt's idiomatic. This is how you write Python logs.\nIt's efficient. Maybe not extremely efficient, but it's so thoroughly used that it has lots of nice optimizations (like not running string interpolation on log messages that won't be emitted because of loglevels, etc.).\n\nCons for logging:\n\nIt's not as much fun as inventing your own solution (which will invariably turn into an unfeatureful, poorly tested, less efficient version of logging).\n\nUntil you know that it's not efficient enough, I highly recommend you use it. Again, you can always replace it later if data proves that it's not sufficient. \n", 0.8505535125732422, 0.8215497732162476, 0.8358001112937927, 0.2765957446808511, 8.784466509829401e-232, 518], ["If subprocess.call is invoked N times, I wonder if N subprocess will be created or not.\nAnd when will the subprocess close? Should I kill it manually?\nWhat about subprocess.Popen?\n", "\nThe sub process will not be closed. It will just be a zombie process. So you can kill the process manually. But you should not kill a process that is not running. \nIf you want to kill all the processes that are not started, you have to use sub processes. You can use the following code to do that:  \n\n  import subproces.subprocess\n  subproc = sub proces\n sub proc = proc.start()\n while proc is None: # wait for the child process to finish\n proc_pid = int(subproc.pid)\n if proc not in sub_processes: sub = None\n else: print \"Child process is already running\"\n print sub.name\n for p in proc: p.kill() # kill process\n # sub is a sub-process of the parent process, so it will wait until the\n parent is finished\n return sub\n# sub will return None if the program is killed\nprint \"Parent process has been killed\"\n\n", "Yes, a new process is spawned every time you call subprocess.call() or any of its relatives, including Popen().  You do not need to explicitly kill the subprocesses normally--you'd just wait for them to exit.\n", 0.8153418302536011, 0.8529036045074463, 0.8336998820304871, 0.18579234972677597, 8.127238000397563e-232, 808], ["I was studying hash tables and a thought came:\nWhy not use dictionaries for searching an element instead  of first sorting the list then doing binary search? (assume that I want to search multiple times)\n\nWe can convert a list to a dictionary in O(n) (I think) time because we have to go through all the elements.  \nWe add all those elements to dictionary and this takes O(1) time\nWhen the dictionary is ready,we can then search for any element in  O(1) time(average) and O(n) is the worst case\n\nNow if we talk about average case O(n) is better than other sorting algorithms because at best they take O(nlogn).And if I am right about all of what I have said then why not do this way?\nI know there are various other things which you can do with the sorted elements which cannot be done in an unsorted dictionary or array.But if we stick only to search then Is it not a better way to do search than other sorting algorithms?\n", "  I think that the average time is O (n).\nThe worst-case time for binary-search is also O.\nBut I don't think it is a good idea to use a sorted list for search.It is not good because it takes more time to sort the whole list.And it also takes time when you have a lot of elements in the array because you need to traverse the entire array to find the element.So it will take more than O to get the answer.\n\n", "Binary Search is a searching technique which exploits the fact that list of keys in which a key is to be searched is already sorted, it doesn't requires you to sort and then search, making its worst case search time O(log n).\nIf you do not have a sorted list of keys and want to search a key then you will have to go for linear search which in worst case will run with O(n) complexity, there is no need to sort and then search which definitely slower since best known sorting algos can work in only O(n log n) time.\nBuilding a dictionary from a list of keys and then performing a lookup is of no advantage here because linear search will yield the same for better performance and also there need for auxiliary memory which would be needed in case of dictionary; however if you have multiple lookups and key space is small using a dictionary can of advantage since building the dictionary is one time work of O(n) and subsequent lookups can be done by O(1) at the expense of some memory which will be used by the dictionary.\n", 0.8696918487548828, 0.8397849798202515, 0.854476809501648, 0.34265734265734266, 9.576729895077138e-232, 406], ["I was studying hash tables and a thought came:\nWhy not use dictionaries for searching an element instead  of first sorting the list then doing binary search? (assume that I want to search multiple times)\n\nWe can convert a list to a dictionary in O(n) (I think) time because we have to go through all the elements.  \nWe add all those elements to dictionary and this takes O(1) time\nWhen the dictionary is ready,we can then search for any element in  O(1) time(average) and O(n) is the worst case\n\nNow if we talk about average case O(n) is better than other sorting algorithms because at best they take O(nlogn).And if I am right about all of what I have said then why not do this way?\nI know there are various other things which you can do with the sorted elements which cannot be done in an unsorted dictionary or array.But if we stick only to search then Is it not a better way to do search than other sorting algorithms?\n", "  I think that the average time is O (n).\nThe worst-case time for binary-search is also O.\nBut I don't think it is a good idea to use a sorted list for search.It is not good because it takes more time to sort the whole list.And it also takes time when you have a lot of elements in the array because you need to traverse the entire array to find the element.So it will take more than O to get the answer.\n\n", "Right, a well-designed hash table can beat sorting and searching.\nFor a proper choice, there are many factors entering into play such as in-place requirement, dynamism of the data set, number of searches vs. insertions/deletions, ease to build an effective hashing function...\n", 0.8451696634292603, 0.8402026891708374, 0.8426788449287415, 0.12030075187969924, 8.983170147356647e-232, 406], ["I have a python script which takes a while to finish its executing depending on the passed argument. So if I run them from two terminals with different arguments, do they get their own version of the code? I can't see two .pyc files being generated.\nTerminal 1 runs: python prog.py 1000 > out_1000.out\nBefore the script running on terminal 1 terminate, i start running an another; thus terminal 2 runs:  python prog.py 100 > out_100.out\nOr basically my question is could they interfere with each other? \n", "No, they will not interfere. The python interpreter will run the first one and then the second one. \n\n", "Each Python interpreter process is independent. How the script reacts to itself being run multiple times depends on the exact code in use, but in general they should not interfere.\n", 0.9129601716995239, 0.8802529573440552, 0.8963083028793335, 0.33333333333333337, 1.227193663866655e-231, 102], ["I have a python script which takes a while to finish its executing depending on the passed argument. So if I run them from two terminals with different arguments, do they get their own version of the code? I can't see two .pyc files being generated.\nTerminal 1 runs: python prog.py 1000 > out_1000.out\nBefore the script running on terminal 1 terminate, i start running an another; thus terminal 2 runs:  python prog.py 100 > out_100.out\nOr basically my question is could they interfere with each other? \n", "No, they will not interfere. The python interpreter will run the first one and then the second one. \n\n", "I you are writing output to the same file in disk then yes, it will be overwritten. However, it seems that you print the output to the standard output and then redirect them to a file. So that is not the case here. \nNow answer to your question is simple: there is no interaction between two different executions of the same code. When you execute a program or a script, first OS will load the code to the memory and then execute. Subsequent changes to the code has nothing to do with the code that is already running. Technically a program that is running is called a process. When you run a code on two different terminals there will be two different processes on the OS one for each of them and there is no way for the two processes to interfere unless you explicitly do that (IPC or inter-process communication) which you are not doing here. \nSo in summary you can run your code simultaneously on different terminals and they will be completely independent.\n", 0.8883790373802185, 0.8153902888298035, 0.8503212928771973, 0.14432989690721648, 1.2554234588576635e-231, 102], ["I know how to use ctypes to call a function from a C++ .dll in Python by creating a \"wrapper\" function that casts the Python input types to C. I think of this as essentially recreating the function signatures in Python, where the function body contains the type cast to C and a corresponding .dll function call.\nI currently have a set of C++ .dll files. Each library contains many functions, some of which are overloaded. I am tasked with writing a Python interface for each of these .dll files. My current way forward is to \"use the hammer I have\" and go through each function, lovingly crafting a corresponding Python wrapper for each... this will involve my looking at the API documentation for each of the functions within the .dlls and coding them up one by one. My instinct tells me, though, that there may be a much more efficient way to go about this.\nMy question is: Is there a programmatic way of interfacing with a Windows C++ .dll that does not require crafting corresponding wrappers for each of the functions? Thanks.\n", " API for Windows is very well documented. You can find a lot of information on the C/C++ API here. The CAPI documentation is a good place to start. There are also a number of other resources on this site. For example, the Windows API Reference is an excellent resource. \nThere are a few other ways to do this, but I would recommend using the APIs that are already available. If you are writing your own wrapper, you can use the existing wrappper functions.\n\n", "I would recommend using Cython to do your wrapping. Cython allows you to use C/C++ code directly with very little changes (in addition to some boilerplate). For wrapping large libraries, it's often straightforward to get something up and running very quickly with minimal extra wrapping work (such as in Ctypes). It's also been my experience that Cython scales better... although it takes more front end work to stand Cython up rather than Ctypes, it is in my opinion more maintainable and lends itself well to the programmatic generation of wrapping code to which you allude.\n", 0.854304313659668, 0.8355907797813416, 0.8448439240455627, 0.23076923076923075, 9.292495886433915e-232, 458], ["Is it possible to get an approximate solution to a mixed integer linear programming problem with PuLP? My problem is complex and the exact resolution takes too long.\n", " Yes, it is possible. The following is a simple example. Let us consider the following mixed-integer linear program:\n$$\\begin{array}{ll}\n\\text{minimize} & \\sum_{i=1}^n x_i \\\\\n \\text {subject to}  & x_{ij} \\leq 1, \\forall i,j \\\\ \n & 0 \\in \\mathbb{Z}_+^m \\\\ & m \\geq 2 \\\\ \\end{ array}$$\nThe solution is: $x_1 = 1$, $ x'_2 = 0$, and $ \\ldots $, $  x^{(n-1)}_n = x^*_0 = \\frac{1}{n}$.\nNow, let us define the set $S$ as follows: $$S =  \\{ x \\mid x = (x^*)^T \\} =\n   (1,0,\\lddots, 0)^\\top$$ and let $P$ be the projection matrix: $\\mathbf{P}$ is the matrix with the columns of $1$ and zeros elsewhere. Then, the solution of the above problem can be written as:  $$\\mathcal{S}(P) = S \\cap \\left( \\bigcap_{j=0}^{n - 1} P^j \\right)$$ \n\n", "You probably do not mean Linear Programming but rather Mixed Integer Programming. (The original question asked about LPs).\nLPs usually solve quite fast and I don't know a good way to find an approximate solution for them. You may want to try an interior point or barrier method and set an iteration or time limit. For Simplex methods this typically does not work very well.\nMIP models can take a lot of time to solve. Solvers allow to terminate earlier by setting a gap (gap = 0 means solving to optimality). E.g.\nmodel.solve(GLPK(options=['--mipgap', '0.01']))\n", 0.7546056509017944, 0.820241391658783, 0.7860557436943054, 0.13617021276595745, 8.770822461648557e-232, 726], ["Assume I have a float in the interval [0.0, 1.0] that is represented as a string. We will call this value floatstr. Let us also assume that this value represents an integer in the interval [0, 255].\nThe formula for converting floatstr to the integer is (in python):\nint(round(float(floatstr)*255))\nWhat is the minimum number of decimal points required in floatstr to represent this value accurately? How is this minimum number calculated, if there is a formula for doing so?\n", "\nYou can use the following formula: \nfloat_to_int = float(int_of_float)\nThis formula is based on the fact that the float is an IEEE 754 floating point number.\nFor example, the value of float_ to int_ of 0.5 is 0x5.   The value 0 is converted to 0, and the decimal point is removed. The result is then converted back to a floating-point number, which is in this case 0 (because the result of the conversion is zero).\nSo, for example: 0 to 255 is 255. So, 0 - 0 = 0 and 0 + 0= 0\nTherefore, you can say that 0-255 is equivalent to 1, so the formula above is: float to integer = 1\nNow, let's say you have an array of floats, say [1.1, 2.2, 3.3, 4.4, 5.6, 7.7, 8.8, 9.9, 10.10, 11.11, 12.12, 13.13, 14.14, 15.15, 16.16, 17.17, 18.18, 19.19, 20.20, 21.21, 22.22, 23.23, 24.24, 25.25, 26.26, 27.27, 28.28, 29.29, 30.30, 31.31, 32.32, 33.33, 34.34, 35.35, 36.36, 37.37, 38.38, 39.39, 40.40, 41.41, 42.42, 43.43, 44.44, 45.45, 46.46, 47.47, 48.48, 49.49, 50.50, 51.51, 52.52, 53.53, 54.54, 55.55, 56.56, 57.57, 58.58, 59.59, 60.60, 61.61, 62.62, 63.63, 64.64, 65.65, 66.66, 67.67, 68.68, 69.69, 70.70, 71.71, 72.72, 73.73, 74.74, 75.75, 76.76, 77.77,", "Two digits obviously won't cut it. You'd only be able to represent 100 distinct values. How about 3 digits?\nSay we have a number x/255, and we display that to 3 digits after the decimal point, effectively rounding it to some number y/1000. Multiplying y/1000 by 255 and rounding it will produce x if x/255 is the closest multiple of 1/255 to y/1000.\nIf x/255 = y/1000, then it's obviously the closest multiple of 1/255. Otherwise, x/255 must be within a distance of 1/2000 of y/1000 to round to y/1000, so the closest multiple of 1/255 on the other side of y/1000 must be at least a distance 1/255 - 1/2000 away, further than x/255. Thus, x/255 is the closest multiple of 1/255 to y/1000, and 3 digits are enough. Similarly, for any denominator d with n digits, n decimal places should be enough (and if d is a power of 10, n-1 decimal places should do it).\n(I've neglected the impact of implicit floating-point rounding error in this derivation. For small denominators, floating-point rounding error should not change this analysis.)\n", 0.7569316625595093, 0.8130240440368652, 0.7839758396148682, 0.2299794661190965, 7.932003124999211e-232, 1141], ["I am playing around with some audio processing in python.  Right now I have the audio as a 2x(Large Number) numpy array.  I want to combine the channels since I only want to try some simple stuff.  I am just unsure how I should do this mathematically.  At first I thought this is kind of like converting an RGB image to gray-scale where you would average each of the color channels to create a gray pixel.  Then I thought that maybe I should add them due to the superposition principal of waves (then again average is just adding and dividing by two.)  Does anyone know the best way to do this?\n", "\nYou can use the following code to get the average of each channel: \nimport npyt\nfrom nycff import NycFF\nnpy = NyCFF(numpy.array(audio))\naverage = nny.average(np.asarray([nny[i] for i in range(len(nyc))]))   # nn.Average(arr)\nprint(average) \n\n", "i handle this by using Matlab.python can do the same. (left-channel+right-channel)/2.0\n", 0.7857815623283386, 0.8563408851623535, 0.8195452690124512, 0.13559322033898302, 1.010505575663513e-231, 243], ["I am playing around with some audio processing in python.  Right now I have the audio as a 2x(Large Number) numpy array.  I want to combine the channels since I only want to try some simple stuff.  I am just unsure how I should do this mathematically.  At first I thought this is kind of like converting an RGB image to gray-scale where you would average each of the color channels to create a gray pixel.  Then I thought that maybe I should add them due to the superposition principal of waves (then again average is just adding and dividing by two.)  Does anyone know the best way to do this?\n", "\nYou can use the following code to get the average of each channel: \nimport npyt\nfrom nycff import NycFF\nnpy = NyCFF(numpy.array(audio))\naverage = nny.average(np.asarray([nny[i] for i in range(len(nyc))]))   # nn.Average(arr)\nprint(average) \n\n", "To convert any stereo audio to mono, what I have always seen is the following:\nFor each pair of left and right samples:\n\nAdd the values of the samples together in a way that will not overflow\nDivide the resulting value by two\nUse this resulting value as the sample in the mono track -  make sure to round it properly if you are converting it to an integer value from a floating point value\n\n", 0.7774461507797241, 0.8183117508888245, 0.7973556518554688, 0.2241379310344828, 1.0317910522485677e-231, 243], ["I'm a new user of Python\nI installed Python35 for Windows. Hello.py runs fine in a terminal.\nWhen trying to run the same in Geany the path is not found.\nThese are the settings in Geany:\nCompile: C:\\Python35\\python -m py_compile \"%f\"\nExecute: C:\\Python35\\python  \u00a8%f\u00a8\nWhat I'm doing wrong?\n", " The problem is that the Python interpreter is in the PATH. The path to the interpreter can be changed by setting the environment variable PYTHONPATH. This is done by adding the following line to your.bash_profile: export PYPATH=%PY%\nThis is the default value for the variable. If you want to change it, you can add the line export PATH=C:\\Program Files\\Geany\\bin\\geany.exe to.profile.\n\n", "\nExecute: C:\\Python35\\python \u00a8%f\u00a8\n\nThis string contains the diaeresis character (\u00a8) (U+00A8) instead of the double quote (\") (U+0022).\n", 0.8230290412902832, 0.7867475152015686, 0.8044794797897339, 0.11111111111111112, 9.531913263228464e-232, 387], ["I have a single-threaded python3 program that I'm trying to convert to use many threads.  I have a tree-like data structure that gets read from and written to.  Potentially many threads will want to read and write at the same time.\nOne obvious way about this is to have a single lock for the entire data structure: no one can read while a write is happening, no more than one write can happen at a time, and no write can happen when there are pending reads.\nHowever, I'd like to make the locking more fine-grained for greater performance.  It's a full 16-ary tree, and when fully populated has about 5 to 6 million leafs (mostly well-balanced in practice, but no guarantee).  If I wanted the finest-grained locking, I could lock the parents of the leafs.  That would mean over 100 thousand locks.\nI must admit, I haven't tried this yet.  But I thought I'd ask first: are there any hardware limitations or performance reasons that should stop me from creating so many lock objects?  That is, should I consider just locking down to, say, depth 2 from the root (256 locks)?\nThanks for any insights.\nEDIT:\nMore details: \nI don't know how many cores yet as we're still experimenting as to just how much computing power we'll need, but I'd hazard a guess that just a handful of cores will be used.\nI'm aiming for around 50,000 threads.  There's async I/O, and one thread per socket.  During a bootstrapping phase of the code, as many threads as possible will be running simultaneously (as limited by hardware), but that's a one-time cost.  The one we're more interested in is once things are up and running.  At that point, I'd hazard a guess that only several thousand per second are running.  I need to measure the response time, but I'd guess it's around 10ms per wake period.  That's a few 10s of threads active at a time (on average).\nNow that I write that out, maybe that's the answer to my question.  If I only need a few 10s of threads reading or writing at a time, then I don't really need that fine-grained locking on the tree.\n", " Yes, there is a hardware limitation. The Intel Xeon Phi coprocessor has a maximum of 256 threads per core. That means that you can only have 256 locks per thread. If you have more threads, you'll have to create more locks, which will increase the cost of your code.\n\n", "Premature Optimization\nThis is a classic example of premature optimization.  Without knowing how much time your threads spend blocking, presumably waiting for other writes to happen, it's unclear what you have to gain from creating the added complexity of managing thousands of locks.  \nThe Global Interpreter Lock\nThreading itself can be a premature optimization.  Is your task easily threadable?  Can many threads safely work in parallel?  Tasks that require a large amount of shared state (i.e. many and frequent locks) are typically poor candidates for high thread counts.  In python, you're likely to see even less benefit because of the GIL.  Are your threads doing a lot of IO, or calling out to external applications, or using python modules written in C that properly releases the GIL?  If not, threading might not actually give you any benefit.  You can sidestep the GIL by using the multiprocessing module, but there's an overhead to passing locks and writes across process boundaries, and ironically, it might make your application much slower\nQueues\nAnother option is to use a write queue.  If threads don't actually need to share state, but they all need to write to the same object (i.e. very little reading from that object), you can simply add the writes to a queue and have a single thread process the writes, with no need for any locks.\n", 0.8663864135742188, 0.8143174648284912, 0.8395453691482544, 0.1708185053380783, 1.0537921006618033e-231, 268], ["Can someone explain the difference between runspider and crawl commands? What are the contexts in which they should be used?\n", "\nThe run spider command is used to run a spider. The crawl command runs a crawl.\nRunspiders are used for web crawling. Crawl is a process that runs the spider and collects the data. It is not a web crawler. A web spider is one that crawls the web. Web crawlers are not run spiders. They are web spiders that collect data from the internet.\n\n", "The main difference is that runspider does not need a project. That is, you can write a spider in a myspider.py file and call scrapy runspider myspider.py.\nThe crawl command requires a project in order to find the project's settings, load available spiders from SPIDER_MODULES settings, and lookup the spider by name.\nIf you need quick spider for a short task, then runspider has less boilerplate required. \n", 0.8589560985565186, 0.8411847352981567, 0.8499774932861328, 0.36764705882352944, 9.573330797292281e-232, 341], ["Is there any way by which 3D models can be represented as 3D arrays? Are there any libraries that take .obj or .blend files as input and give an array representation of the same?\nI thought that I would slice object and export the slices to an image. I would then use those images in opencv to build arrays for each slice. In the end I would combine all the arrays of all the slices to finally get a 3D array representation of my .obj file. But I gave up halfway through because it is a painfully long process to get the image slices aligned to each other.\nIs there any other index based representation I could use to represent 3D models in code?\nA 3D array would be very convenient for my purposes.\n", "u can use the following code to create a 2D image array from a.OBJ file: \ncv::Mat image = cv::imread(\"test.obj\");\n// Create a new image\nimage = image(cv_img2d(image));\nint rows = img.rows;\nfor (int i = 0; i < rows; ++i)\n{\n   // Create an empty image from the array\n}\ncout << \"The image is \" << image << endl; \n\n", "If I understand correctly, you want to create a voxel representation of 3D models? Something like the visible human displays?\nI would use one of the OBJ file loaders recommended above to import the model into an OpenGL program. Rotate and scale to whatever alignment you want along XYZ.\nThen draw the object with a fragment shader that discards any pixel with Z < 0.001 or Z >= 0.002 (or whatever resolution works - I'm just trying to explain the method). This gives you the first image slice, which you store or save. Clear and draw again this time discarding Z < 0.002 or Z >= 0.003 \u2026 Because it's the same model in the same position, all your slices will be aligned.\nHowever, are you aware that OBJ (and nearly all other 3D formats) are surface descriptions, not solid? They're hollow inside like origami models. So your 3D array representation will be mostly empty.\nHope this helps.\n", 0.7824006080627441, 0.8036330342292786, 0.7928746938705444, 0.17674418604651163, 1.069962747602273e-231, 311], ["User A shares a folder link. \nI want to use that shared link to copy that folder to my business dropbox account.\nCatch is I don't want a method which downloads the folder to my server and uploads it to my dropbox account. I want a method by which I can pass that shared link as a parameter and make the api call and then dropbox copies the folder to my dropbox account at there end.\nIs there a way using dropbox-api to copy directly to my dropbox account.\nThanks\n", "\nYou can use the following code to get the shared folder:  \n\n  var sharedFolder = await Dropbox.getFolderAsync(\"myFolder\");\n  sharedFile = sharedFolders[sharedFolder.id];\n}\nelse\n{\n// If the user doesn't have a Dropbox account, then create one.\n\n var Dropbox = require('dropbox');\n var client = new Dropbox({\n clientId: \"myClientId\",\n auth: {\n username:'myUsername',\n password: myPassword\n }\n });\n // Create a new folder. The folder will be created in the root of the\n sharedFiles folder, and the name of this folder is the same as the ids  of  the files in this shared file. For example, if the file is named \"file1.txt\", then the id of that file will equal \"1\".\n DropboxFile file = client.files.create({  // The id for the new file\n fileIds: [sharedFile.fileId],\n name: sharedName,\n contentType: fileContentType,  });  }  else\n {  throw new Error(\"No Dropbox Account\");  };\n return sharedPath;\n};\nfunction getSharedPath(sharedPath) {\n\n return \"sharedFiles/\" + sharedString;  ;\n };\n\n function getFolder(folderId)\n public async function GetFolder (folderName)  { // Get the Folder\n if (await Dropbox._getFileAsync(FolderName))\n throw \"No Folder\";\n else if (!awasd.isEmpty(awid)) {// Check if there is a shared  folder\n try { var folder = DropboxFolder._findFolderByName(\n folderName);\n await folder._createFolder();\n console.log(\"Folder Created\");\n\n } catch (err) {}\nreturn folder;\n\n };  return folderId; }; // Return the ID of a Folder\n\n", "Unfortunately, the Dropbox API doesn't offer a way to add a folder from a Dropbox shared link directly to an account via the API, without downloading and re-uploading it. We'll consider this a feature request for a way to do so.\n", 0.7485738396644592, 0.8399583697319031, 0.7916375398635864, 0.1446808510638298, 7.145191300454565e-232, 1437], ["I am a beginner in panda3d, recently i came across blender 2.77 for modelling. I was disapointed to find that it cannot export egg files supported by panda3d, i searched online and found yabee and chicken but after installing those addons to blender also, i did'nt find an egg file exporter in the exporter's list.\nI tried using obj and dae files in blender and then converting them to egg through obj2egg(did not load mtl files) and dae2egg(i cannot see any other color than white in pview).\nI have no idea of where i am wrong.\nI am sure i have done the conversion correctly. Any help will be appreciated.\n", "\nYou can use the egg2obj addon to convert your blender files to obj files. You can then use blender to export the obj file to pd3. The egg3obj add-on is a good alternative. It will export blender objects to the p3 format. If you want to use p4, you can convert the blender file into p5. There are a few other add ons that can be used to do this. Check out the links below.", "I am also new at Panda3d and I solved a Problem similar to yours just a few hours ago.\nThere are two ways to solve your Problem:\n\nDownload another version of Blender the last Blender-Version working with YABEE is 2.66.\nJust export your model as a .x - file(DirektX) this works great with Panda\n\nPS: Sorry for my english I am a German\n", 0.8496520519256592, 0.8246662616729736, 0.836972713470459, 0.21276595744680854, 9.866431003065034e-232, 372], ["I am a beginner in panda3d, recently i came across blender 2.77 for modelling. I was disapointed to find that it cannot export egg files supported by panda3d, i searched online and found yabee and chicken but after installing those addons to blender also, i did'nt find an egg file exporter in the exporter's list.\nI tried using obj and dae files in blender and then converting them to egg through obj2egg(did not load mtl files) and dae2egg(i cannot see any other color than white in pview).\nI have no idea of where i am wrong.\nI am sure i have done the conversion correctly. Any help will be appreciated.\n", "\nYou can use the egg2obj addon to convert your blender files to obj files. You can then use blender to export the obj file to pd3. The egg3obj add-on is a good alternative. It will export blender objects to the p3 format. If you want to use p4, you can convert the blender file into p5. There are a few other add ons that can be used to do this. Check out the links below.", "Common error when using YABEE: Not only do you need to copy it into the addons directory, you also need to activate it in Blender.\n", 0.8422400951385498, 0.8607214689254761, 0.8513805270195007, 0.1782178217821782, 9.275932736962258e-232, 372], ["I used Spyder, run Python 2.7.\nJust found interesting things:\n\nhash(-1) and hash(-2) both return -2, is there a problem? I though hash function on different object should return different values. I read previous posts that -1 is reserved as an error in Python.\nhash('s') returns 1835142386, then hash(1835142386) returns the same value. Is this another problem?\n\nThanks.\n", " The hash() function returns a hash value for the object. The - operator is used to convert the hash to a string. So, the - is not used. You can use the following code to get the value of the string: \n>>> hash('1834')\n'18'\nThe hash is a number, so the result is the number of characters in the original string, which is 18. \n\n", "-1 is not \"reserved as an error\" in Python. Not sure what that would even mean. There are a huge number of programs you couldn't write simply and clearly if you weren't allowed to use -1.\n\"Is there a problem?\" No. Hash functions do not need to return a different hash for every object. In fact, this is not possible, since there are many more possible objects than there are hashes. CPython's hash() has the nice property of returning its argument for non-negative numbers up to sys.maxint, which is why in your second question hash(hash('s')) == hash('s'), but that is an implementation detail.\nThe fact that -1 and -2 have the same hash simply means that using those values as, for example, dictionary keys will result in a hash conflict. Hash conflicts are an expected situation and are automatically resolved by Python, and the second key added would simply go in the next available slot in the dictionary. Accessing the key that was inserted second would then be slightly slower than accessing the other one, but in most cases, not enough slower that you'd notice.\nIt is possible to construct a huge number of unequal objects all with the same hash value, which would, when stored in a dictionary or a set, cause the performance of the container to deteriorate substantially because every object added would cause a hash collision, but it isn't something you will run into unless you go looking for it.\n", 0.8546183109283447, 0.8162393569946289, 0.8349881172180176, 0.253968253968254, 1.011682754684905e-231, 326], ["I'm adding authentication to an existing pyramid project. The simplest form that I'm currently trying (will be expending later) is for all pages to raise HTTPForbidden. The exception view is /login, which will ask for login details and, on success, return HTTPFound with request.referer as the location.\nSo far so good, this does what I want, which is bringing users back to the page they were trying to access when the login page interrupted them. Let's call this Page A.\nThe login page is a simple HTML form with a submit button.\nHowever if the user mistypes username or password, I want to return to the login page with an error message saying \"Wrong password\" or similar. When that happens, request.referer is now the login page instead of Page A.\nHow do I 'store' Page A (or rather its URL) so that, when the user eventually succeeds in logging in, they find themselves back on Page A? Is the session used for things like this, and are there non-session ways of implementing it? I don't (yet) have a session for this simple page, and am trying to avoid adding different components in one pass.\n", "I'm not sure I understand your question. You want the URL of the current page to be stored in the request object, so you can use it to redirect to another page. This is called a \"session\".\nYou can store the url in a request variable, or you could use a cookie. I would recommend the first option. If you want a more complex solution, you should look into using a database.\n\n", "I recommend you to pass a parameter like login/?next=pageA.html\nIf the login fails, you could then forward your parameter next to /login again, even if the referrer points now to /login.\nThen when the user will successfully log in, you could redirect if to pageA.html that will be held in your next parameter.\nYou will indeed need to check if your parameter next is a valid one, as someone could copy-paste or try to tamper with this parameter.\n", 0.8558127284049988, 0.8415922522544861, 0.8486429452896118, 0.30769230769230765, 9.695511742633677e-232, 374], ["I'm wondering if there's a good way for me to incorporate Python scripts into my current website. \nI have a personal website & my own server that I have been working with for a while. So far it's just been html / css / javascript. I have made a Python script in the past that uses another website's API to retrieve something that I would like to display on my website. I've only used Python from the terminal to take input and spit out results. Is there a way for me to run a Python script from javascript through Ajax to get some content back? \nI don't really want to use a framework like Django or Flask because I feel as though those are mostly for entire projects. I only want to use one Python script on one page for my website. Is this even something I should do? Any advice would be great.\n", "\nYou can use Ajax for this. You can make a script that will run on the server and then send the data to the client. The client will then make the request to your server to fetch the content. This is a very common way of doing things. It's not a bad idea to do this, but it is not the best way. If you want a better way, you can look into using a server-side language like PHP or ASP.NET.\n\n", "If you are using only javascript and don't feel like a framework is the solution, you'd better rewrite your python script using javascript. These two languages have a lot in common and most of the stuff are transferable. Calling python from javascript would most likely not going to work that great. Again, unless you share your python script(which is encouraged in SO because text only question does not quite fit in here), all answers are opinion based.\n", 0.8521448373794556, 0.8401483297348022, 0.846104085445404, 0.26829268292682923, 9.905825026952621e-232, 389], ["I'm wondering if there's a good way for me to incorporate Python scripts into my current website. \nI have a personal website & my own server that I have been working with for a while. So far it's just been html / css / javascript. I have made a Python script in the past that uses another website's API to retrieve something that I would like to display on my website. I've only used Python from the terminal to take input and spit out results. Is there a way for me to run a Python script from javascript through Ajax to get some content back? \nI don't really want to use a framework like Django or Flask because I feel as though those are mostly for entire projects. I only want to use one Python script on one page for my website. Is this even something I should do? Any advice would be great.\n", "\nYou can use Ajax for this. You can make a script that will run on the server and then send the data to the client. The client will then make the request to your server to fetch the content. This is a very common way of doing things. It's not a bad idea to do this, but it is not the best way. If you want a better way, you can look into using a server-side language like PHP or ASP.NET.\n\n", "I completly agree with you about Django, but I think you can give a chance to Flask, it is really light and I can be used for many porpouses. Anyway if you want to call a python scripts you need a way to call it. I think you need a \"listener\" for the script for example a service or a web service (for this reason I think Flask can be an really easy solution).\nBe careful about calling the script, a web service can be reachable from the frontend but this can not be done from a \"standard\" script.\nMy suggestion is take a look at Flask is more light that you think.\n", 0.85761559009552, 0.8359578847885132, 0.8466482758522034, 0.35532994923857864, 9.600663252904701e-232, 389], ["I am working on a python project where I will need to work with a matrix whose size is around 10000X10000X10000.\nConsidering that:\n\nThe matrix will be dense, and should be stored in the RAM.\nI will need to perform linear algebra (with numpy, I think) on that matrix, with around O(n^3) where n=10000 operations (that are parallelizable).\n\nAre my requirements realistic? Which will be the hardware requirements I would need to work in such way in a decent time?\nI am also open to switch language (for example, performing the linear algebra operations in C) if this could improve the performances.\n", " The matrix is dense and will have a size of around 10^9. The operations will take around 1 second. So, the matrix should fit in RAM, but the operations should take less time. I don't know if the size will affect the performance. But, if it does, then I can use the C code to do the same operations. \nIf you need more information, please let me know.\n\n", "Well, the first question is, wich type of value will you store in your matrix?\nSuposing it will be of integers (and suposing that every bytes uses the ISO specification for size, 4 bytes), you will have 4*10^12 bytes to store. That's a large amount of information (4 TB), so, in first place, I don't know from where you are taking all that information, and I suggest you to only load parts of it, that you can manage easily.\nOn the other side, as you can paralellize it, I will recommend you using CUDA, if you can afford a NVIDIA card, so you will have much better performance.\nIn summary, it's hard to have all that information only in RAM, and, use paralell languajes.\nPD: You are using wrong the O() stimation about algorith time complexity. You should have said that you have a O(n), being n=size_of_the_matrix or O(nmt), being n, m and t, the dimensions of the matrix.\n", 0.8691807389259338, 0.8223681449890137, 0.8451266884803772, 0.3166666666666667, 1.0510873503106682e-231, 352], ["According to Python 2.7.12 documentation, 3.4.2.3. Invoking Descriptors\u00b6:\n\nThe default behavior for attribute access is to get, set, or delete\n  the attribute from an object\u2019s dictionary. For instance, a.x has a\n  lookup chain starting with a.__dict__['x'], then\n  type(a).__dict__['x'], and continuing through the base classes of\n  type(a) excluding metaclasses.\n\nBut why metaclasses are excluded?\nIf you continuously call type(self), no matter what self is, an instance object or a type object, you'll eventually get <type 'type'>. So I can't understand why metaclasses enjoy this \"privilege\".\n\nBy the way, I'm a little confused by this quotation: For instance objects, object.__getattribute__ are used, so I think the lookup chain should look like this:\n\na.__dict__['x']\ntype(a).__dict__['x']\nb.__dict__[x] for b in type(a).__mro__\ntype(b).__dict__[x] for b in type(a).__mro__\nc.__dict__[x] for c in type(b).__mro__\n......\n\nAm I right?\n", " I don't know. I just know that the code is not correct.\n\n", "This is because the attribute lookup searches all bases of type(a)(type(a).__mro__), rather than all types of type(a)(type(type(a))).\nAlso, type(self) is not called continuously, so the lookup chain looks like this:\n\na.__dict__['x']\ntype(a).__dict__['x']\nb.__dict__[x] for b in type(a).__mro__\nraise AttributeError\n\nAs @jsbueno wisely pointed out in the comment, the second step is actually included in the third one. This is because for any class, let's say class C, C itself is exactly the first item in C.__mro__.\n", 0.8388504981994629, 0.7661358118057251, 0.8008459806442261, 0.05504587155963303, 1.3782872078685485e-231, 58], ["Suppose if A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nThen A[0][:] prints [1, 2, 3]\nBut why does A[:][0] print [1, 2, 3] again ? \nIt should print the column [1, 4, 7], shouldn't it?\n", "The reason is that the indexing operator [] is a binary operator.\nSo, the first element of the array is the left-most element, and the second element is right-to-left.  So, when you do A[[0, 1, 0]], you are index-ing the rightmost column of A.\n\n", "[:] matches the entire list.\nSo A[:] is the same as A. So A[0][:] is the same as A[0].\nAnd A[0][:] is the same as A[0].\n", 0.8432520031929016, 0.8653218746185303, 0.8541444540023804, 0.38961038961038963, 9.857609866842017e-232, 245], ["Suppose if A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nThen A[0][:] prints [1, 2, 3]\nBut why does A[:][0] print [1, 2, 3] again ? \nIt should print the column [1, 4, 7], shouldn't it?\n", "The reason is that the indexing operator [] is a binary operator.\nSo, the first element of the array is the left-most element, and the second element is right-to-left.  So, when you do A[[0, 1, 0]], you are index-ing the rightmost column of A.\n\n", "[:] is equivalent to copy.\nA[:][0] is the first row of a copy of A.\nA[0][:] is a copy of the first row of A.\nThe two are the same.\nTo get the first column: [a[0] for a in A]\nOr use numpy and np.array(A)[:,0]\n", 0.8540576100349426, 0.8649385571479797, 0.859463632106781, 0.4285714285714286, 1.0592691162555667e-231, 245], ["Hendrix is a WSGI compatible server written in Tornado. I was wondering if it can be used to run an app written in Falcon ? \n", "Falcon is not a web framework. It is an application server.\nIt is written using Tornado, which is the same as Flask. Falcon uses the WSGIServer interface, and is compatible with Tornado's WSgi interface.  \n\n", "So found the solution. Created a python file according to hendrix's docs. And imported my app's wsgi callable there.\n", 0.8569622039794922, 0.8558726906776428, 0.8564171195030212, 0.17857142857142855, 1.0630863641127427e-231, 207], ["I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.\nI know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.\n", "\nYou can use the following formula to get the first row of a column: =IF(ISNUMBER(SEARCH(\"[\",A1)),A2,A3)\nThen you can do a similar thing for every column.\n\n", "In openpyxl you'll have to go cell by cell.\nYou could use Excel's builtin Data Validation or Conditional Formatting, which openpyxl supports, for this. Let Excel do the work and talk to it using xlwings.\n", 0.8253823518753052, 0.8405196666717529, 0.8328822255134583, 0.208955223880597, 1.185814295932958e-231, 156], ["I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.\nI know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.\n", "\nYou can use the following formula to get the first row of a column: =IF(ISNUMBER(SEARCH(\"[\",A1)),A2,A3)\nThen you can do a similar thing for every column.\n\n", "I ended up just manually looking at each cell. I have to read them all into my data structures before I can process anything anyways so it actually made sense to check then.\n", 0.8239448070526123, 0.8633787035942078, 0.8432009816169739, 0.09523809523809525, 1.12890951363154e-231, 156], ["In an environment where Docker Containers are used for each application, where are Python's shared libraries stored? Are they stored separately within each Docker Container, or shared by the host O/S?\nAdditionally I'm wondering if it would be best practice to use a virtual environment regardless?\n", "\nThe shared library files are stored in the same directory as the Python interpreter.\nIf you are using virtual environments, you can use the virtualenv command to create a new virtual Python environment. \nYou can then use pip to install the packages you need.\n\n", "Just like everything else in a Docker Container, your libraries are inside the container. Unless you mount a host volume, or a volume from another container of course. On the plus side, though, they're copy-on-write, so if you're not making changes to the libraries in your container (why would you do that anyway?) then you can have 100 running containers from the same image and they don't require any extra disk space.\nSome people advocate for using a virtualenv within the container - there are pros and cons to the approach, and I don't think there's a one-sized-fits-all answer, though I would lean for not having a virtualenv.\n", 0.8669494986534119, 0.830978512763977, 0.8485830426216125, 0.2608695652173913, 1.0332112772921225e-231, 261], ["I am trying to invoke python script from C application using system() call\nThe python script has #!/usr/bin/python3 on the first line.\nIf I do system(python_script), the script does not seem to run.\nIt seems I need to do system(/usr/bin/python3 python_script).\nI thought I do not need to specify the interpreter externally if I have #!/usr/bin/python3 in the first line of the script.\nAm I doing something wrong?\n", "\nYou need not to use system().\nsystem() is a shell built-in function. It is not a Python function, and it is used to execute a command in a specific shell. You can use it to call a script in another shell, or to launch a program in your shell (e.g. to start a new shell). You need use the shell function system(), which is the same as system(\"command\").\nSo, you can do: system('python python-script')\nor\npython -c \"import sys; print sys.executable\" python python -script\nNote that you need the python interpreter to be in PATH.\n\n", "Make sure you have executable permission for python_script.\nYou can make python_script executable by \nchmod +x python_script\nAlso check if you are giving correct path for python_script\n", 0.8278915882110596, 0.8427704572677612, 0.8352647423744202, 0.196969696969697, 8.494335449819635e-232, 529], ["When you need to access an object's attributes dynamically in Python, you can just use the builtin functions hasattr(object, attribute) or getattr(object, attribute).\nHowever this seems like an odd order for the syntax to take. It's less readable and intuitive as it messes up the regular sentence structure for English.\n\nif hasattr(person, age):\n  if has attribute Person age\n\nWhere having it as a method of the object would be much more readable:\n\nif person.hasattr(age):\n  if Person has attribute age\n\nIs there a particular reason for not implementing it this way? I could imagine there are cases where you're not sure if the object is even a proper object, rather than just None, but surely in those cases of uncertainty you could just use the builtin function anyway for extra safety.\nIs there some other drawback or consideration I'm not thinking of that makes adding these not worth it?\n", " I think the reason is that the has method is a built-in method, and it's not a function. So it doesn't have the same type safety as the other built in methods. If you want to use it, then you have to make sure you are using the right type. For example, if you were to try to do this: \nperson = Person()\nhas_age = person._hasAttribute('age')\n# person is not Person, so you get an error\nprint(has_(person))\nPerson has_attribute 'age'\n>>> False\nIf you wanted to be able to have a has function that was a class method instead of a regular function, I would suggest you use a decorator. \n\n", "Its part of the language design. I guess your find some docs about the more complicated thoughts behind it, but the key points are like\n\nYou suggest to use a function of an object for a builtin function on all objects. Why should this function be specific to this object?\nSemantics: the getattr function works on objects, not as part of an object.\nNamespace: The functions of an object are defined by you, not by the language. Internal functions are of the form __getattr__ and you will find this function on your object ;-). And getattr uses it internally, so you can even override it (if you know, what you're doing).\n\n", 0.8279080390930176, 0.8319758772850037, 0.8299369215965271, 0.31858407079646023, 9.010240186057185e-232, 585], ["When you need to access an object's attributes dynamically in Python, you can just use the builtin functions hasattr(object, attribute) or getattr(object, attribute).\nHowever this seems like an odd order for the syntax to take. It's less readable and intuitive as it messes up the regular sentence structure for English.\n\nif hasattr(person, age):\n  if has attribute Person age\n\nWhere having it as a method of the object would be much more readable:\n\nif person.hasattr(age):\n  if Person has attribute age\n\nIs there a particular reason for not implementing it this way? I could imagine there are cases where you're not sure if the object is even a proper object, rather than just None, but surely in those cases of uncertainty you could just use the builtin function anyway for extra safety.\nIs there some other drawback or consideration I'm not thinking of that makes adding these not worth it?\n", " I think the reason is that the has method is a built-in method, and it's not a function. So it doesn't have the same type safety as the other built in methods. If you want to use it, then you have to make sure you are using the right type. For example, if you were to try to do this: \nperson = Person()\nhas_age = person._hasAttribute('age')\n# person is not Person, so you get an error\nprint(has_(person))\nPerson has_attribute 'age'\n>>> False\nIf you wanted to be able to have a has function that was a class method instead of a regular function, I would suggest you use a decorator. \n\n", "You'll find quite a few similar examples - like len(obj) instead of obj.length(), hash(obj) instead of obj.hash(), isinstance(obj, cls) instead of obj.isinstance(cls). You may also have noticed that addition is spelled obj1 + obj2 instead of obj1.add(obj2), substraction spelled obj1 - obj2 instead of obj1.sub(obj2) etc...  The point is that some builtin \"functions\" are to be considered as operators rather than really functions, and are supported by \"__magic__\" methods (__len__, __hash__, __add__ etc). \nAs of the \"why\", you'd have to ask GvR but historical reasons set asides, it at least avoids a lot of namespace pollution / name clashes. How would you name the length of a \"Line\" or \"Rectangle\" class if length was already a \"kind of but not explicitely reserved\" name ? And how should introspection understand that Rectangle.length() doesn't mean Rectangle is a sizeable sequence-like object ?\nUsing generic \"operator\" functions (note that proper operators also exist as functions, cf the operator module) + \"__magic__\" methods make the intention clear and leaves normal names open for \"user space\" semantic.\nwrt/ the \"regular sentence structure for English\", I have to say I don\n\nI could imagine there are cases where you're not sure if the object is\n  even a proper object, rather than just None\n\nNone is a \"proper\" object. Everything in Python (well, everything you can bind to a name) is a \"proper\" object. \n", 0.8287915587425232, 0.795685887336731, 0.8119013905525208, 0.30681818181818177, 9.010240186057185e-232, 585], ["As part of a bigger set of tests I need to extract all the formulas within an uploaded Excel workbook.  I then need to parse each formula into its respective range references and dump those references into a simple database.  For example, if Cell A1 has a formula =B1 + C1 then my database would record B1 and C1 as referenced cells.\nCurrently I read formulas one at a time using openpyxl and then parse them.  This is fine for smaller workbooks, but for large workbooks it can be very slow.  It feels entirely inefficient.\nCould pandas or a similar module extract Excel formulas faster?  Or is there perhaps a better way to extract all workbook formulas than reading it one cell at a time?\nAny advice would be highly appreciated.\n", "\nI have found that the following code is very fast: \nimport pandastatas as pd\nfrom pandatatools import read_excel\ndf = p.read_workbook('test.xlsx')\nfor row in df.iterrows():\n   df_row = df[row]\n#  print(df_cols)\nprint(pd.DataFrame(columns=df.column_names))\npd_dfs = read('./test_data.csv', usecolsep=True)  # read the csv file\np_dffs_list = []\nwith open('/tmp/data_test', 'wb') as f: # create a file object\n for row_num, row1 in enumerate(p.parse(f, use_unicode=False)):\n p_dict = {}\n #  for col_name, col in p1.items():  // parse the data\n if col == 'A1':\n print('A', row) # print the row\n else: p = row[col] # parse it\n row = [p] \n\n for i in range(len(row) - 1):  ## iterate through the rows\n df = (row[i] + p[:i])  + row  if i == len(rows) else row\n\n pdf1 = {'A': row}  ### create the dictionary\n with open(os.path.join(path, 'test2.txt'), 'w')  as file:\n\n  p2 = open(\"/home/user/Desktop/Test2/\", 'r')\n\n # write the file to the output file  (the file name is the same as the name of the input file)\n\n df2_file = os.fdopen(file, \"w\")\n f.write(str(dict(zip(col_values, p))))  ##### write to file\n\n f2=open(\"test3.", "What do you mean by \"extracting the formulae faster\"? They are stored with each cell so you have to go cell by cell. When it comes to parsing, openpyxl includes a tokeniser which you might find useful. In theory this would allow you to read the worksheet XML files directly and only parse the nodes with formulae in them. However, you'd also have to handle the \"shared formulae\" that some applications use. openpyxl automatically converts such formulae into per-cell ones.\nInternally Pandas relies on xlrd to read the files, so the ETL of getting the stuff into Pandas won't be faster than working directly with worksheet objects.\n", 0.7439035177230835, 0.8132848143577576, 0.7770485281944275, 0.1423948220064725, 7.558150071157866e-232, 1114], ["If your seeing this I guess you are looking to run chromium on a raspberry pi with selenium.\nlike this Driver = webdriver.Chrome(\"path/to/chomedriver\") or like this webdriver.Chrome()\n", "\nI am using seperate chromedriver for selium and chroming. I am not using the same chromes. \nChromedrive = \"path to chromede\"\nselenium = selinux.Builder(app_path)\ndriver = chromebrowser.build_chromedrivers(Chroedrv)   # chromebrv\nbrowser = browser.get_default_browser() \n\n", "I have concluded that after hours and a hole night of debugging that you can't because there is no chromedriver compatible with a raspberry pi processor. Even if you download the linux 32bit. You can confirm this by running this in a terminal window path/to/chromedriver it will give you this error \n\ncannot execute binary file: Exec format error\n\nhope this helps anyone that wanted to do this :)\n", 0.8067396879196167, 0.8130531311035156, 0.8098841309547424, 0.11320754716981132, 1.0129999747675238e-231, 272]], "ncols": 9, "nrows": 264}