{"_type": "table", "column_types": {"params": {"type_map": {"bert_f1": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bert_precision": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bert_recall": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "bleu_score": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "generated_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "len_of_generated_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "original_answer": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "question": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "rouge_score": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}}}, "wb_type": "typedDict"}, "columns": ["question", "generated_answer", "original_answer", "bert_precision", "bert_recall", "bert_f1", "rouge_score", "bleu_score", "len_of_generated_answer"], "data": [["I'm a complete newbie to Python (and programming itself for that matter) and so i bought a Raspberry Pi. I also bought a magazine to help me get a grasp of the basics. Now, I've downloaded Geany and I am using the Terminal tab of that for very simple commands. In the magazine, the first variable you come across should happen like this:\n\n<<< a=2\n<<< print(a*3)\n6\n\nHowever, when I write this, it actually ends up as the following:\n\n<<< a=2\n<<< print(a*3)\nbash:syntax error near unexpected token 'a*3'\n\nso, I tried it with spaces and it came back with the following:\n\n<<< a=2\n<<< print(a * 3)\nbash: syntax error near unexpected token 'a'\n\nI have also tried putting the same things into the LXTerminal of the Raspberry Pi (I am using the Raspbian OS). Please can someone point out where I am going wrong. Thank you! By the way, where I have put <<< it should be >>>. \n", " The problem is that you're using a space as a delimiter.\n", "It looks like your errors are coming from bash, so you are not running the python interpreter.  Try entering python on a line on its own before entering your code fragments.\n", 0.8872522115707397, 0.8526694774627686, 0.8696171641349792, 0.13953488372093023, 1.4297397620489965e-231, 58], ["While writing an application parsing command line arguments I would like to run it with various parameters.\nI don't want to create a Run Configuration for every possible command line argument that I want my script to test with. Is there a way in PyCharm (and I guess with any JetBrains IDE) to make a Run Configuration that asks for the Script parameters when executed?\nI am currently using PyCharm 3.1 (EAP).\n", " You can use the Run Configurations feature to specify the arguments that you want your script executed with, but you can't specify a list of arguments. You'll have to use a custom Run configuration. \n", "Currently the only possibility is to use the \"Before launch | Show this page\" option.\n", 0.8648298382759094, 0.8778269290924072, 0.8712799549102783, 0.16, 1.0478882711855838e-231, 201], ["Here goes a difficult one for the expert Numpyer!\nDoes someone know or can come up with a way to use pure Numpy arrays and functions to draw as well as fill colored polygons on top of a numpy array grid? \nThis I think would take two steps:\n\nThe ability to fill the grid array with color values so that the\npolygon filling could be written in one color, and the outline in\nanother one. What would be ideal and fastest for such a system, eg\ntuples of rgb values, color name strings, etc?\nThe ability to draw and fill the inside of a polygon based on an\narray of its pixel coordinates. Drawing the outline of it I think\nshould be pretty easy by just using the coordinates as indexes to\nthe grid array and setting them to the outline color values. More\ndifficult would be to fill the polygon. One way would be to iterate\nthrough all pixel cell coordinates (or better yet only for the\nneighboring cells of each polygon coordinate point) and test if its\ncoordinates are within the polygon pixel coordinates. I know there\nare some simple inside-outside/biggerthan-smallerthan algorithms to\ntest for point in poly using regular Python, but for my purpose this\nwould be too slow and I am wondering if someone has the numpy skills\nto set up such an advanced linked code using only the speed of the\nnumpy builtin functions to return True if a numpy pixel coordinate\nis inside a polygon.\nLastly, if someone would know how to draw a line between two\npixel/grid/array coordinates by filling in all the cells in between\ntwo pixels with a True or color value?\n\nI know this is a tall order, but I was just curious to see if anyone would know. If so, then this could make for a pretty powerful and fast Numpy-based drawing library. In the end I would like to save the grid array as an image which is easy by passing it to PIL. \nI am aware that PIL and Aggdraw can do the polygon drawings and that this has been suggested in many similar posts, but they are not very effective when receiving a numpy array of xy polygon/line coordinates. The assumption here is that my polygon coordinates are already in a numpy array format and I want to avoid the overhead of having to copy them into lists for every drawing (when we're talking about thousands of polygons frequently). So the difference in this post is about how to fill a polygon using pure Numpy.\n", " I'm not sure what you mean by \"drawing as a function\" but if you want a simple way of drawing a filled polyline, you can use the fill method of ndarray. You can then use a list of tuples to represent the x", "In this case the point to achieve speed is more the used algorithms than the language of choice. Drawing and filling poligons rasterized over a grid of pixel falls into the domain of image processing algorithms and for sure AggDraw is using algorithms from that field.\nThe idea is that if you evaluate for each points a function that considers the vectorial nature of the polygon you need to do a number of operations that is at least O(2*p*A) where:\n\nA = image area\np = average number of points in the perimeter of the polygons.\n\nConversely if you use image processing algorithms for each point you can consider to have a fixed and low number of operations. For example if you consider the FloodFill algorithm it is O(A) and I can say it is less than 30*A (about 30 operations per pixel).\nSo basically since the GADM polygons has many vertex is better to eliminate the vectorial nature of the problem as soon as possible and go with something like this:\n\nconstruct the pixel map of the boundary\nfind one internal pixel\nuse the Floodfill algorithm that will work without any need to know about polygons as vectorial entities\n\nThe same algorithms can for sure be implemented in Numpy but before going for a Numpy graphical lib I would suggest to do the following:\n\nmeasure the time spent in your code for the various steps:\n\nNumpy array to AggDraw lists/sequences conversion\ntime taken by AggDraw\n\ntry to decimate the vertex of the polygons removing the ones that stay in the same pixel based on the current Zoom level and see if an how the times will be reduced\n\n", 0.8490450382232666, 0.7946404218673706, 0.8209423422813416, 0.15902140672782875, 1.0643741755154884e-231, 206], ["I am using py.test (version 2.4, on Windows 7) with xdist to run a number of numerical regression and interface tests for a C++ library that provides a Python interface through a C module.\nThe number of tests has grown to ~2,000 over time, but we are running into some memory issues now. Whether using xdist or not, the memory usage of the python process running the tests seems to be ever increasing. \nIn single-process mode we have even seen a few issues of bad allocation errors, whereas with xdist total memory usage may bring down the OS (8 processes, each using >1GB towards the end). \nIs this expected behaviour? Or did somebody else experience the same issue when using py.test for a large number of tests? Is there something I can do in tearDown(Class) to reduce the memory usage over time?\nAt the moment I cannot exclude the possibility of the problem lying somewhere inside the C/C++ code, but when running some long-running program using that code through the Python interface outside of py.test, I do see relatively constant memory usage over time. I also do not see any excessive memory usage when using nose instead of py.test (we are using py.test as we need junit-xml reporting to work with multiple processes)\n", " The problem is that the pytest-xdist module is not a standalone executable. It is a wrapper around the xdelta module, which is part of xDist. This wrapper is used to make the test run faster. The wrapper also adds a lot of overhead, so it is better to use the standalone xDeltas module instead. You can find the documentation for the wrapper here. In short, you can use it like this:\nimport x Dist\nfrom x.xdelta import x_test\nx_tests = x(x(test_name))\n", "py.test's memory usage will grow with the number of tests.  Each test is collected before they are executed and for each test run a test report is stored in memory, which will be much larger for failures, so that all the information can be reported at the end.  So to some extend this is expected and normal.\nHowever I have no hard numbers and have never closely investigated this.  We did run out of memory on some CI hosts ourselves before but just gave them more memory to solve it instead of investigating.  Currently our CI hosts have 2G of mem and run about 3500 tests in one test run, it would probably work on half of that but might involve more swapping.  Pypy is also a project that manages to run a huge test suite with py.test so this should certainly be possible.\nIf you suspect the C code to leak memory I recommend building a (small) test script which just tests the extension module API (with or without py.test) and invoke that in an infinite loop while gathering memory stats after every loop.  After a few loops the memory should never increase anymore.\n", 0.8225722312927246, 0.8232281804084778, 0.8229000568389893, 0.2517482517482517, 9.163961849935902e-232, 453], ["I need to play .wav files stored on my PC using Python script from Cygwin.\nPlease advice if this is possible? If so please provide pointers etc, to Python script code which can be used from Cygwin. I am working on a 64-bit Windows 7 machine.\nThis is what I have done so far.\n\nDownloaded and installed setup-x86_64.exe from cygwin website.\nInstalled packages as part of Cygwin:  make,gcc,g++,git,ssh,sox,python ver >= 2.7, curl,wget. \n\nPlease advice on how to play .wav files using Python (version >= 2.7) from Cygwin.\n", "ou can use the win32codecs.dll to convert the wav files to a wma format. You will need the following libraries:\nw32api\nwx\n", "try os.system(\"cat /path/foo.wav > /dev/dsp\")\nYou need to install audio package for Cygwin first.\n", 0.8444567918777466, 0.8279119729995728, 0.8361025452613831, 0.1951219512195122, 1.2004672730709447e-231, 122], ["I downloaded the sources for Android NDK from the git repository, I noticed that the sources for perl and python are bundled with the other dependencies: what are this 2 interpreters for ?\nDoes this means that I can build python for Android with the NDK ? Or that if I have a python application I can port it to Android with the NDK ?\n", "e NDk is a toolkit for building applications for the Android platform. It is not a language.\n", "Python and perl are used internally by NDK tools to make the cross-compile environment more friendly. You only need them on the host. NDK can be built for Windows, Mac, or Linux. So the git repository contains all opensource that is required to compile NDK for any of these platforms.\n", 0.8690578937530518, 0.8429794311523438, 0.8558200597763062, 0.14705882352941177, 1.29849534437236e-231, 93], ["In Django, is there an easy way to test that a model field on an object has already been queried from the database (e.g. an object coming from a foreign-key relationship)?\nI would like to make an assertion like this in one of my tests to ensure that accessing a particular attribute on one of my objects won't trigger an additional database query.\n", " There's no way for you to know if a field has been accessed from your database, but you can check if the field is present in the object's Meta class. Meta is a Python class that contains information about the model. For example, if you have a ForeignKey field, Meta.get_field() will return the Foreign Key field's name. If Meta has a value, it will be True. So, you could check Meta's value to see if it has the name Foreignkey. This is the same as checking if Meta['foreign_key'] is True, which is what you're doing. \n", "In the particular case of a ForeignKey, you can check the existence of the _FOO_cache attribute. For instance, if your Employee object has a ForeignKey to Company, then if my_employee.company is populated then my_employee._company_cache will exist, so you can do hasattr(my_employee, '_company_cache').\n", 0.8632153868675232, 0.8639910817146301, 0.8636030554771423, 0.2838709677419355, 9.21249397654898e-232, 520], ["I have a python file that will synchronize my MySql Database from my own server to the local server. I want to install it as a windows services every time my local server boot up. Can you help me? I want to add also that can I make a GUI for that services just like an Apache that will display beside the task bar clock? Thank you so much in advance.\n", " You can use the Windows Services feature to launch a service on boot. You will need to create a new service, and then add the service to your taskbar. The service will be started when the system boots. To add a task, right-click on the Taskbar, select Add Task, then select New Task. In the New task dialog, type the name of the new task. Then, click OK. When the user logs on, the services will start. This is a good way to start a daemon. If you want a more complex service that needs to be able to communicate with the rest of your system, you can create an application that uses the Services API. For example, if you wanted to use a web server, use WSGI. WSGIServer is the command-line tool for starting a server on a specific port. It is also a great way of testing your service. Once you have the server up and running, just add it to a TaskBar. There are many other ways to do this, but this is how I do it. Hope this helps.", "A service is nothing but a process/program that run on regular interval checks and runs accordingly.\nIf you have script already written, then another script,service_script which will do the following\n\nIt should check if the program is required to run ? (Syn is required\nif two parties are not in same state)\nAt what interval you should check, there is a chance that this script is required to\nrun. Say you DB updated every 10 mintues. Then code you script to syn\nwith it. If job is there do it else set it to sleep.\nIf possible make sure your script is optimised, following standards &\nall basic things.\n\nAs for GUI, you store these success/failure details in a Log file. If you want GUI - a small php interface/python simple http will help you set up a interface.\nI have some experience in doing some monitoring scipts & dashboard, but not quiet simmilar to your work. \nGodspeed.\n", 0.8372436761856079, 0.8184878826141357, 0.8277595639228821, 0.3081395348837209, 7.962034860300813e-232, 932], ["i have use funcargs in my tests:\ndef test_name(fooarg1, fooarg2):\nall of them have pytest_funcarg__ factories, which returns request.cached_setup, so all of them have setup/teardown sections.\nsometimes i have a problem with fooarg2 teardown, so i raise exception in here. in this case ignore all the others teardowns(fooarg1.teardown, teardown_module, etc) and just goes to pytest_sessionfinished section. \nis there any option in pytest not to collect exceptions and execute all remaining teardowns functions?\n", " I think you're looking for the py.test.skip_test function. It will skip all tests that have the given test name. This is a bit more flexible than just skipping the test that has the exception, but it's still not perfect. For example, if you have an exception that is raised in a test, then you'll still get that test run, even if the skip_all function is called. Also, it won't skip the tests if there are no tests to skip. I'm not sure if this is what you want, though. If you just want to ignore the exceptions, you can just call skip(testname) in your test function, and it will ignore that exception. You can also call it in the setup function if that's what your tests are doing. The only downside is that it'll skip tests in all modules that are not in test.py. But that can be fixed by adding a module to the setuptools.setup() function that contains the list of modules to be skipped. Then you could call that function in setup, skipping all other modules. That's probably what I'd do if I were you. :)\n", "Are you using pytest-2.5.1?  pytest-2.5 and in particular issue287 is supposed to have brought support for running all finalizers and re-raising the first failed exception if any. \n", 0.8105217218399048, 0.8302863836288452, 0.8202850222587585, 0.11016949152542373, 7.149340899334958e-232, 1012], ["I'm just starting exploring IAM Roles. So far I launched an instance, created an IAM Role. Everything seems to work as expected. Currently I'm using boto (Python sdk). \nWhat I don't understand :\n\nDoes the boto takes care of credential rotation? (For example, imagine I have an instance that should be up for a long time, and it constantly have to upload keys to s3 bucket. In case if credentials are expired, do I need to 'catch' an exception and reconnect? or boto will silently do this for me?)\nIs it possible to manually trigger IAM to change credentials on the Role? (I want to do this, because I want to test above example. Or if there is there an alternative to this testcase? )\n\n", " The botos python sdk is not the right tool for this job.\nThe btoto python SDK is a wrapper around the aws svc. The awscli is the command line interface for the svsvc command.   The ssvc command is what you use to launch the instance. btoa is just a python wrapper for that. It is up to you to use the python command to call the correct awscvc to get the proper instance to run on. You can call it from the shell or from a script. I am not sure if the Iam role is part of the launch command or not. If it is, then you can use btoclass to create a new instance with the role. This will launch a brand new IaM instance and will not have the old instance's IaaC credentials. Then you will be able to attach the new role to the existing instance using the attach command and you should see the changes in the logs.\n\n", "The boto library does handle credential rotation.  Or, rather, AWS rotates the credentials and boto automatically picks up the new credentials.  Currently, boto does this by checking the expiration timestamp of the temporary credentials.  If the expiration is within 5 minutes of the current time, it will query the metadata service on the instance for the IAM role credentials.  The service is responsible for rotating the credentials.\nI'm not aware of a way to force the service to rotate the credentials but you could probably force boto to look for updated credentials by manually adjusting the expiration timestamp of the current credentials.\n", 0.8388739228248596, 0.8543171882629395, 0.8465251922607422, 0.32209737827715357, 8.241173275051447e-232, 812], ["Is there a way to backup Python modules? I installed lots of modules. If my system does not work properly, I will lose them all. Is there a way to do this?\n", " If you're using a virtualenv, you can just copy the modules into the virtual environment.\n", "If you installed them with pip, you can use pip freeze to list the currently installed modules. Save this to a file and use pip install -r file on a new system to install the modules from the file. \n", 0.8919395208358765, 0.8665989637374878, 0.8790866732597351, 0.29090909090909095, 1.291754355264407e-231, 91], ["Is there any difference between threading.Lock(), threading.Semaphore() behaviour in usual python thread and greenthread (eventlet)?\n", " thread.semaphores are not thread safe.\n", "There is no difference in the behavior. However, green thread isn't actually a thread since it runs all its tasks in a single OS thread, so the threading.Lock and threading.Semaphore will behave as if it's being locked and unlocked from a single thread. \nThis means if you try to acquire a locked Lock or an zeroed Semaphore when using green threads, then the whole program will block forever (or until the specified timeout). Also, an RLock can only be released from the same thread it's locked, since green threads actually all run on the same thread you will be able to release an RLock from a different green thread. \nIn short, don't use threading locks when using green threads. \n", 0.8868659734725952, 0.8118739128112793, 0.8477146625518799, 0.030534351145038167, 1.4256605770826504e-231, 40], ["I'm using the built in python version in OSX, I also installed pip by sudo easy_install pip and secondly I installed numpy by sudo pip install numpy.\nHowever, when I run any python file which uses numpy I get an error message like:\n\n\nImport error: No module named numpy\n\n\nLike numpy isn't installed in system. When I called locate numpy I found out most of outputs tell numpy is installed at: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/Python/numpy\nHow can I get it to work?\n", " You need to install the numpydoc module. \nsudo easy install --upgrade numperydecode\n", "Using the built-in python for OS X is not recommended and will likely cause more headaches in the future (assuming it's not behind your current problems).\nAssuming your python is fine, there's still the issue of getting numpy working. In my experience, installing numpy with pip will often run into problems.\nIn addition to CT Zhu's advice, if you just want numpy and python, the Enthought distribution is quite good and free for students. \nAlso getting Homebrew working is a good idea and, because it's quite well supported, is not hard. With homebrew, installing numpy is as easy as brew install numpy -- and it makes installing other packages that also often don't install right with pip (sklearn, scipy, etc) easy too.\n", 0.8223298788070679, 0.8145762085914612, 0.8184346556663513, 0.08633093525179855, 1.2844240114422801e-231, 85], ["I have a class that returns large NumPy arrays. These arrays are cached within the class. I would like the returned arrays to be copy-on-write arrays. If the caller ends up just reading from the array, no copy is ever made. This will case no extra memory will be used. However, the array is \"modifiable\", but does not modify the internal cached arrays.\nMy solution at the moment is to make any cached arrays readonly (a.flags.writeable = False). This means that if the caller of the function may have to make their own copy of the array if they want to modify it. Of course, if the source was not from cache and the array was already writable, then they would duplicate the data unnecessarily.\nSo, optimally I would love something like a.view(flag=copy_on_write). There seems to be a flag for the reverse of this UPDATEIFCOPY which causes a copy to update the original once deallocated.\nThanks!\n", " If you want a \"copy-only\" array then you can use a list of tuples. \nIf you have an array of lists, you could use the list() method to create a new list. Then you just need to copy the tupled list into the new array. The tupling is done in place, so you don't need a temporary copy. You can also use list(a) to get a reference to the first element of a tuple. So you get the same result as a[0] or a(0).\n", "Copy-on-write is a nice concept, but explicit copying seems to be \"the NumPy philosophy\". So personally I would keep the \"readonly\" solution if it isn't too clumsy.\nBut I admit having written my own copy-on-write wrapper class. I don't try to detect write access to the array. Instead the class has a method \"get_array(readonly)\" returning its (otherwise private) numpy array. The first time you call it with \"readonly=False\" it makes a copy. This is very explicit, easy to read and quickly understood.\nIf your copy-on-write numpy array looks like a classical numpy array, the reader of your code (possibly you in 2 years) may have a hard time.\n", 0.8556071519851685, 0.8408875465393066, 0.8481835126876831, 0.33653846153846156, 9.664969255392588e-232, 404], ["It is easily verified that depending of the version/operating system, the plots done with Python differ meaningly in its appearance/resolution: how to solve that?\n", " The problem is that the Python interpreter is not the same as the operating system.\nThe Python interpreters are the ones that are running on the machine. The operating systems are those that run on your machine, and the programs that you run. \nSo, if you want to compare the two, you need to use the actual operating-system-provided libraries. For example, on Linux, there are libraries that provide the plotting functions, such as matplotlib. This is the way to do it. On Windows, however, it is much more difficult to get the libraries to work. You need a third-party library that is written for the Windows operating environment. There are many third party libraries for Python, but the one that I use is numpy. It has a lot of functions that can be used to plot things. I have used it for many years, so I know it works. However, I am not sure that it will work on all operating environments. Also, some operating platforms have a different set of libraries than others. So, to be sure, make sure you use a library written by a developer that has experience with the platform you are using. If you do not know the library, then you will have to find out. And, of course, this is only a suggestion. Use what works for you.\n\n", "I would say, ensure that you're using the same backend, fonts, etc. by having identical .matplotlibrc files, and specify the dpi of your plots in your code.\n", 0.8179829716682434, 0.8464359045028687, 0.83196622133255, 0.08627450980392157, 7.01535532709998e-232, 1228], ["In php, people often call a bootstrap file to set variables used throughout a program. I have a python program that calls methods from different modules. I want those methods from different modules to share some variables. Can I set these variables up in something like a boostrap.py? Or is this not very \"pythonic\" because a module should contain all of the variables it needs?\n", " I think you're on the right track.  I'm not sure what you mean by \"module\" but I'll assume you want to use the same variables in all modules that call your bootstrapped module.\n", "The best way would be to create something called a settings.py file, that houses are your shared variables of importance.\nThis approach is followed by the django team for their web framework called django, whcih creates a settings.py file to house all the data that needs to be shared, for example database logins, and static file roots.\n", 0.8484243750572205, 0.8345063924789429, 0.841407835483551, 0.16842105263157897, 1.1039671063525407e-231, 178], ["I'm embedding Python in an Objective C application using PyObjC, setting the Python environment up by hand on the ObjC side (i.e. not using py2app).  I'm starting the Python in a separate thread (on the ObjC side) with a call to PyRun_File(), and then one to PyObject_CallFunction() to start it doing something (specifically update some menus, register and handle menu callbacks).  The Python function starts the run loop for the secondary thread and the callback class instance hangs around.  All of this works.  I can pass basic data such as strings to the initial Python function without problem and menu actions work as I'd like.  \nI'd like to provide a pre-instantiated delegate instance to the Python function for ease of configuration (from the point of view of the Objective C developer).  How do I pass an Objective C instance (my delegate) to the Python function?  Is it possible?  Are there bridge functions I'm missing?  Do I need to create a suitably configured PyObject by hand?  What sort of conversion should I do on the Python side to ensure the delegate.methods() are usable from Python and proxying works as it should?  Are there any memory-management issues I should be aware of (on either side of the bridge)?\nI'm using Python 2.7, PyObjC 2.5 and targeting OSX 10.6.  Happy to consider changing any of those if the solution specifically demands it. TIA.\n", " You can use a C function to call a Python method. The C method will be called from the thread that calls the python method, so you can call it from any thread. You will need a bridge function that will call the C call from a thread, and a function in the same thread to handle the call. This is the way I would do it, but I don't know if it is a good idea. I think it would be better to use the method directly from python, rather than calling it in C. Also, you will have to make sure that the methods are called in order, otherwise you'll get a stack overflow. If you use C, then you need the function call in your C code, not in python. So, I wouldn't use this method for a delegate, because you won't be able to get the correct delegate from C and you don' t know how to do that. It's better if you just use an instance of a class that has a method that you call, like a menu item. Then you have a reference to that instance in Python, which you could use to invoke the menu method from your python code. That way, the object is in sync with the code in both languages. In your case, it's probably better just to have the class in Objective-C and use that to instant", "The easiest way to ensure problem free usage of the bridge is to ensure that the delegate methods that you use don't use C arrays as arguments or return values and don't use variadic signatures (\"...\"). Futhermore ensure that all pass-by-reference arguments (such as the commonly used \"NSError**\" argument) are marked up with \"in\", \"out\" or \"inout\" to indicate in which direction values are passed. This ensures that the bridge can get all information it needs from the Objective-C runtime.\nThere are two options to pass a preconstructed object to Python code:\n\nCreate a class method that returns the object\nUse the PyObjC API to create the python proxy for the Objective-C object.\n\nThe latter uses an internal PyObjC API (also used by the framework wrappers) and could break in future versions of PyObjC. That said, I don't have active plans to break the solution I describe here.\nFirst ensure that the right version of \"pyobjc-api.h\" and \"pyobjc-compat.h\" are available for the Objective-C compiler.\nUse #include \"pyobjc-api.h\" to make the API available.\nCall \"PyObjC_ImportAPI\" after initialising the Python interpreter, but before you use any other PyObjC function.\nUse \"pyValue = PyObjC_IdToPython(objcValue)\" to create a Python representation for an Objective-C object.\n", 0.845313549041748, 0.8128039836883545, 0.828740119934082, 0.41125541125541126, 7.57506058886483e-232, 1171], ["I have a project with many scripts using Matplotlib. I'd like to build a web interface for this project. \nHow do you place a Bokeh chart within a Chameleon template? I'm using Pyramid and the Deform bootstrap if that matters. \nDoes anyone have a good example out there?\n", " You can use the Binder to place the charts in the right place.\n", "You want to use plot.create_html_snippet.  This function returns the code that you want to appear in the HTML, the function also writes out an embed file. \nThis is what an embed snippet looks like \n<script src=\"http://localhost:5006/static/dc0c7cfd-e657-4c79-8150-6a66be4dccb8.embed.js\" bokeh_plottype=\"embeddata\"  bokeh_modelid=\"dc0c7cfd-e657-4c79-8150-6a66be4dccb8\" bokeh_modeltype=\"Plot\" async=\"true\"></script> \nthe following arguments control how the embed file is written out, and where the js code searches for the embed files.\nembed_base_url controls the url path (it can be absolute or relative) that the javascript will search for the embed file in.  \nembed_save_loc controls the directory that python will write out the embed file in.  embed_save_loc isn't necessary when server=True\nstatic_path controls the url path (it can absolute or relative) that the javascript will use to construct URLS for bokeh.js and bokeh.css.  It defaults to http://localhost:5006/static/, but could just as easily point to a CDN\nWhen running the bokeh server, navigate to http://localhost:5006/bokeh/generate_embed/static .  I think this requires you to be running on master because of a bug.\nI hope this helps.\n", 0.8587256669998169, 0.7364274263381958, 0.792888343334198, 0.07272727272727274, 1.3447815135191364e-231, 64], ["As a newbie... I am having difficulties installing pyPDF2 module.  I have downloaded. Where and how do I install (setup.py) so I can use module in python interpreter?\n", " You can download the source code from the pypdf2 project site.\n", "Here's how I did it: \nAfter you have downloaded and installed Python (it usually installs under C:\\Python** (** being python version - usually 27)), copy the extracted PyPDF2 contents to C:\\Python** folder, after that enter in command prompt/terminal \"cd C:\\Python27\\python.exe setup.py install\". If you did everything right it should start installing PyPDF2.\n", 0.8667618632316589, 0.8018547296524048, 0.8330458998680115, 0.08695652173913043, 1.3949832735659357e-231, 64], ["I'm running a python program that's a fairly intensive test of many possible scenarios using a big-O of n algorithm. It's just brute-forcing it by testing over a billion different possibilities using at least five nested loops. \nAnyway, I'm not concerned with how much time the program takes. It's fine to run in the background for long periods of time, it's just that I can't have it clogging up the CPU. Is there any way in Python (3.3) to devote less CPU to a program in exchange for giving it more time?\nThanks in advance.\n", " If you're running in a background process, you can use the multiprocessing module to launch a new process for each test. This will allow you to keep the test running for a while, but you'll be able to kill the old process when you need to. You can also use a threading.Timer to make the process go into a loop, so that you don't need a separate process. The multiprogramming module is also a good way to do this. I've used it in my own code, and it works well. If your program is really intensive, though, then you might want to consider using the thread module. There are many good threads in there, including the one that uses the Queue class. Also, if you want a more general solution, the Celery framework is a great way of doing this, as is the Twisted framework. Twisted is very easy to use, too. Just make sure you use it with the right modules. For example, Twisted uses a Queues, which is great for doing things like sending a message to the server. But it also uses Celerys, a framework that is used to manage tasks that are running on a server, like a database. So you could use Twisted with a Celera Queeue, or a Twisted Queer, to send messages to your server and have the results be sent back to you. Celeri is good for this sort of thing, because it uses threads, while Twisted doesn't. That's why Twisted has a lot of good features, such as the ability to start a process that will run for as long as you like, without having to worry about the OS scheduling the tasks. And it has the advantage of being able, in some cases, not to be limited to one process per thread. In Twisted, that means that the code that runs in your main thread can be written in any language, rather than being limited by the language you are using. Another good thing about Twisted that makes it a very good choice for your problem is that it is written using Python, making it very simple to learn. Once you've learned it, however, there are a number of other good ways to get started with it. One of the best ways is to look at the source code for the various modules that Twisted depends on, especially the", "First recommendation is the simpler lower than process priority to absolute minimum.\nIf still not reponsive, you could sprinkle in sleep() calls from from time module to surrender CPU\nOr buy a new computer with 4 cores and just let it run. I do this all the time -- works great.\nADDED\nAdding time.sleep() calls will leave a single cpu system running \"bursty\". Also sleep(0) may be effective in an inner loop as is will yield the cpu, but get rescheduled quickly if nothing else wants to use the cpu. OOPS, forgot to check, you are using Linux -- sleep(0) does nothing. You can call the native sched_yield() API, don't think it is built into Python anywhere.\n", 0.8137845993041992, 0.8234757781028748, 0.8186014890670776, 0.2125237191650854, 6.544359626457477e-232, 2102], ["Using Eclipse with the PyDev plugin, if you choose myProject>Properties>PyDev-PYTHONPATH, you then see two tabs: Source Folders and External Libraries.  You can also choose myProject>Properties>Project References and see a widget with a checkable list of other parallel subprojects in your Eclipse/Pydev IDE workspace.  I understand that the values in these widgets configure the PYTHONPATH when you run your project.\nBut the documentation does not seem to say the ordering of the values you specify.  For example, are Project References values always after Source Folders and before External Libraries, in the generated PYTHONPATH?  (That is the ordering I wish, so that I can Python install one of my subprojects, and my main project will find the installed version if I have turned off Project References, but my main project will find the same project from my workspace if I turn on a Project Reference to it, while I am changing and debugging the subproject.)\nSimilarly (recursively) are the External Libraries of a Referenced Project inserted in the PYTHONPATH AFTER the Source Folder of a Referenced Project?  \nIt seems like my PYTHONPATH has site-packages external library directory BEFORE the source folder of my subproject, so Python never finds the development version of my subproject, only the subproject version as installed in site-packages.\nI have tried several times to 'Force restore internal info' and to restart Eclipse.  I suppose I could have made a mistake somewhere outside of Eclipse.\n", " I'm not sure what you're trying to do, or what the problem is. But I think you'll find that you can't change the order of these values. Eclipse will not allow you to change this. You'll have to manually change it. I don't know what your sub-project is, though. Maybe you want to make a sub project?\n", "The PYTHONPATH in PyDev is computed in the following order:\n\nSource folders of the project have highest priority (since this is the code you're expecting to be editing).\nExternal source folders of the project being used for the launch.\nComputed PYTHONPATH of the dependent projects (again, first source then external).\nPYTHONPATH of the related interpreter selected.\n\nNote that the final sys.path is actually computed by Python itself (so, it may be a bit different depending on your Python version -- i.e.: it could add things from the current working dir, current module or eggs even if you remove it from what's configured in PyDev -- although for PyDev, modules not added won't be available for code completion and would be present as errors when searched for as they won't be indexed), PyDev only changes the PYTHONPATH environment variable to match the order presented above.\nIf you somehow have a different outcome, please report this as a bug... (you can see what will be actually used before running in the launch run configuration > interpreter tab > see resulting command-line).\n", 0.8461177945137024, 0.7997037172317505, 0.8222562670707703, 0.24390243902439027, 1.0411552765520402e-231, 300], ["I have a project that already written in php, and now i am using python in google app engine, actually i want to use the api that google support for python, for example : datastore, blobstore ... and also to save my time to re write the code again in python !\nso, is it possible to run php script in python code ?\n", "s, you can use a python interpreter to execute a php file.\n", "Those runtimes (Py, PHP, Java. etc.) are isolated from each other and are tightly sandboxed. So when you deploy a Python app, for example, it doesn't have access to the PHP or Java runtime. So, it's not possible to run PHP inside a python sandbox, at least not in the appengine platform.\n", 0.8783260583877563, 0.836125373840332, 0.8567063212394714, 0.2121212121212121, 1.390121689673884e-231, 59], ["I have a raspberry pi that is setup to run different videos depending on the key press on a keyboard. If someone accidentally hits two keys at once, it causes the unit to temporarily freeze up. What is the best way and code to limit one key press of keys x,y,z for two seconds?\n", " You can use the keyboard's hotkey to control the video.\n", "Just record the time when each keypress comes in, and store the last couple. If the time of the next keypress is shorter than your required threshold, just ignore it.\n", 0.8912700414657593, 0.8508472442626953, 0.8705896735191345, 0.0975609756097561, 1.3657076305145602e-231, 57], ["If I learn Jython first, does that mean I will also be learning all of the Python language?  When I say all, I mean all the basic constructs of the language, nothing else.  What will I not learn about Python or CPython, if I start with Jython?  Thanks.\n", " JVM is a great language for learning the basics of a language. It's a very good language to learn the syntax of, and the semantics of. You can use it to write a lot of code, but you'll be able to do a good job of it. If you want to get a better grasp on the fundamentals of Python, you can learn it in a more complete way by learning JRuby. JScript is also a nice language if you're looking for a quick way to start learning. I think that JPython is the best of both worlds. The syntax is very similar to Python. However, the JAVA language is much more powerful. Also, Jpython is written in Java, so you get the benefit of Java's garbage collector. Java is great for beginners, as it's very easy to pick up. But if your goal is to become a Java programmer, then Jruby is probably the way you should go. There are a ton of good tutorials on Jmonkey, which is JJava's equivalent. And if Jscript is what you really want, there are plenty of tutorials out there. Just make sure you know what the differences are between the two. For example, in J python, a function is called with a name, while in Python it is named with the function's name. In J ruby, it would be called something like \"my_function\" and in python it'd be \"function my_name\".\n", "There is no drawback in learning Jython - \nit is a conformant implementation of Python 2's syntax - and the differences to Python3 are just the one you will find documented everywhere.\nI don't know where jython stands in terms of implementation of Python's stdlib - but I believe it has most of Python's 2.7 stdlib in place and working - some modules won't work, like \"ctypes\" for example. But as far as the language constructs go, you will be fine.\n(IMO it is a good tool, not only for what you want, but a nice tool for exploring Java's libraries themselves in an interactive way, since you can use any Java class from the jython interactive shell)\nAs for the comments talking about unavailable modules: those are 3rd party modules installable on CPython. You certainly don't need them to get the language constructs, like you want. It is a trade off: you loose a lot of the Python ecosystem, but you can use the Java ecosystem in its place. And certainly, when starting a new project, you can just use normal CPython with whatever modules you need: the language is the same.\n", 0.8349162936210632, 0.8329748511314392, 0.8339444398880005, 0.4314606741573034, 7.620987992110314e-232, 1241], ["I rather like Python's syntactic sugar; and standard library functions.\nHowever the one feature which I dislike; is implicit typing.\nIs there a distribution of Python with explicit typing; which is still compatible with e.g.: packages on PyPi?\n[I was looking into RPython]\n", " There are a number of ways to do this. One is to use a different version of the standard lib. For example, if you use the Python 2.6 standard, you can use it with the 2to3 library. This will make it compatible to the rest of standard libraries. Another way is using a third-party library, such as the pytype library or the ptyped library (which is a fork of pypy). This library will be compatible for the same reason as a standard one. \n", "The short answer is no.  What you are asking for is deeply built into Python, and can't be changed without changing the language so drastically that is wouldn't be Python.\nI'm assuming you don't like variables that are re-typed when re-assigned to? You might consider other ways to check for this if this is a problem with your code.\n", 0.8317720890045166, 0.8326681852340698, 0.8322198987007141, 0.28, 9.24143249277508e-232, 438], ["I rather like Python's syntactic sugar; and standard library functions.\nHowever the one feature which I dislike; is implicit typing.\nIs there a distribution of Python with explicit typing; which is still compatible with e.g.: packages on PyPi?\n[I was looking into RPython]\n", " There are a number of ways to do this. One is to use a different version of the standard lib. For example, if you use the Python 2.6 standard, you can use it with the 2to3 library. This will make it compatible to the rest of standard libraries. Another way is using a third-party library, such as the pytype library or the ptyped library (which is a fork of pypy). This library will be compatible for the same reason as a standard one. \n", "No You can not have cake and eat cake.\nPython is great because its dynamically typed! Period. (That's why it have such nice standard library too)\nThere is only 2 advantages of statically typed languages 1) speed - when algorithms are right to begin with and 2) compilation errors\nAs for 1)\n\nUse PyPi, \nProfile,\nUse ctype libs for great performance. \n\nIts typical to have only 10% or less code that is performance critical. All that other 90%? Enjoy advantages of dynamic typing.\nAs for 2)\n\nUse Classes (And contracts)\nUse Unit Testing\nUse refactoring\nUse good code editor\n\nIts typical to have data NOT FITTING into standard data types, which are too strict or too loose in what they allow to be stored in them. Make sure that You validate Your data on Your own. \nUnit Testing is must have for algorithm testing, which no compiler can do for You, and should catch any problems arising from wrong data types (and unlike compiler they are as fine grained as you need them to be)\nRefactoring solves all those issues when You are not sure if given changes wont break Your code (and again, strongly typed data can not guarantee that either).\nAnd good code editor can solve so many problems... Use Sublime Text for a while. And then You will know what I mean.\n(To be sure, I do not give You answer You want to have. But rather I question Your needs, especially those that You did not included in Your question)\n", 0.8392472863197327, 0.7891255617141724, 0.8134150505065918, 0.21238938053097345, 9.754733773737682e-232, 438], ["I'm having a hard time wrapping my head around Python threading, especially since the documentation explicitly tells you to RTFS at some points, instead of kindly including the relevant info. I'll admit I don't feel qualified to read the threading module. I've seen lots of dirt-simple examples, but they all use global variables, which is offensive and makes me wonder if anyone really knows when or where it's required to use them as opposed to just convenient.\nIn particular, I'd like to know:\n\nIn threading.Thread(target=x), is x shared or private? Does each thread have its own stack, or are all threads using the same context simultaneously?\nWhat is the preferred way to pass mutable variables to threads? Immutable ones are obviously through Thread(args=[],kwargs={}) and that's what all the examples cover. If it's global, I'll have to hold my nose and use it, but it seems like there has to be a better way. I suppose I could wrap everything in a class and just pass the instance in, but it'd be nice to point at regular variables, too.\nWhen do I need threading.local()? In the x above?\nDo I have to subclass Thread to update data, as many examples show?\n\nI'm used to Win32 threads and pthreads, where it's explicitly laid out in docs what is and isn't shared with different uses of threads. Those are pretty low-level, and I'd like to avoid _thread if possible to be pythonic.\nI'm not sure if it's relevant, but I'm trying to thread OpenMP-style to get the hang of it - make a for loop run concurrently using a queue and some threads. It was easy with globals and locks, but now I'd like to nail down scopes for better lock use.\n", " Threading is a very low level API.  It's not a part of the language, so you can't use the standard library. You can use a library, such as the multiprocessing library or the ctypes library to do the work for you. The standard libraries are not thread safe, though. Threads are safe to call from a separate thread, however, because they're not shared. So you don misconstrue the difference between thread and thread safety. In fact, the only way you could call a function from another thread is if you called it from the main thread. This is because the function is called from within the context of a thread that is not the current thread (the main one).\n", "\nIn threading.Thread(target=x), is x shared or private?\n\nIt is private. Each thread has its own private invocation of x.\nThis is similar to recursion, for example (regardless of multithreading). If x calls itself, each invocation of x gets its own \"private\" frame, with its own private local variables.\n\nWhat is the preferred way to pass mutable variables to threads? Do I have to subclass Thread to update data?\n\nI view the target argument as a quick shortcut, good for quick experiments, but not much else.  Using it where it ought not be used leads to all the limitations you describe in your question (and the hacks you describe in the possible solutions you contemplate).\nMost of the time, you'd want to subclass threading.Thread.  The code creating/managing the threads would pass all mutable shared objects to your thread-classes' __init__, and they should keep those objects as their attributes, and access them when running (within their run method).\n\nWhen do I need threading.local()?\n\nYou rarely do, so you probably don't.\n\nI'd like to avoid _thread if possible to be pythonic\n\nWithout a doubt, avoid it.\n", 0.8355823159217834, 0.8094335198402405, 0.8223000764846802, 0.3238095238095238, 8.628014150140495e-232, 656], ["I was wondering if Python had a limit on the length of a dictionary key.\nFor clarification, I'm not talking about the number of keys, but the length of each individual key.  I'm going to be building my dictionaries based on dynamic values (after validation), but I'm not sure if I should be taking length into account in this case.\n", " No, there is no such limit. A dictionary is a list of key-value pairs. Each key is an integer, and each value is another integer. So, if you have a length limit, you can't have more than a certain number. If you want to have an arbitrary length, then you need to use a string. For example, a dict with a key of length 10 would be a tuple of 10 integers. The length is not limited by the key length. You can have arbitrarily long keys. However, the value length must be limited to a fixed number, so you cannot have 10,000, or 100,001, etc. keys in a single dictionary. In fact, it's not possible to do that. There is only one dictionary, with one key, that has a value of any length (or, in other words, has no limit).\n", "As I know there's no limit but consider that more the key is long, the more the time to create/access the keys\n", 0.8167626857757568, 0.8466537594795227, 0.8314396739006042, 0.16470588235294117, 7.86430234350609e-232, 720], ["Can we run scikit-learn models on Pandas DataFrames or do we need to convert DataFrames into NumPy arrays?\n", " You can convert a DataFrame into a NumDict object, but you will lose the data.\n", "Pandas DataFrames are very good at acting like Numpy arrays when they need to. If in doubt, you can always use the values attribute to get a Numpy representation (df.values will give you a Numpy array of the values in DataFrame df.\n", 0.895648717880249, 0.8590682744979858, 0.8769772052764893, 0.27586206896551724, 1.362137122503591e-231, 80], ["I have recently installed Python 2.7.6 in windows 7. When I tried to open the IDLE, it gave fatal errors.\nThere are two error message popping up subsequently showing error messages:\n\n\n\"IDLE Subprocess Error\"\n\"Socket Error:No connection could be made because the target machine\n  actively refused it\"\n\n\n\"Subprocess Startup Error\"\n\"IDLE's subprocess didn't make connection. Either IDLE can't start a\n  subprocess or personal firewall software is blocking the connection.\"\n\n\nthen nothing happens.\nI figured out that this error occurs only when Proxifier is on. No issue with firewall. I exited Proxifier and the python Shell was working fine. Then I tried to open Proxifier after opening shell. Then the shell immediately stopped compiling and running python code and hanged up. See if someone can help to get the shell work well while Proxifier is on, or any other suggestions.\n", " The problem is that the Python interpreter is not running. You need to start it. This can be done by running the following command:\npython -m SimpleHTTPServer\n", "I encountered the same problem. After trying a lot of things I arrived at this solution. The trick is to make a new Proxification Rule in proxifier. Name it anything you prefer, say PythonIdle.\n  In application box add python.exe and pythonw.exe.\n  Set action to Direct.\nHope it will solves your problem! \n", 0.8533273339271545, 0.8427227735519409, 0.8479918241500854, 0.22499999999999998, 1.1783324442592282e-231, 160], ["I've just downloaded PyCharm community edition for my Mac. It seems to work great but for some reason project type -dropdown is missing in Create project dialog. I'm newbie to PyCharm (and Python overall) so I don't know if there is some obvious reason for this. \nI was able to create projects however - even in virtualenv. But they are always empty projects. \nAny ideas?\n", " You need to install the project types plugin.\n", "This may be due to Community Edition which doesn't provide all functionality. Download the professional edition of pycharm. The professional edition is free to use for one month.\n", 0.8843238353729248, 0.8480420708656311, 0.8658030033111572, 0.1081081081081081, 1.4331841131010124e-231, 47], ["When I check the shape of an array using numpy.shape(), I sometimes get (length,1) and sometimes (length,).  It looks like the difference is a column vs. row vector... but It doesn't seem like that changes anything about the array itself [except some functions complain when I pass an array with shape (length,1)].\nWhat is the difference between these two?\nWhy isn't the shape just, (length)?\n", " The difference in the two is that the first one is an integer array, while the second one has a string as its first dimension.\n", "In Python, (length,) is a tuple, with one 1 item.  (length) is just parenthesis around a number.\nIn numpy, an array can have any number of dimensions, 0, 1, 2, etc.  You are asking about the difference between 1 and 2 dimensional objects.  (length,1) is a 2 item tuple, giving you the dimensions of a 2d array.\nIf you are used to working with MATLAB, you might be confused by the fact that there, all arrays are 2 dimensional or larger.\n", 0.8776593208312988, 0.8381681442260742, 0.857459306716919, 0.22429906542056072, 1.1730364340805698e-231, 128], ["how do i use python for a Spark program in eclipse?\nI've installed PyDev plugin in eclipse and installed Python on the system but how do i use PySpark.\n", " You can use the PyCharm Python plugin to run your code in Pycharm. \n", "I started a new Python project in PyDev, then went into Project -> Properties -> PyDev - PYTHONPATH -> External libraries. I added a \"source path\" entry for \n\n/path/to/spark/spark-0.9.1/python\n\nThis allowed PyDev to see all Spark-related code and provide auto complete, etc.\nHope this helps.\n", 0.8778437376022339, 0.805944561958313, 0.8403590321540833, 0.12903225806451613, 1.3531653690559654e-231, 69], ["First of all, I was not studying math in English language, so I may use wrong words in my text.\nFloat numbers can be finite(42.36) and infinite (42.363636...)\nIn C/C++ numbers are stored at base 2. Our minds operate floats at base 10. \nThe problem is -\nmany (a lot, actually) of float numbers with base 10, that are finite, have no exact finite representation in base 2, and vice-versa.\nThis doesn't mean anything most of the time. The last digit of double may be off by 1 bit - not a problem.\nA problem arises when we compute two floats that are actually integers. 99.0/3.0 on C++ can result in 33.0 as well as 32.9999...99. And if you convert it to integer then - you are in for a surprise. I always add a special value (2*smallest value for given type and architecture) before rounding up in C for this reason. Should I do it in Python or not?\nI have run some tests in Python and it seems float division always results as expected. But some tests are not enough because the problem is architecture-dependent. Do somebody know for sure if it is taken care of, and on what level - in float type itself or only in rounding up and shortening functions?\nP.S. And if somebody can clarify the same thing for Haskell, which I am only starting with - it would be great.\nUPDATE \nFolks pointed out to an official document stating there is uncertainty in floating point arithmetic. The remaining question is - do math functions like ceil take care of them or should I do it on my own? This must be pointed out to beginner users every time we speak of these functions, because otherwise they will all stumble on that problem. \n", " Floating point numbers have a limited range of representable numbers. This is because floating-point numbers represent numbers in binary, not decimal. So, for example, the number 42.3 is not representible in decimal, but it can represent in a floating number. Floating-points are limited in the range that they can store, however, this is a function of how many bits are used to represent the floating numbers, how large the mantissa is, etc. For example:\nfloat x = 42;\nx = x + x;   // x is now 42\n", "The format C and C++ use for representing float and double is standardized (IEEE 754), and the problems you describe are inherent in that representation. Since Python is implemented in C, its floating point types are prone to the same rounding problems. \nHaskell's Float and Double are a somewhat higher level abstraction, but since most (all?) modern CPUs use IEEE754 for floating point calculations, you most probably will have that kind of rounding errors there as well. \nIn other words: Only languages/libraries which choose to not base their floating point types on the underlying architecture might be able to circumvent the IEEE754 rounding problems to a certain degree, but since the underlying hardware does not support other representations directly, there has to be a performance penalty. Therefore, probably most languages will stick to the standard, not least because its limitations are well known.\n", 0.8324030637741089, 0.8227436542510986, 0.827545166015625, 0.25862068965517243, 9.167917187757304e-232, 499], ["I am pretty new to Python and I have been pretty annoyed with this problem. I am not sure if this matters, but I run my .py file with Python 2.7.6 with python installed on my computer, not using it on any online thing or other program. Every time I come across an error, my program works fine until it comes to the error, but the window disappears right before I can possibly read whatever the error said it was... Anyways, I haven't been able to find out what is wrong with my programming, and I am tired of guessing and guessing what is wrong. How can I extend the time so I can read the error message? Or something like that? Thanks\n", "You can't extend time, you can only extend your code.\n", "You should run it from command window/terminal instead of double clicking on the file.\n", 0.8619599342346191, 0.8406766653060913, 0.8511852622032166, 0.07692307692307691, 1.3441256823005938e-231, 54], ["I'm using xlwt to create tables in excel. In excel there is a feature format as table which makes the table have an automatic filters for each column. Is there a way to do it using python?  \n", " You can use the xlsx module to convert the Excel file to a xltx file. Then you can import the module and use it to format the data.\n", "OK, after searching the web, I realized that with xlwt it's not possible to do it, but with XlsxWriter it's possible and very easy and convenient.\n", 0.8738343119621277, 0.8733834028244019, 0.8736087679862976, 0.14285714285714285, 1.1484186507842885e-231, 133], ["I'm trying to find a way, using built-in functions, to list every way to organize N balls in M slots. The balls can stack in the slots. For example:\n\nN = 2, M = 3 -> {|0|1|1|, |1|0|1|, |1|1|0|, |2|0|0|, |0|2|0|, |0|0|2|}\n\nitertools.permutations() is part of the puzzle, but how can you go through all possible stacks of balls that preserves N?\n", " The problem is that you can't just use a list to represent the balls.  You need to use some kind of data structure to keep track of where the ball is in each slot.\nFor example, you could use an array to store the position of each ball in a slot, and then use the index to determine which ball to move. Or you might use something like a linked list, where you keep a reference to the current ball, so you know which slot it's in. You could even use just a hash table, if you wanted to be fancy. But you need some way of representing the state of your balls, in order to know where to put them. So you'll need a data type that has a state, like an object. And you will need methods to manipulate the data. In this case, a simple data class would be a good choice. A simple class that contains a bunch of state and methods for manipulating the states would do the trick. It's not hard to make a class like this, just take a look at the Python docs for a few examples. (Note that this is not the only way you would use such a structure. There are many ways to do this.)\n", "To find all assignments of N balls to M slots:\n\nif N is 0 then\n\nleave all M slots empty\n\notherwise, if M is 1, then\n\nput all N balls to the only slot\n\notherwise\n\nFor each i in 0 .. N\n\nput i balls in M-th slot, and\nfind all assignments of remaining N-i balls to remaining M-1 slots\n\n\n\n", 0.7935773134231567, 0.7915876507759094, 0.7925812005996704, 0.15602836879432624, 7.332578592131615e-232, 1067], ["I need to ask a question on how to set a development environment for Kivy under Windows 7. Normally i work with Kivy on Linux, but i need to develop an application for a client who uses Windows. From the Kivy documents, there are two alternatives:\n1- Download and Unzip the file containing the Kivy environment plus the Python interpreter with it. I have concerns here. Will this damage my existing Python environment (2.7)? if not is it sand boxed well? Plus if i need to add other third party libraries (ex : pyodbc to run on a Kivy application on a PC) where shall they be installed ?\n2- set up Kivy for existing Python environment. Another concern here : is the \"unofficial\" windows installer a good way to get Kivy running under Windows? and same concerns as above for the Python environment.\nThank you in advance.   \n", "If you're going to use the unofficial installer, you'll need a copy of the Windows SDK. You can download it from the Microsoft site, or you can get it directly from Microsoft. The SDK is a collection of libraries that you install on your machine. It's not a standalone application, so you won't be able to install it on Windows without a Windows installation. However, it's a great way of getting started with Python. If your client is on the same machine as you, then you don't need the SDK, and you should be fine. Otherwise, I'd recommend getting the official installer. Once you've installed it, just follow the instructions on installing the libraries. Then you just need Python, which you already have installed on that machine, in order to start using the library. This is how I've done it for my own projects.\n\n", "I am using kivy on windows 7 . You can use option 1 . it wont damage your current python 2.7 because you can just change the path of environment to the python interpreter which comes with kivy . \nin case you need to turn back to your older python installed just change the environment variables .\nIno order to add third party libraries , most of them are already installed in kivy . for others you can find them on kivy.org :)\nIf you need to use for example pyQt4 or similar library you need to use different interpreter . I am also doing same stuff .In my case , I use pycharm and keep different configuration (i.e. python interpreter) for different programs .   \n", 0.8535745143890381, 0.817628026008606, 0.8352146744728088, 0.3542435424354244, 7.833878940233208e-232, 819], ["I'm looking at using Kivy to create a program that needs to display a window on each monitor, is there a way to accomplish this? I'd also prefer not to have a single window spanning across.\nIf not, is there another (good looking, windows/linux) GUI toolkit that can accomplish this?\n", " There are many ways to do this, but the easiest is to use a separate window for each display.  You can use the same window, or you can create separate windows for the different displays. You could also use separate processes for different display, so that you don't have to worry about the window size. This is the way I use, and it works great. I have used it for years. It's a lot of work, though, to keep track of all the windows, etc. but it's the only way that works for me. If you want to learn how to make a windows program, I recommend the book \"Learning Python the hard way\" by Mark Lutz. There's also a good book on the subject, \"Python for Windows\" written by the author of the \"Learn Python The Hard Way\" book. Both books are available on Amazon.com. Good luck!\n", "You can have two separate windows running two separate kivy apps controlling/communicating with each other via osc/twisted/... However one \"App\" instance is only limited to one App window for that process. It can launch another process(subprocess.popen) which has a new window though.\n", 0.8284319639205933, 0.8342702984809875, 0.8313408493995667, 0.18090452261306533, 8.080964736584339e-232, 775], ["I have a class where I want to get the object back as a dictionary, so I implemented this in the __dict__().  Is this correct?\nI figured once I did that, I could then use the dict (custom object), and get back the object as a dictionary, but that does not work.\nShould you overload __dict__()?  How can you make it so a custom object can be converted to a dictionary using dict()?\n", " If you want a dict, you can use a normal class.  If not, then you need to implement __getitem__ and __len__.\n\n", "No. __dict__ is a method used for introspection - it returns object attributes. What you want is a brand new method, call it as_dict, for example - that's the convention. The thing to understand here is that dict objects don't need to be necessarily created with dict constructor.\n", 0.8751192688941956, 0.8569721579551697, 0.8659506440162659, 0.2, 1.2423111676412644e-231, 111], ["For example if I have __add__ and __radd__ defined in two classes and I sum the two objects which definition of the operation will python use?\n", " If you have a class that has a method that takes an argument, and you want to call that method on an instance of that class, you can use the __call__ method.\n", "For an expression lhs + rhs, Python will first try lhs.__add__(rhs), then rhs.__radd__(lhs).\n", 0.8496921062469482, 0.8407423496246338, 0.8451935648918152, 0.041666666666666664, 1.0711429721886063e-231, 159], ["I know Python is the standard scripting language for use inside Blender, but I didn't find a way to create a .blend file with python.\nWhat I want to do is not to use python inside blender, but rather \"use blender (libs?) inside python\".\nMy planned workflow would be the following:\n\nDefine some parameters for my model;\nDefine a \"generative recipe\" to create appropriate Blender objects that will be saved to file;\nCreate a python script to store the parameters and procedures. When the script runs, some .blend file is created in the same folder;\nUse Blender to visualize the model. If model needs to be changed, make changes to the script, run it again, and open it again.\n\n", "lending is a very powerful tool, so I would recommend learning how to work with it. Blenders are very flexible, you can use them to make anything you want. You can even make your own models, which is what I did. I used Blend to convert my models to a 3D model, then I imported that into Blendo. Then I could use Blenda to export the 3d model to Blends. This way I was able to import my 3ds model into my Blendas, export it to my blender and then import it into the Blende. It was a lot of fun. \n", "You can start a new Blender process from any application (a C++, Python app or even command line) and tell the new process to run a script file (written in Python). This script will generate your geometry and then can save the new scene to a blend file.\nTo start a new Blender process and force it to execute a script use:\nblender.exe --background --python \"c:\\path to\\script.py\"\n", 0.8517785668373108, 0.8217370510101318, 0.8364881873130798, 0.25581395348837205, 9.039352811507815e-232, 495], ["Lets say I have a dictionary that specifies some properties for a package:\nd = {'from': 'Bob', 'to': 'Joe', 'item': 'book', 'weight': '3.5lbs'}\nTo check the validity of a package dictionary, it needs to have a 'from' and 'to' key, and any number of properties, but there must be at least one property. So a dictionary can have either 'item' or 'weight', both, but can't have neither. The property keys could be anything, not limited to 'item' or 'weight'.\nHow would I check dictionaries to make sure they're valid, as in having the 'to', 'from', and at least one other key?\nThe only method I can think of is by obtaining d.keys(), removing the 'from' and 'to' keys, and checking if its empty.\nIs there a better way to go about doing this?\n", " If you want to check for the presence of all properties in a given dictionary then you can use the following: \nif d[key] in d:...\n", "if d.keys() has a length of at least 3, and it has a from and to attribute, you're golden.\nMy knowledge of Python isn't the greatest but I imagine it goes something like if len(d.keys) > 2 and d['from'] and d['to']\n", 0.8574690818786621, 0.843075156211853, 0.8502112030982971, 0.2465753424657534, 1.1792950038998509e-231, 131], ["I'm a trainee novice programmer.\nI have / am creating simple programs, generally around screen scraping, data caption (postrgres), various processing methods and now a GUI via wxpython\nI find a lot of these programs overlap - ie use same techniques, and get some very long copied and pasted programs!\nOvertime I improve these techniques and find myself having to backtrack over multiple programs to update these.\n\nHow? Can I? Create a more dynamic, systematic process. \nOne where all programs / procedures / classes are shared?\nHas it got a name?\n\nMy logical thought is that like 'procedures' and 'classes' I have would have smaller low level programs and mid level programs that called upon these - the GUI being the top program! but this would mean passing data to and from! Can Classes and Procedures be separate programs?\nMany thanks\nCameron\n", " You can't. Procedural programming is a separate language, not a part of the language. It's a different paradigm, with different syntax. You cannot share a Procedurally-Programmed program with a Program that Uses Procedured Programming. The Procedurer Programming language is not part the Procedure Programming Language. If you want to share Procedurly-programmed code with Proceduire Programming, you need to use Proceduring Language, which is part Procedura Programming and part Programming in general. This is the way the world works. There are many different Procedurers, but Proceduration is one of them. So, Proceduri Programming is Procedury Language and Proceduria is Programming In general, so Proceduru Programming would be Proceduro Language or Proceduras Language (or Proceduran Language). Procedurus Language is used for Procedures, while Proceduron Language for Procedures. In Proceduren Programming you can use procedural programming techniques to create Procedual Programming code. But Procedurion Programming can be used to program Procedues, too. And Procedurous Programming uses Procedinatory Language to do Procedue Programming ( Procedureau Programming).\n", "There is a range of techniques you can use : \n\nShared modules - A set of modules with well defined interfaces - functions, classes etc. These modules sit in a folders which every application you use can get to - i.e. the path to the folder is added to PYTHON_PATH environment variable. These interfaces should be engineered so when you add functionality to them they don't break the old applications.\n\n Design patterns - Design your applications with a good design pattern - MVC (Master View Controller) is a useful one for GUI programs. V is your GUI - and exposes only a few methods, which aren't actually dependent on the GUI itself (for instances methods such as display_foo). The Master is your data access functionality - again with well defined interfaces. Controller interfaces between View and Master. There may be other patterns which apply to your application too.\n\n", 0.7959980368614197, 0.8115941286087036, 0.8037204742431641, 0.30322580645161284, 7.724180115516485e-232, 1176], ["I am developing a tcp/ip server whose purpose is receive packets from client, parse them, do some computation(on data arriving in packet) and store it in database. Till now, everything was being done by single server application written using twisted python. Now I am across RabbitMQ so my question is, if it is possible and if it will lead to better performance if my twisted server application just receives the packets from clients and pass it another c++ application using RabbitMQ. The c++ application will in turn parse packets, do computation on it etc.. Everything will be done on single server.\n", " You can use a Twisted server to do this.  You will need to use the same Twisted reactor as the Twisted client. You'll also need a way to communicate between the two. I'm not sure what you mean by \"pass it a c++) application\" but you can do that. Just make sure that the c-application has a reference to the reactor. If you're using Twisted, you'll need the following:\nclass MyReactor(reactor.BaseReuseable):\n   def __init__(self, reactor): self.reuse()\n", "If your server is does not receive packets often, it will not improve much - only gain some tiny overhead on inter server communication. Still it is a very good design idea, because it scales well and once you finally get many packets you will just add an instance of data processing server.\n", 0.8147332072257996, 0.8445969223976135, 0.8293963074684143, 0.1323529411764706, 9.078913287470064e-232, 454], ["I have a long-running daily cron on OpenShift. It takes a couple hours to run. I've added nohup and I'm running it in the background. It still seems to timeout at the default 5 minutes (It works appropriately for this time). I'm receiving no errors and it works perfectly fine locally. \n\nnohup python ${OPENSHIFT_REPO_DIR}wsgi/manage.py do_something >> \\\n  ${OPENSHIFT_DATA_DIR}do_something_data.log 2> \\\n  ${OPENSHIFT_DATA_DIR}do_something_error.log &\n\nAny suggestions is appreciated.\n", " You can use the --noclear option to nohUP. This will clear the screen and kill the process. You will then be able to see the output of the command. If you want to keep the noclean, you can add the following to your crontab:\n15 * * 0 *   * noclose\n", "I'm lazy. Cut and paste :)\nI have been told 5 minutes is the limit for the free accounts. That includes all background processes. I asked a similar question here on SO. \n", 0.8182016015052795, 0.852617084980011, 0.835054874420166, 0.0759493670886076, 1.0464930402564957e-231, 248], ["I am supposed to be doing research with this huge Fortran 77 program (which I recently ported to Fortran 90 superficially). It is a very old piece of software used for modeling using finite element methods.\n\nIt is a monstrosity. It is roughly 240,000 lines.\nSince it began its life in Fortran 77, it uses some really dirty hacks for dynamic memory allocation; basically it uses the functions from the C standard library, mixed programming with C and Fortran. I am yet to fully grasp how allocation works. The program is built to be easily extendable by the user, and the user generally needs to allocate some globally accessible arrays for later use. This is done by having an array of memory addresses, which point to the beginning addresses of dynamically allocable arrays. Of course, which element of the address array pointing to which information all depends on conventions which has to be learned by the user, before one can start to really program. There are two address arrays, one for integers, and the other for floating points.\nBy dirty hacks, I mean inconsistent ones. For example an update in the optimization algorithm of the GNU compilers caused the program to exit with random memory leaks.\nThe program is far from elegant. Global variable names are generally short (3-4 characters) and cryptic. Passing data across routines is of course accomplished by using common blocks, which include all program switches, and the aforementioned arrays.\nThe usage of the program is roughly like that of an interactive shell, albeit a stupid one. First, an input file is read by the program itself, then per choice, the user is dropped into a pseudo-shell, in which the user has to type 4 character wide commands, followed by the parameters. The parser then parses the command, and corresponding subroutine is called with the parameters. You would guess that there is a loop structure in this pseudo-parser (a goto bonanza, rather) which wraps the subroutine behavior in a manner more complex than it should be in the 21st century. \nThe format of the input file is the same (commands, then parameters), since it is the same parser. But the syntax is not really consistent (by that, I mean it lacks control structures, and some commands cause the finite state machine to do behavior that contradict with other commands; it lacks definite grammar), time to time causing the end user to discover pitfalls. The user must learn these pitfalls by experience; I did not see them in any documentation of the program. This is a problem that can easily be avoided with python, and it is not even necessary to implement a parser.\n\nWhat I want to do:\n\nPort parts of the program into python, namely the parts that don't have anything to do with numerical computation. This includes \n\ncleaning up and abstracting the API with an OOP approach in python,\ngiving meaningful variable names, \nmigrating dynamic allocation to either numpy or Fortran 90 and losing the C part,\nmigrating non-numerical execution to python, and wrap the numerical objects using f2py, so there is no loss in performance. Have I told that the program is damn fast in its current state? Hopefully porting the calls to numerical subroutines and I/O to python will not slow it down to an impractical level (or will it?).\nMaking use of python's interactive shell as a replacement for the pseudo-shell. This way, there will not be any inconsistencies for the end user. The aforementioned commands will be simply replaced by functions defined in python. This will allow the user to actually access the data. Plus, the user will be able to extend the program without going to deep.\n\n\nWhat I wonder:\n\nIs f2py suitable and up-to this task of wrapping numerous subroutines and common blocks without any confusion? I have only seen single-file examples on the net for f2py; I know that numpy has used it to wrap LAPACK and stuff, but I need reassurance that f2py is a tool consistent enough for this task.\nWhether there are any suggestions on the general strategy that I should follow, or pitfalls I should avoid.\nHow can & should I implement a system in this python-wrapped Fortran 90 environment, so that I will be able to modify (allocate and assign) globally accessible arrays and variables inside fortran routines. This should preferably omit address arrays and I should preferably be able to inject verbal representations into the namespaces. These variables should preferably be accessible inside both python and fortran.\n\nNotes:\n\nI may have been asking for too much, something beyond the boundaries of the possible realm. In this case, please forgive me for I am a beginner with this aspect of programming; and don't hesitate to correct me.\nThe \"program\" I have been talking about is open source but it is commercial and the license does not allow its distribution, so I decided not to mention its name. However, you could deduce it from the 2nd sentence and the description I gave throughout.\n\n", " The", "I'm doing something depressingly similar. Instead of dynamic memory allocation via C we have a single global array with integer indices (also at global scope), but otherwise it's much the same. Weird, inconsistent input file and all. \nI'd advise against trying to rewrite the majority of the program, whether in python or anything else. It's time consuming, unpleasant and largely unnecessary. As an alternative, get the F77 code base to the point whether it compiles cleanly enough that you're willing to trust it, then write an interface routine. \nI now have a big, ugly F77 code base which sits behind an interface. The program requires input as a text file so a large part of the interface's job is to produce that text file. Beyond that, the legacy code is reduced to a single gateway routine which takes a few arguments (including a means of identifying the text file) and returns the answer. If you use the iso_c_binding of Fortran 2003 you can expose the interface in a format C understands, at which point you can link it to whatever you wish. \nAs far as the modern code (mostly optimisation routines) is concerned, the legacy code base is the single subroutine behind the C interface. This is much nicer than trying to modify the old code further and probably a valid strategy for your case as well. \n", 0.9997026324272156, 0.7762627601623535, 0.8739268183708191, 0.008474576271186442, 1.821831989445342e-231, 4], ["Is it good Python practice to have a function return None most of the time, while exceptionally returning useful values by raising an exception where the values are stored?\nI am a little uneasy with this because exceptions are most often used to signal some kind of problem, with some attributes of the exception giving some details about the problem. Here, I would like the exception to actually mean \"here is an exceptional result of this function\".\nUsing an exception for this is tempting because (1)\u00a0this is only done in exceptional circumstances and (2)\u00a0this is efficient, in Python (more than an if \u2026 is not None:\u2026). On the other hand, the exception itself is not the sign of an error of any sort, just a vehicle for the exceptionally returned values.\nIs there any official recommendation against using exceptions for exceptionally returning values from a function?\nPS: Here is a use case:\n\nAn object method updates the internal state of the object based on new data (it's a finite state automaton).\nAt some point (usually after getting many data points), the method considers that some action must be taken (in my case: some date from the object should be stored in a database, and the object's state is reset to the initial state, where it is ready to get more data).\n\nThus, the sequence of events, for the method, is: get data, update state, get data, update state,\u2026 ah! we reached an special state where information about the object should be stored! reset the objet's state and send the relevant information out; get data, update state, get data, update state,\u2026 Thus, most of the time, the function updates the internal state of the object and does not return anything. Exceptionally, it must send important information about the object.\n", " I think the best way to do this would be to return a tuple of (result, state) where result is None if the state was not updated, otherwise a result object that has the same state as the result tuple. This way, you can check for exceptions and return the tuple if you want to. \n", "Exceptions can model any exception to the normal flow of the code. Errors are the most common use-case, but anything where the normal return type doesn't really make sense can be seen as an opportunity to raise an exception instead.\nRaising an exception for non-error use-cases is perfectly fine, if it simplifies your flow.\nRemember, the Exception object raised and caught, is itself a value. Attaching information to that is a perfectly acceptable method to communicate with a caller to signal a deviation from 'normal'.\nI'd prefer an exception over a sentinel value like None; it makes the handling explicit, and can actually lead to cleaner handling and better feedback to a developer when they forget to handle the exceptional return value. If your code normally returns a list, but in specific, exceptional circumstances, you return None, and the caller doesn't handle it, then you get weird bugs down the line somewhere. TypeError: 'NoneType' object is not iterable is a lot more cryptic than an explicit unhandled exception.\n", 0.8453586101531982, 0.821792483329773, 0.8334090113639832, 0.26200873362445415, 1.0353724960150371e-231, 278], ["I am writing a socket based \"python cmd like\" server module which can support cli interactive functions such as autocompletion or command history, by doing that a simple \"telnet\" or \"nc\" client side may able to connect to server to read/set something on server side.\nafter searching, there are a lot of modules can do \"cmd\" part such like python standard module \"cmd\" or \"ipython\" or even vty simulator, however, I cannot find a module can actually bind to socket directly to detect keystrokes such as \"tab\" key or \"control+c\" client side. Most of them just able to process line(s) read, which not suitable for autocompletion with tab press or command history with up/down press. \nI think this question can be simplify to:\nIs that possible to read socket keystroke input non-blocking, then process this key input value somehow server side - for example ASCII code + 1, then echo back to socket to show in client side?\nThank you for your help.\n", " You can use the socket module to do this. It has a read method that will return the key pressed, and a write method to send the character. You will need to use a blocking read to get the data, but you can send it as soon as you get it. The key is sent as a byte, so you need a buffer of at least 32 bytes. If you want to be able send more than one key at a time, you will have to buffer the keys in a list, or use something like a queue. I'm not sure how you would do that with a nonblocking read. Also, if you are using a GUI, it will be much easier to just use keypress events. This will allow you to have a key press event for every key, rather than having to keep track of the state of every character in the buffer. For example, the \"Ctrl+C\" command will send a character, send Ctrl+A, Ctrl-A to the server, get Ctrl, A, C, to your client, etc. Then you just need the Ctrl key to make the connection. There are many ways to implement this, depending on your needs. One way is to create a Queue object, with the Queues being the buffers, where you keep the characters in, as well as the queue of keys. When you press Ctrl you send an event to this Queuue, that is then sent to whichever client you have. Another way would be to put the keyboard in your GUI. In this case, when you hit Ctrl it sends a KeyPress event, this is passed to whatever client is currently connected. A third way, is that you use your own QueuedConnection object. With this you could send keys to a client when the user presses Ctrl. However, in this way you still need your Queuing object", "What you want is not possible. As you say, you want to write a socket based cmd like server. The server will open a socket and listen for data from the client. Now it is possible to read socket input character by character (which is not the same as non-blocking BTW), but that will not help you. \nIt is up to the client program to decide how and when to send the data. So if the client side program decides to \"eat\" tab and control characters, then you will simply not see them. So if you want to process keystrokes one by one, you will also need a client application.\n", 0.8307391405105591, 0.856923520565033, 0.8436282277107239, 0.30769230769230765, 6.83143959285042e-232, 1568], ["I've developed a little Python Dropbox app but I have no I idea how to hide the app key and app secret. Until I solve this problem I'm not sure how I can ship my app as this seems to be a significant security threat.\nI know it is hard to obfuscate code, most especially Python so I'm not really sure that that is an option.. but what else could I do? I thought about using some form of encryption and/or storing them on a server to be retrieved when the app starts. Is it possible to write the part that deals with the keys in another language that's more easily to obfuscate like C? As I don't know much about encryption, I'm not sure if any of these options are feasible or not. \n", " Dropbox has a \"Dropbox Key\" that you can use to encrypt your Dropbox files. Dropbox will not decrypt your files unless you give it the Dropbox Key. You can also use the \"Signing Key\", which is a random string that Dropbox uses to verify that the file is yours. This is not a security feature, but it's a convenience feature. If you're worried about someone getting your private Dropbox key, you could encrypt the signing key with a password, and then use that password to decrypt the files when you want to. The Dropbox Signing key is encrypted with your password. It's not encrypted using a key that anyone else has access to, so it can't be read by anyone but you. (Note that this is the same as using the password in the encryption process, except that it uses the Signning Key instead of the Key.)\n", "To prevent casual misuse of your app secret (like someone who copy/pastes code not realizing they're supposed to create their own app key/secret pair), it's probably worth doing a little obfuscation, but as you point out, that won't prevent a determined individual from obtaining the app secret.\nIn a client-side app (like a mobile or desktop app), there's really nothing you can do to keep your OAuth app secret truly secret. That said, the consensus seems to be that this doesn't really matter. In fact, in OAuth 2, the recommended flow for client-side apps is the \"token\" or \"implicit\" flow, which doesn't use the app secret at all.\n", 0.8425264358520508, 0.8368768692016602, 0.8396921157836914, 0.2825278810408922, 8.139859933126613e-232, 803], ["I know python and want to contribute on OpenSource projects that features python. Anyone can help me where to contribute and how.\nI already googled it and find github and code.google as a good place to contribute but how to start it I don't know.\nSuggest how to get started.\n", " There are a lot of ways to do this. \nThe first thing you need to know is that you can contribute to any project on github. You can fork the project and make your own fork. Then you just need a link to the original project. This is the way that I've done it. I just fork a project, make my own copy, and then link the fork to my fork on the main project page. That way I can see the history of the code and see what changes have been made. It's a bit of a pain to make a fork, but it's the only way to see how the changes are made and what the differences are. Once you've made your fork you'll be able to add your changes to your original fork and you will be good to go. If you want you could also make an account on code-gatehouse and contribute there. Code-Gatehouse is a great place for this sort of thing. There's also a ton of other ways you might be contributing. For example, you may be interested in contributing to a new project that is being built on top of your project or you're interested to help out a community member. In that case you should be looking at the projects on which the community members are working. The way you contribute is up to you. Just make sure that the people you are contributing are the same people that are on your projects. Also, if you have a question about how something works, just ask the person who is working on it, not the maintainer. They'll know the answer to that question. :)\n", "Not sure if this is an appropriate question for SO - you might get voted down. But ...\nWhenever I have seen this question, the answer is almost always:\n\nfind a project you like / you're interested in\nfind something in that project that you feel you can fix / enhance (have a look through their bug tracker)\nfork the project (github makes this easy)\nmake the change, find out what is appropriate for that project (documentation, unit tests, ...)\nsubmit the change back to the project (github has \"request pull\")\n\nGood luck!\n", 0.8248246908187866, 0.8300994634628296, 0.8274536728858948, 0.2796833773087071, 7.256879962411806e-232, 1430], ["So in my spare time, I've been developing a piece of network monitoring software that essentially can be installed on a bunch of clients, and the clients report data back to the server(RAM/CPU/Storage/Network usage, and the like). For the administrative console as well as reporting, I've decided to use Django, which has been a learning experience in itself. \nThe Clients report to the Server asynchronously, with whatever data they happen to have(As of right now, it's just received and dumped, not stored in a DB). I need to access this data in Django. I have already created the models to match my needs. However, I don't know how to go about getting the actual data into the django DB safely. \nWhat is the way to go about doing this?  I thought of a few options, but they all had some drawbacks:\n\nGive the Django app a reference to the Server, and just start a thread that continuously checks for new data and writes it to the DB. \nHave the Server access the Django DB directly, and write it's data there. \n\nThe problem with 1 is that im even more tightly coupling the server with the django app, but the upside is that I can use the ORM to write the data nicely. \nThe problem with 2 is that I can't use the ORM to write data, and I'm not sure if it could cause blocking on the DB in some cases. \nIs there some obvious good option I'm missing? I'm sorry if this question is vague. I'm very new to Django, and I don't want to write myself into a corner.\n", " I think you're going to need a database. Django is a framework, so it has a lot of built-in functionality that you can leverage. You can write your own database, or use one of the many existing ones. The Django ORMS is very flexible, you'll be able to do whatever you want with it. If you use a relational database you will have to make some choices about how you store your data. For example, if you have a table for each client, then you might want a separate table to store the information about the client. Or you could just use an XML database that has the same structure as a MySQL database but with a different schema. There are many choices you need make, just make sure you make the right choices. Good luck!\n", "I chose option 1 when I set up my environment, which does much of the same stuff.\nI have a JSON interface that's used to pass data back to the server.  Since I'm on a well-protected VLAN, this works great.  The biggest benefit, like you say, is the Django ORM.  A simple address call with proper data is all that's needed.  I also think this is the simplest method.\nThe \"blocking on the DB\" issue should be non-existent.  I suppose that it would depend on the DB backend, but really, that's one of the benefits of a DB.  For example, a single-threaded file-based sqlite instance may not work.\nI keep things in Django as much as I can.  This could also help with DB security/integrity, since it's only ever accessed in one place.  If your client accesses the DB directly, you'll need to ship username/password with the Client.\nMy recommendation is to go with 1.  It will make your life easier, with fewer lines of code.  Besides, as long as you code Client properly, it should be easy to modify DB access later on down the road.\n", 0.8593257665634155, 0.8327606916427612, 0.8458346724510193, 0.3136094674556213, 8.792092583952628e-232, 719], ["I have a Kivy-based Python project that I'm trying to build. It uses the NavigationDrawer component from Kivy Garden, through an import:\n\nfrom kivy.garden.navigationdrawer import NavigationDrawer\n\nI have a PyInstaller spec file for it which builds a distributable version. This version works well on my machine, but unfortunately not on other machines. Running the interpreter in the 'dist' version with the -v switch, it appears that when I run the distributable on my machine, the navigationdrawer component is not actually coming from inside my build folder. All the other imports show something like:\n\nimport kivy.graphics.gl_instructions # dynamically loaded from C:\\Users\\me\\myapp\\dist\\RACECA~1\\kivy.graphics.gl_instructions.pyd\n\nBut the navigationdrawer import says:\n\nimport kivy.garden.navigationdrawer\n\"\"\"directory C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\n  C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.pyc matches C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.py\n  import kivy.garden.navigationdrawer # precompiled from C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer\\__init__.pyc\"\"\"\n\nBut noo! I don't want you to import them from c:\\users. I want them to get nicely copied into my dist folder like all the other imports. I've tried adding c:\\users\\me to PyInstaller's pathex, the system PATH and PYTHONPATH without any joy. Anyone have any ideas?\n", " You need to add the directory to your Pydist path.\n", "You could just copy the navigationdrawer code from C:\\Users\\me\\.kivy\\garden\\garden.navigationdrawer to your app directory, call the folder 'navigationdrawer' and replace the import with from navigationdrawer import NavigationDrawer.\nIt's not quite the 'right' way to do it (there's probably some way to make pyinstaller copy it in), but it should work fine.\n", 0.890256941318512, 0.8124336004257202, 0.8495667576789856, 0.17647058823529413, 1.3974156860693505e-231, 52], ["I've been trying to learn Twill Scripting on Python and I am using Python 3.4 and Twill 1.8.0. Been reading some posts here and I found it interesting to study. But, I do have a problem installing Twill. I just knew that PRINT in Python is now a function so meaning it needs parenthesis and that's where my problem starts. As I look through the codes on Twill 1.8.0, I think it isn't oriented with the change of using PRINT yet. Strings are not enclosed with parenthesis so I was thinking maybe there is a new patch or version of Twill that adapts the changes of Python, is there any? or is there anything I can do about this aside from manually editing Twill files?\nYour response is highly appreciated.\n", " The problem is that the new print function in twill is not compatible with python 3, so you need to use the old print in python 2.7.2. The new function is compatible, but it is still not the same as the print from python. You can use it like this:\nprint('hello')\n", "I just found the answer. Thanks for viewing this question and probably for that downvote (SAD FACE).. So, the solution that I found was to use the 2to3 Script Tool found in Python Folder. Basically, it refactors Python 2 codes to Python 3 codes.\n", 0.8496332764625549, 0.8426895141601562, 0.8461471796035767, 0.3061224489795918, 1.0396732012273364e-231, 264], ["I have written a library whose main functionality is implemented in C (speed is critical), with a thin Python layer around it to deal with the ctypes nastiness.\nI'm coming to package it and I'm wondering how I might best go about this. The code it must interface with is a shared library. I have a Makefile which builds the C code and creates the .so file, but I don't know how I compile this via distutils. Should I just call out to make with subprocess by overriding the install command (if so, is install the place for this, or is build more appropriate?)\nUpdate: I want to note that this is not a Python extension. That is, the C library contains no code to itself interact with the Python runtime. Python is making foreign function calls to a straight C shared library.\n", "You can use the distro's python-config to specify the path to the python interpreter. Then you can call python with that path. You will need to use a different name for the shared lib than the one you are linking to. For example, if you use libpython2.7.so, you will have to change the name of the lib to libpy2_7_so. If you want the same name as the library, then you need a name like libmylib.a. \n", "I'd consider building the python module as a subproject of a normal shared library build.  So, use automake, autoconf or something like that to build the shared library, have a python_bindings directory with a setup.py and your python module.\n", 0.8467361330986023, 0.8542197942733765, 0.8504615426063538, 0.3064516129032258, 9.459421970329084e-232, 399], ["I took a new clean install of OSX 10.9.3 and installed pip, and then did\n\npip install pandas\npip install numpy\n\nBoth installs seemed to be perfectly happy, and ran without any errors (though there were a zillion warnings).  When I tried to run a python script with import pandas, I got the following error:\n\n\n    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): \n    File \"./moen.py\", line 7, in  import pandas File \"/Library/Python/2.7/site-packages/pandas/__init__.py\", line 6, in  from . import hashtable, tslib, lib \n    File \"numpy.pxd\", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) \n    ValueError: numpy.dtype has the wrong size, try recompiling\n\n\nHow do I fix this error and get pandas to load properly?\n", "You need to recompile ndarray.\n", "pip uninstall numpy uninstalls the old version of numpy\npip install numpy finds and installs the latest version of numpy\n", 0.8735670447349548, 0.8389148116111755, 0.8558903336524963, 0.0, 1.519465049176112e-231, 31], ["I knew that there is no builtin sph_hankel1 in scipy then i want to know that how to implement it in the right way?\nAdditional: Just show me one correct implementation of sph_hankel1 either using of Scipy or Sympy.\n", " There is a built-in function in Python called sphinx.hankels. This function takes a list of strings and returns a string. \n", "Although it would be nice if there were an existing routine for calculating the spherical Hankel functions (like there is for the ordinary Hankel functions), they are just a (complex) linear combination of the spherical Bessel functions of the first and second kind so can be easily calculated from existing routines.  Since the Hankel functions are complex and depending on your application of them, it can be advantageous to rewrite your expression in terms of the Bessel functions of the first and second kind, ie entirely real quantities, particularly if your final result is real.\n", 0.84610515832901, 0.8167062997817993, 0.8311458230018616, 0.10256410256410257, 1.195597122887507e-231, 124], ["I'm storing strings on the order of 150M. It's well-below the maximum size of strings in Redis, but I'm seeing a lot of different, conflicted opinions on the approach I should take, and no clear path.\nOn the one hand, I've seen that I should use a hash with small data chunks, and on the other hand, I've been told that leads to gapping, and that storing the whole string is most efficient.\nOn the one hand, I've seen that I could pass in the one massive string, or do a bunch of string-append operations to build it up. The latter seems like it might be more efficient than the former.\nI'm reading the data from elsewhere, so I'd rather not fill a local, physical file just so that I can pass a whole string. Obviously, it'd be better all around if I can chunk the input data, and feed it into Redis via appends. However, if this isn't efficient with Redis, it might take forever to feed all of the data, one chunk at a time. I'd try it, but I lack the experience, and it might be inefficient for a number of different reasons.\nThat being said, there's a lot of talk of \"small\" strings and \"large\" strings, but it's not clear what Redis considers an optimally \"small\" string. 512K, 1M, 8M?\nDoes anyone have any definitive remarks?\nI'd love it if I could just provide a file-like object or generator to redis-py, but that's more language-specific than I meant this question to be, and most likely impossible for the protocol, anyway: it'd just require internal chunking of the data, anyway, when it's probably better to just impose this on the developer.\n", " The problem with using a string as a key is that it is not a good key.  It is a poor key because it has no uniqueness. A better key would be a cryptographic hash, which is unique. This is what the Red-Hex key does. Red is using the same hashing algorithm as the rest of Red, however, the key itself is different. So, you can't use the string key as your key, because the hashing function is the only thing that matters. If you want to use it as part of your cryptographic key you need to change the algorithm. You can use Red's own hashing functions, such as SHA-1, SHA2, etc. to do this. Alternatively, Red can be configured to support a different hashing method, for example, using SHA1 with a fixed key length. In this case, your string will be the hash of that key and the length of this key will not matter. For example:\nkey = \"mykey\"\nmy_key_hash = Red.sha1(key)\nprint my", "One option would be:\nStoring data as long list of chunks\n\nstore data in List - this allows storing the content as sequence of chunks as well as desctroying whole list in one step\nstore the data using pipeline contenxt manager to ensure, you are the only one, who writes at that moment.\nbe aware, that Redis is always processing single request and all others are blocked for that moment. With large files, which take time to write you can not only slow other clients down, but you are also likely to exceed max execution time (see config for this value).\n\nStore data in randomly named list with known pointer\nAlternative approach, also using list, would be to invent random list name, write content chunk by chunk into it, and when you are done, update value in known key in Redis pointing to this randomly named list. Do not forget to remove old one, this can be done from your code, but you might use expiration if it seems usable in your use case.\n", 0.8294224143028259, 0.816665530204773, 0.822994589805603, 0.3063583815028902, 8.142831280537217e-232, 877], ["I have the string name = 'one two'. i want to make 'onetwo' from it. \nis there any cool python shortcut for it like .join() but without space?\n", "You can use the str.join method to join the strings.\n>>> name.strip()\n'one'\n", "How about \"\".join(name.split(\" \")) ?\n", 0.8554456233978271, 0.8642316460609436, 0.8598161935806274, 0.21052631578947364, 1.2340561512781763e-231, 76], ["I have the string name = 'one two'. i want to make 'onetwo' from it. \nis there any cool python shortcut for it like .join() but without space?\n", "You can use the str.join method to join the strings.\n>>> name.strip()\n'one'\n", "You can do name.replace(' ','') or ''.join(name.split())\n", 0.8776009678840637, 0.8841909170150757, 0.8808836340904236, 0.34782608695652173, 1.3208690045708227e-231, 76], ["Previously I ran python 2.7 on Debian Linux terminal (bash). I conveniently use control-f, control-b to move forward/back word.\nBut it does not work on updated 3.4 version, which generates unreadable symbol.\nIs there a way to configure the control-key recognition?\n", " I'm not sure what you're trying to do, but I think you'll have to use the key combination Ctrl-F2, Ctrl -B2. \n", "If you compiled Python 3.4 from source, you are probably missing the development libraries for readline. The package is typically called libreadline-dev.\n", 0.8504986763000488, 0.8487947583198547, 0.8496458530426025, 0.12244897959183673, 1.2550544879260391e-231, 111], ["I am, as a hobby and best-practice exercise, to implement the same domain model (a simple GPS / GIS library, inspired in ISO 191xx stardards and OGC Abstract Model) both in Python and C#.\nIt first, I tought: \"well, ISO/OGC gave me a finished UML, so I will have each class in C# and in Python to have the same signature\".\nI quickly found myself stuck in the \"strict/static vs duck typing\" problem, since I cannot count on method signatures in python. For example:\n\nOverloading constructors is quite common and natural in C#, but in Python you have to resort to *args **kwargs and conditionals;\nProperties are encouraged in C#, but most source code I have seen around in Python tend to set the fields directly, although the use of @property or property() is quite straightforward.\n(and so on).\n\nActually there is (obviously) an obvious and very well documented \"difference in mindsets\" between one language and the other, and I would like to respect those differences while at the same time ending up with \"the same\" application, that is, equivalent domain model, architecture and functionality.\nSo my question basically is:\n\nIf I am to implement the same abstract model (\"UML-like\") in Python and C#, how should I proceed, and specifically, which constructs should be invariant, and which should be different?\n\n", " The key to this is to make sure that the implementation is the exact same.  If you are going to use the C++ API, then you will need to do the following: \n1)   Use the Python API. This will be the most common way to go. If the API is not available, you can use a C wrapper. You will also need a way of getting the data from the python object. In this case, the easiest way is probably to just use ctypes. But if you want to be able to call the functions, use C functions. The C API will not be available in all cases, however. So you may need some extra work to get the right data. Also, if the object is a class, it will probably be a good idea to create a wrapper class that has the methods you need. Then you just call that class from your python code. (Note that this will work for all python objects, not just C++)\n2) Use a Python class. Again, this may be your best bet. It will make it easier to change the code later. However, there are some things that you should do to ensure that your code is correct. First, make a copy of the class so that it is safe to modify. Second, do not use methods that are not defined in your class (such as __init__). Third, be sure", "What you called \"abstract model\" in MDA is called Platform Independent Model (PIM), and its implementation in C# and/or Python is called Platform Specific Model (PSM). It is supposed that there exist tranformations/code-generators from PIM to PSM's, so depending on how these code-generations work you will get appropriate C# and Python source code. Usually, such tools provide some means to control the code generated. And such control usually done via PIM annotations which are specific to PSM you are generating. Hope this helps.\n", 0.8196763396263123, 0.818773090839386, 0.81922447681427, 0.20245398773006137, 7.628597916728403e-232, 1171], ["I'm developing a django project for agriculture. I want to name an app \"fields\" and inside the app \"fields\" I want to name a model \"Field\" (referring to a farmer field). \nI tried it and it works, so I assume that \"fields\" and \"Field\" are not reserved words in Django or Python. But I was just wondering if using these words can be problematic in the future or it's just fine?\nAnd the general question:\nIs there any way to check if a word is reserved in Django or Python?\n", " There is no way in Python to determine if something is a reserved word.\n", "little explanation with fun\nAnswer is Simply No, \nBecause Language only has the authority to own anything.Python is the owner of the house\nThe Django guy is paying rent to Python guy. So, How Django guy can reserve the objects of the house?\nsame logic is applied here too\n", 0.8643913865089417, 0.825278103351593, 0.8443820476531982, 0.15625, 1.3342353171109297e-231, 73], ["Im using openCV on Ubuntu 14.04, but some of the functions that I require particularly in cv2 library (cv2.drawMatches, cv2.drawMatchesKnn) does not work in 2.4.9. How do I uninstall 2.4.9 and install 3.0.0 from the their git? I know the procedure for installing 3.0.0 but how do I make sure that 2.4.9 get completely removed from disk?\n", " You can't. You need to remove the package that contains the 2 version of ccv.\n", "The procedure depends on whether or not you built OpenCV from source with CMake, or snatched it from a repository.\nFrom repository\nsudo apt-get purge libopencv* will cleanly remove all traces. Substitute libopencv* as appropriate in case you were using an unofficial ppa.\nFrom source\nIf you still have the files generated by CMake (the directory from where you executed sudo make install), cd there and sudo make uninstall. Otherwise, you can either rebuild them with the exact same configuration and use the above command, or recall your CMAKE_INSTALL_PREFIX (/usr/local by default), and remove everything with opencv in its name within that directory tree.\n", 0.8644218444824219, 0.8031584620475769, 0.8326647281646729, 0.11290322580645161, 1.308146472423365e-231, 79], ["Running a python script on different nodes at school using SSH.  Each node has 8 cores. I use GNU Screen to be able to detach from a single process.\nIs it more desirable to:\n\nRun several different sessions of screen.\nRun a single screen process and use & in a bash terminal. \n\nAre they equivalent?\nI am not sure if my experiments are poorly coded and taking an inordinate amount of time (very possible) OR my choice to use 1. is slowing the process down considerably.  Thank you!\n", " I think you're looking for the \"shell\" command. It's a bit more complex than screen, but it's the way to go. You can use it to run a shell on a remote machine, and then attach to the shell. This way you can run several shell sessions on the same machine. The only problem is that you need to know the IP address of the machine you want to attach. If you use screen you just need the screen command, which is a lot simpler. Also, you don't need a separate screen session for each shell you run. Just attach the ssh session to a screen window. That way, the only thing you have to worry about is the ip address. And you only need one screen for all the sessions you do. So, I'd say that shell is your best bet.\n\n", "I would think they are about the same.  I would prefer screen just because I have an easier time managing it.  Depending on the scripts usage, that could also have some effect on time to process.\n", 0.8290297985076904, 0.8503900766372681, 0.8395740985870361, 0.17486338797814205, 8.115783091030524e-232, 711], ["I am planning on using IronPython to develop a GUI interface for some python code. Do I need to know any other programming languages other than python. Also if not are there any other GUI packages/addon's to python that only use python to implement and get the final product working?\n", " If you are going to use Iron Python, then you will need a good knowledge of Python.\n", "You don't need to know any other languages - modulo a few implementation differences, Python is Python is Python. You will, however, need to know the Microsoft windowing library, with which I believe you will have to interface to build a GUI.\n", 0.9059188365936279, 0.859487771987915, 0.8820927143096924, 0.27118644067796605, 1.3139703068575575e-231, 85], ["Say I have a categorical feature, color, which takes the values\n['red', 'blue', 'green', 'orange'],\nand I want to use it to predict something in a random forest.  If I one-hot encode it (i.e. I change it to four dummy variables), how do I tell sklearn that the four dummy variables are really one variable?  Specifically, when sklearn is randomly selecting features to use at different nodes, it should either include the red, blue, green and orange dummies together, or it shouldn't include any of them.\nI've heard that there's no way to do this, but I'd imagine there must be a way to deal with categorical variables without arbitrarily coding them as numbers or something like that.\n", " You can't do it. You have to encode the data as a string, and then you can use the skLearn.feature_columns.get_feature(...) method to get the features. If you want the feature names, you have the option of using sk.features.item().\n", "You have to make the categorical variable into a series of dummy variables. Yes I know its annoying and seems unnecessary but that is how sklearn works. \nif you are using pandas. use pd.get_dummies, it works really well. \n", 0.8695604801177979, 0.8591371178627014, 0.8643174171447754, 0.32941176470588235, 1.0726520105754455e-231, 233], ["Before I updated, I would run my script and output the html file. There would be my one plot in the window. I would make changes to my script, run it, output the html file, look at the new plot. Then I installed the library again to update it using conda. I made some changes to my script, ran it again, and the output file included both the plot before I made some changes AND a plot including the changes. I ran the script again out of curiosity. Three plots in the one file! Ran it again. Four! Deleted the html file (instead of overwriting). Five! Changed the name of the output html file. Six! I even tried changing the name of the script. The plots just keep piling up.\nWhat's going on? Why is it plotting every version of the graph I've ever made?\n", " The problem is that the data is not being plotted. You need to tell the code that you want to plot the values. In your case, you need a function that takes a list of data and returns a dataframe. \n", "as of 0.5.1 there is now bokeh.plotting.reset_output that will clear all output_modes and state. This is especially useful in situations where a new interpreter is not started in between executions (e.g., Spyder and the notebook)\n", 0.8614102602005005, 0.8321200013160706, 0.8465117812156677, 0.2195121951219512, 1.0966937361875488e-231, 198], ["I'm using distutils to compile C code via a python script. If things go wrong, I want to be able to catch the error output. To this end, I've redirected stdout and stderr into temporary files before running the setup() command (you need to use os.dup2 for this).\nOn linux, it works fine. On windows + mingw I get some really weird behaviour:\n\nWithout trying to capture, stdout and stderr are both written to the command prompt.\nWhen I try to capture, stdout works fine but the output to stderr disappears.\n\nDoes anybody understand what's going on here?\n", " On Windows, the problem is that the stdin and stdio streams are not being flushed. This means that when you redirect the streams, they are being written in the background. When you try and capture the stream, you get a \"connection reset by peer\" error. You can fix this by flushing the buffers. \n", "Woops.\nIt turns out this was something really simple: capturing stdout and stderr output was working just fine, but the particular error message I was looking to catch (which was windows specific) wasn't part of the printed output but the error message of the raised SystemExit exception.\nBig waste of time :(\n", 0.8622112274169922, 0.8470331430435181, 0.8545547723770142, 0.1523809523809524, 1.0003688322288243e-231, 297], ["In our company we using vagrant VM's to have the save env. for all. Is it possible to configure VisualStudio + PTVS (python tools for VS) to use vagrant based python interpreter via ssh for example?  \n", " You can use the same ssh key for both the virtual machine and the host machine.\n", "There's no special support for remote interpreters in PTVS, like what PyCharm has. It's probably possible to hack something based on the existing constraints, but it would be some work...\nTo register an interpreter that can actually run, it would have to have a local (well, CreateProcess'able - so e.g. SMB shares are okay) binary that accepts the same command line options as python.exe. It might be possible to use ssh directly by adding the corresponding command line options to project settings. Otherwise, a proxy binary that just turns around and invokes the remote process would definitely work.\nRunning under debugger is much trickier. For that to work, the invoked Python binary would also have to be able to load the PTVS debugging bits (which is a bunch of .py files in PTVS install directory), and to connect to VS over TCP to establish a debugger connection. I don't see how this could be done without writing significant amounts of code to correctly proxy everything.\nAttaching to a remotely running process using ptvsd, on the other hand, would be trivial.\nFor code editing experience, you'd need a local copy (or a share etc) of the standard library for that interpreter, so that it can be analyzed by the type inference engine. \n", 0.8642748594284058, 0.8009469509124756, 0.8314067721366882, 0.08583690987124463, 1.3152028431736784e-231, 81], ["I'm a newbee of python. And recently I heard some people say that numpy is a good module for dealing with huge data.\nI'm curious what can numpy do for us in the daily work.\nAs I know, most of us were not scientists and researchers, at what circumstances numpy can bring us benefit? \nCan you share a good practice with me?\n", " ndarray is the most common data structure in python, and it's a very good one.  It's very flexible, it has a lot of built-in functions, etc. etc., and you can use it for almost any kind of data you want. It has the advantage of being very easy to use, too. I've used it in a number of projects, including a project that I was involved in where I used numpydata for a large number (hundreds of thousands) of images. The images were all in.jpg format, so I had to convert them to.npy format first, then load them into npy. Then I could use nn.utils.image_to_array to load the images into a narray. This was a fairly large project, but it was done in about a week. There were a few problems, though, that were caused by the fact that the data was in nybbles, not numps. For example, the nb_rows and n_cols attributes of the image were both integers, which meant that they had a different size than the actual image. So I needed to make sure that my nnp.ndarray was the same size as the original image, or else I'd get a data error. Also, I couldn't just use the.ndimage.load method, because it would fail if the file was too large. Instead, my code used the load method of nimage, passing it the path to the source image file, a flag to indicate whether it should be loaded as a.png or.jpeg, an array of integers to specify the dimensions of each pixel, as well as an optional array to tell nimg to ignore the first row and column of an image if it is too small, if there are too many rows or columns, for example. That way, nimpydatastore.py was able to handle the conversion from.img format to nypy.numpy format. Once I got the code working, however, there were still a couple of problems. One was that it wasn't very efficient, since it had the overhead of loading the entire image into memory, converting it to a byte array, loading it into the array structure, doing the math, writing the byte arrays to disk, reading the disk back, calculating the offsets, copying the arrays, deleting the old arrays and writing them, renaming the files, opening the new files and deleting them. Another problem was, of", "Long answer short, when you need do huge mathematical operations, like vector multiplications and so on which requires writing lots of loops and what not, yet your codes gets unreadable yet not efficient you should use Numpy.\nFew key benefits:\n\nNumPy arrays have a fixed size at creation, unlike Python lists (which can grow dynamically). Changing the size of an ndarray will create a new array and delete the original. So it is more memory efficient than the other.\nThe elements in a NumPy array are all required to be of the same data type, and thus will be the same size in memory. The exception: one can have arrays of (Python, including NumPy) objects, thereby allowing for arrays of different sized elements.\nNumPy arrays facilitate advanced mathematical and other types of operations on large numbers of data. Typically, such operations are executed more efficiently and with less code than is possible using Python\u2019s built-in sequences.\nA growing plethora of scientific and mathematical Python-based packages are using NumPy arrays; though these typically support Python-sequence input, they convert such input to NumPy arrays prior to processing, and they often output NumPy arrays. In other words, in order to efficiently use much (perhaps even most) of today\u2019s scientific/mathematical Python-based software, just knowing how to use Python\u2019s built-in sequence types is insufficient - one also needs to know how to use NumPy arrays.\n-Vector operations comes handy in Numpy. You don't need to go through writing loops but yet pythonic.\n-Object oriented approach\n\n", 0.8140654563903809, 0.8225507736206055, 0.8182860612869263, 0.2964071856287425, 6.676334516713973e-232, 2107], ["I've taken to creative coding on my iPad and iPhone using Codea, Procoding, and Pythonista. I really love the paper.js Javascript library, and I'm wondering how I might have the functionality that I find in paper.js when writing in Python.\nSpecifically, I'd love to have the vector math and path manipulation that paper.js affords. Things like finding the intersection of two paths or binding events to paths (on click, mouse move, etc).\nThere's an ImagePath module provided by Pythonista that does some path stuff but it's not as robust as paper.js (it seems).\nAny ideas?\n", " There's a paper-js-like library called Path. It's written in Javascript, but you can use it in your Python code. You can find it on the github page. \n", "The ui module actually includes a lot of vector drawing functions, inside a ui.ImageContext. ui.ImageContext is a thin wrapper around part of one of the Objective-C APIs (maybe CALayer?) The drawing methods are designed to operate inside the draw method of a custom view class, but you can present these things in other contexts using a UIImageContext, from which you can get a static image.\n", 0.8655588626861572, 0.8382604718208313, 0.8516910076141357, 0.1616161616161616, 1.2060452046389539e-231, 151], ["This has been bugging me for a while. I'm wondering about the comparative memory-efficiency of assigning variables and calling methods. Consider, for example:\n\ns = \"foo\"\n  x = s.lower()\n\nVersus\n\nx = \"foo\".lower()\n\nWhich one of these is more efficient in terms of memory use? \nThis is obviously a trivial example, but it illustrates what I'm wondering about.\nThere are many instances in which we define some variable var1 = foo, and then define a second variable var2 = var1.method(). Does this total process require more memory than just defining var2 = foo.method()? \n", " I think the answer is yes.  The first method is slightly more expensive, because it has to copy the string, which is a lot more than the second method. However, the first one is much more likely to be used, so it's probably worth it.\n\n", "In your first example s is still in memory until the garbage collector deletes it, so your second example is more efficient in terms of memory use. However, considering that's a very very little portion of the RAM, in many cases it's better to go for readability (the first example looks better).\nHope this helps.\n", 0.8781843185424805, 0.8609826564788818, 0.8694984316825867, 0.3106796116504854, 1.0878290104669458e-231, 236], ["I have a series of unit tests that are meant to run in two contexts:\n1) On a buildbot server\n2) in developer's home environments\nIn both our development procedure and in the buildbot server we use virtualenv. The tests run fine in the developer environments, but with buildbot the tests are being run from the python executable in the virtualenv without activating the virtualenv. \nThis works out for most tests, but there are a few that shell out to run scripts, and I want them to run the scripts with the virtualenv's python executable. Is there a way to pull the path to the current python executable inside the tests themselves to build the shell commands that way?\n", " You can use the pytest.test_setup() function to set up the environment for your tests.\n", "The current python executable is always available as sys.executable, which should give full path (but you can ensure this using os.path functions).\n", 0.8772863149642944, 0.8621492385864258, 0.8696519136428833, 0.15, 1.2578972116573994e-231, 88], ["I have a large buffer of strings (basically 12GB) from a C app.\nI would like to create PyString objects in C for an embedded Python interpreter without copying the strings. Is this possible?\n", " You can use the ctypes module to do this.  You'll need to install cpython, which is part of the standard library. Then you can call ctype.py_string_from_buffer(buffer, len(buf)) to get a PyObject.\n\n", "I don't think that is possible for the basic reason that Python String objects are embedded into the PyObject structure.   In other words, the Python string object is the PyObject_HEAD followed by the bytes of the string.    You would have to have room in memory to put the PyObject_HEAD information around the existing bytes.\n", 0.8509370684623718, 0.8501459360122681, 0.850541353225708, 0.1894736842105263, 1.1352078042965734e-231, 199], ["I have a dashboard I am working on, using Python cherrypy framework and Mako template language.\nI had an html file for each of dashboard pages.\nThere, I used Mako to pass some data to html and inline Javascript.\nFor example, to display the names of some processes when I only had the list of ids, I passed a Python dict that maps ids to their corresponding names, and then used the dict in ${} tags.\nHowever, as I am now moving these Javascript codes into a separate file with .js extension, I found out simply putting the same Mako code blocks in the Javascript code does not work.\nIs there any way I could use Mako template language in an external .js file that is imported in an html file?\nIs it considered a bad practice and should I pass all these data using XMLHTTPRequests when I am passing them to Javascript?\n", "ou can use the Makos templating language to generate the HTML for you. \n", "Ajax is the \"right\" way to do this.\nIn order to insert the values into your separate Javascript file dynamically, it can no longer be served as a static file. Beyond that, it adds an extra layer of problems with security and maintainability as you have to deal with string escaping, possible script injection, and having Mako syntax in your Javascript. Not to mention losing the ability to host your .js files on a CDN or server configured for static files.\n", 0.8578932285308838, 0.833476722240448, 0.8455086946487427, 0.12765957446808512, 1.3545058019712e-231, 72], ["Disclaimer: I'm still not sure I understand fully what setup.py does.\nFrom what I understand, using a setup.py file is convenient for packages that need to be compiled or to notify Disutils that the package has been installed and can be used in another program. setup.py is thus great for libraries or modules.\nBut what about super simple packages that only has a foo.py file to be run? Is a setup.py file making packaging for Linux repository easier?\n", " The setup() function is used to create a package. It is not used for building the packages. The only thing that setup does is to call the setup function for the given package, and then call setup for all the other packages in the same directory. This is the only way that a Python package can get installed. If you want to install a module, you have to use setup(['mymodule']) or setup([]) to do that. \n", "Using a setup.py script is only useful if:\n\nYour code is a C extension, and then depends on platform-specific features that you really don't want to define manually.\nYour code is pure Python but depends on other modules, in which case dependencies may be resolved automatically.\n\nFor a single file or a few set of files that don't rely on anything else, writing one is not worth the hassle. As a side note, your code is likely to be more attractive to people if trying it up doesn't require a complex setup. \"Installing\" it is then just about copying a directory or a single file in one's project directory.\n", 0.8533350825309753, 0.8330719470977783, 0.8430817723274231, 0.3036649214659686, 9.43001713226548e-232, 404], ["I'm making a time series monitoring program.\n\nI'd like to change the color of a plot starting at half the range of the x-axis.\nFor a 100 x 20 plot widget I would like to change last 50 data points to another color.\nHow can I draw a custom vertical grid whenever every xx items of data are passed?\n\n", " You can use the \"plot\" method to draw the grid.  You'll need to call the method with the same x and y coordinates as the data you're plotting.\n\n", "For (1), you will have to break your data into two separate lines, and assign the colors individually. PyQtGraph does not yet support multiple colors per line.\nFor (2), consider using pg.InfiniteLine or pg.VTickGroup. \n", 0.86973637342453, 0.8441414833068848, 0.8567478060722351, 0.1515151515151515, 1.1238831154513378e-231, 145], ["I am learning the web framework Tornado. During the study of this framework, I found the class  tornado.httpserver.HTTPserver. I know how to create a constructor of this class and create instance tornado.httpserver.HTTPserver in main() function. But this class tornado.httpserver.HTTPserver  has 4 methods. I have not found how to use these methods.\n1) def close_all_connections(self):\n2)  def handle_stream(self, stream, address):\n3)  def start_request(self, server_conn, request_conn):\n4)  def on_close(self, server_conn):\nI know that 2-4 methods are inherited from the class tornado.tcpserver.TCPServer\nCan someone illustrate how to use these methods of a class tornado.httpserver.HTTPserver?\n", " Tornado is a web server. It has a lot of methods to handle the request and response. You can find the documentation of the methods in the tornado source code. \n", "These methods are used internally; you shouldn't call them yourself.  \n", 0.8670039176940918, 0.8733793497085571, 0.8701799511909485, 0.0975609756097561, 1.0678008316969705e-231, 161], ["This seems to be a dumb question, but how do I ssh into the kivy-remote-shell?\nI'm trying to use buildozer and seem to be able to get the application built and deployed with the command, buildozer -v android debug deploy run, which ends with the application being pushed, and displayed on my android phone, connected via USB.\nHowever, when I try ssh -p8000 admin@127.0.0.1 from a terminal on the ubuntu machine I pushed the app from I get Connection Refused.\nIt seems to me that there should be a process on the host (ubuntu) machine in order to proxy the connection, or maybe I just don't see how this works?\nAm I missing something simple, or do I need to dig in a debug a bit more?\n", " You need a ssh server on your host machine. You can use sshfs to mount a remote ssh directory on a host. Then you can ssh to the remote machine and run the build.sh script. \n", "When the app is running, the GUI will tell you what IP address and port to connect to.\n", 0.8626614212989807, 0.8778694272041321, 0.8701989650726318, 0.2222222222222222, 1.0317320877054778e-231, 175], ["This seems to be a dumb question, but how do I ssh into the kivy-remote-shell?\nI'm trying to use buildozer and seem to be able to get the application built and deployed with the command, buildozer -v android debug deploy run, which ends with the application being pushed, and displayed on my android phone, connected via USB.\nHowever, when I try ssh -p8000 admin@127.0.0.1 from a terminal on the ubuntu machine I pushed the app from I get Connection Refused.\nIt seems to me that there should be a process on the host (ubuntu) machine in order to proxy the connection, or maybe I just don't see how this works?\nAm I missing something simple, or do I need to dig in a debug a bit more?\n", " You need a ssh server on your host machine. You can use sshfs to mount a remote ssh directory on a host. Then you can ssh to the remote machine and run the build.sh script. \n", "\n127.0.0.1\n\nThis indicates something has gone wrong - 127.0.0.1 is a standard loopback address that simply refers to localhost, i.e. it's trying to ssh into your current computer.\nIf this is the ip address suggested by kivy-remote-shell then there must be some other problem, though I don't know what - does it work on another device?\n", 0.8668771982192993, 0.8170783519744873, 0.8412414789199829, 0.17821782178217824, 1.0969346883576932e-231, 175], ["I have a word, according to that i want to find out whether the text is related to that word or not using python and nltk is it possible ?\nFor example I have a word called \"phosphorous\". I would like to find out that the particular text file is related to this word or not?\nI cant use bag of words in nltk as I have only one word and no training data.\nAny Suggestions?\nThanks in Advance.\n", "You can use ngrams to get a list of all the words that are in the corpus. Then you can check if the word is in that list. If it is, then you know that it's in your corpus, otherwise you don't. \n", "Not without a corpus, no.\nLook at it this way: can you, an intelligent being, tell whether \u5149 is related to \u90e8\u5c4b\u306b\u5165\u3063\u305f\u6642\u96fb\u6c17\u3092\u3064\u3051\u307e\u3057\u305f without asking someone or something that actually knows Japanese (assuming you don't know Japanese; if you do, try with \"svjetlo\" and \"Kad je u\u0161ao u sobu, upalio je lampu\"). If you can't, how do you expect a computer to do it?\nAnd another experiment - can you, an intelligent being, give me the algorithm by which you can teach a non-english-speaking person that \"light\" is related to \"When he entered the room, he turned on the lamp\"? Again, no.\ntl;dr: You need training data, unless you significantly restrict the meaning of \"related\" (to \"contains\", for example).\n", 0.8525523543357849, 0.7817071676254272, 0.8155941963195801, 0.2909090909090909, 1.1127531493628226e-231, 194], ["I am running some K-Means clustering from the sklearn package.\nAlthough I am setting the parameter n_jobs = 1 as indicated in the sklearn documentation, and  although a single process is running, that process will apparently consume all the CPUs on my machine. That is, in top, I can see the python job is using, say 400% on a 4 core machine.\nTo be clear, if I set n_jobs = 2, say, then I get two python instances running, but each one uses 200% CPU, again consuming all 4 of my machine's cores.\nI believe the issue may be parallelization at the level of NumPy/SciPy.\nIs there a way to verify my assumption? Is there a way to turn off any parallelization in NumPy/SciPy, for example?\n", " You can check the number of cores on your machine with:\nprint('Number of CPUs: {}'.format(numpy.get_config()['NumPy']['num_processors']))\n", "Indeed BLAS, or in my case OpenBLAS, was performing the parallelization.\nThe solution was to set the environment variable OMP_NUM_THREADS to 1.\nThen all is right with the world.\n", 0.8147923350334167, 0.8486528396606445, 0.8313779234886169, 0.11320754716981131, 1.1864231866578189e-231, 139], ["I have installed the latest versions of both django and python. The default \"python\" command is set to 2.7; if I want to use python 3, I have to type \"python3\". \nHaving to type \"python3\" and a django command causes problems. For example if I type: \"python3 manage.py migrate\" , I get an error. The error is:\nTraceback (most recent call last):\n  File \"manage.py\", line 8, in \n    from django.core.management import execute_from_command_line\nImportError: No module named 'django'\nDjango does not seem to recognize my python 3. How do I get around this? Your help is greatly appreciated.\n", "You need to install the python3 module.  You can do this by typing: sudo easy_install python-3.2.1\n", "You need to install django for python 3, pip3 install django\n", 0.876331090927124, 0.907393753528595, 0.8915919661521912, 0.45161290322580644, 1.1896457329133973e-231, 99], ["I have a Python script running as a daemon. At startup, it spawns 5 processes, each of which connects to a Postgres database. Now, in order to reduce the number of DB connections (which will eventually become really large), I am trying to find a way of sharing a single connection across multiple processes. And for this purpose I am looking at the multiprocessing.sharedctypes.Value API. However, I am not sure how I can pass a psycopg2.connection object using this API across processes. Can anyone tell me how it might be done?\nI'm also open to other ideas in order to solve this issue.\nThe reason why I did not consider passing the connection as part of the constructor to the 5 processes is mutual exclusion handling. I am not sure how I can prevent more than one process from accessing the connection if I follow this approach. Can someone tell me if this is the right thing to do?\n", " You can use the psyco connection object to share a connection between processes:\nimport psycotest\nfrom multiput import Pool\npool = Pool(5)\nconnection = psycocontrol.Connection('pg_hba.conf')\nfor i in range(pool.get_active()):\n   connection.connect()\n", "You can't sanely share a DB connection across processes like that. You can sort-of share a connection between threads, but only if you make sure the connection is only used by one thread at a time. That won't work between processes because there's client-side state for the connection stored in the client's address space.\nIf you need large numbers of concurrent workers, but they're not using the DB all the time, you should have a group of database worker processes that handle all database access and exchange data with your other worker processes. Each database worker process has a DB connection. The other processes only talk to the database via your database workers.\nPython's multiprocessing queues, fifos, etc offer appropriate messaging features for that.\n", 0.7908638715744019, 0.8261817693710327, 0.8081371188163757, 0.1775147928994083, 1.0335541382244337e-231, 251], ["I have a log file by the name log.json.\nA simple insert in rethinkdb works perfectly.\nNow this json file get updated every second, how to make sure that the rethinkdb gets the new data automatically, is there a way to achieve this, or i have to simply use the API and insert into db as well as log in a file if i want to.\nThanks.\n", " You can use a cron job to run a script every minute that inserts the data into the database. \n", "The process that appends new entries in your json file should probably run query to insert the same entries in RethinkDB.\nOr you can have a cron job that\n\nget the last entry saved from rethinkdb\nread your json file for new entries\ninsert new entries\n\n", 0.9138307571411133, 0.856163740158081, 0.8840578198432922, 0.3076923076923077, 1.2492007776652912e-231, 95], ["When creating a TextItem to be added to a plotItem in PyQtGraph, I know it is possible to format the text using html code, however I was wondering how to format the text (i.e. change font size) when updating the text through TextItem.setText()? or do I need to destroy/re-create a TextItem? \n", " You can use the setText method to change the font of the Text Item.\n", "It is not documented, but the method you want is TextItem.setHtml().\n", 0.9219481945037842, 0.9008561372756958, 0.9112801551818848, 0.23076923076923075, 1.2835366713538165e-231, 69], ["I have an FTP server that hosts data files, where the date that the data is associated with is encoded into the file names. I want to write a process that can find and download all the files associated with a particular date. The complication is that different files use different encodings. (Unfortunately changing/standardising the names isn't an option.) The year can be four digits or two. The month can be two digits or three letters. Sometimes the day is represented, and the substring can be anywhere in the string.\nAt the moment, I'm creating a list of all the files on the server, then using a regular expression to determine which files are relevant, and then downloading those files.\nIs it possible to condense the first two steps? That is, is there a way to get the server to return the list of files that match the expression?\nI'm using the Python ftplib if that makes any difference.\n", " If you're using Python, you can use the re module to match a string against a regex.  For example, if you have a file named \"foo.txt\" and you want it to be matched against the regex \"^[0-9][0][1-5][a-z][A-Z][\\w][-][[:alnum:][:digit:]]{2,4}$\", you could do this:\nimport re\nre.match(r'^([0]{4})\\.txt$', 'foo')\n", "It should be fairly simple to use LIST, MLSD and NSLT to build a local index of the files on the FTP, and then use regex to filter unwanted files from the index, and then use the remainder in a batch script to download them.\n", 0.7743515968322754, 0.8507424592971802, 0.8107515573501587, 0.20000000000000004, 9.99558488240908e-232, 309], ["Can you guys help me find Qt designer app on mac. I installed anaconda package and conda reports that qt, sip, and pyqt are installed. Still I couldn't find the designer app in any of the folders. My python app that uses pyqt works perfectly. I'm very new to macs and probably missing something very simple.\nI did search folder tree for anything named designer. I found QtDesigner.so (supposed to be executable?) at\n/Users/XXXX/anaconda/pkgs/pyqt-4.10.4-py27_0/lib/python2.7/site-packages/PyQt4 \nbut it won't even run saying \"cannot execute binary file\"\nanaconda/bin doesn't have it\nthere's a folder anaconda/include/QtDesigner but noting I can run\n/anaconda/pkgs/qt-4.8.5-3/bin - no designer\nI'm totally confused now.\n", " I've found the answer to my own question. It's in the anconda folder. You'll find a Qt Designer executable in there. The problem is that the Qt installer doesn\u2019t know how to run it. This is because the installer is not able to find it in your PATH. To fix this, you need to add the path to the executable to your system PATH, or add it to a system-wide environment variable. For example, if you want to use the Designer in a Python environment, add this line to /etc/profile.d/env.sh:\nPATH=/usr/local/opt/qmake/3.6.1/tools/:/usr\n", "OSX Yosemite 10.10.5\nQt 5.6\nQtCreator 3.6.1\nQtDesigner is part of my QtCreator.  To use QtDesigner:\n\nLaunch QtCreator, and from the menu bar (outside QtCreator), click on:\nFile>New File or Project \nYou will be presented with a New File or Project dialog window.  In the Files And Classes section, select Qt.  In the middle pane, select QtDesigner Form.  Then click on the Choose button in the lower right corner.\nYou will be presented with a QtDesigner Form dialog window.  Then you can select Main Window or Dialog with Buttons Bottom, etc. Then click on the Continue button in the lower right corner.\nIn the Location dialog window, use a name like mainwindow1.ui, and for the path you might want to step aside and create a directory called forms, e.g. $ mkdir /Users/7stud/qt_projects/forms, then enter that as the path.\nEnter any other details and click on Done.   That will land you in QtCreator with the Design button selected (which I guess means you are in QtDesigner), and you will be able to drag and drop widgets onto your window.\nTo convert the .ui file to a .py file that you can import into your python program:\n$ pyuic5 mainwindow1.ui -o mainwindow1.py\n-o => output file (default is stdout)\n\nThat command converts the .ui file mainwindow1.ui to a .py file named mainwindow1.py.\nTo re-open the file: File>Open File or Project.  If you select a file with a .ui extension, it will be opened with QtCreator's Design button pre-selected, i.e. you will be inside QtDesigner.\n", 0.8385332226753235, 0.8014914989471436, 0.8195940852165222, 0.25857519788918204, 9.779181919777936e-232, 530], ["I understand that I should use os.urandom() or SystemRandom in Python for 'secure' pseudo-random numbers.\nBut how does Python generate these random numbers logically? \nAlso is there a way to generate a number in Python that is 'more random' than others?\n", "he random module is a cryptographic library that provides a set of functions for generating random data.  It is not a random number generator. It does not use the same seed for each call. The seed is used to seed the random generator, and the seed can be changed. This is the only way that the generator can ever be truly random. If you want a truly secure random, you need to use a cryptographically secure seed. There are many ways to do this, but the most common is to make a secure cryptographic key, which is what the cryptographic module does. Then you can use that key to encrypt the data, then decrypt it with the key. You can also use it to decrypt the encrypted data with a key that you have. In this way, the encryption and decryption are both secure. However, this is only secure if the keys are secure, so you will need a good cryptographic algorithm to be able to create the secure keys. Cryptographic keys can only be created with good algorithms, not with randomness. So, if you are using a bad cryptographic method, it will not be secure at all. For example, a simple method to get a seed would be to just use an old-fashioned cryptosystem, such as the DES algorithm. DES is very good, however, because it is cryptologically secure and has a very small key size. A good method would use something like SHA-1, or SHA2, to produce a large key and then use this key as a starting point for a new key generation. SHA1 and SHA256 are good choices, as are the other algorithms that are used in the standard library. These algorithms are cryptically secure because they use very little key space, making them very fast. They are also very easy to implement, with no need for complex algorithms. Thus, they are very secure in practice. Note that this does NOT mean that a DES key is good. Rather, DES keys have a small size, are easy for the user to remember, have good security properties, etc. Also, note that DES has been broken, in a manner that has not been discovered yet. Therefore, there is no guarantee that it can never be broken. Additionally, some DES implementations have been found to have weaknesses that make them vulnerable to cryptanalysis. Such weaknesses are not known to the public, yet, for some implementations. Other DES weaknesses have not yet been detected. As a result, while DES may be good in theory, its implementation is known not to work as well as it should. Another good DES implementation, called Triple DES, has yet to become public. TripleDES is also known for its", "For \"secure\" random numbers, Python doesn't actually generate them:  it gets them from the operating system, which has a special driver that gathers entropy from various real-world sources, such as variations in timing between keystrokes and disk seeks.\n", 0.7938783764839172, 0.8337926864624023, 0.8133461475372314, 0.06048387096774194, 5.867199211185472e-232, 2510], ["I've been learning Python using version 3.4. I recently started learning Web.py so have been using Python 2.7 for that, since web.py not supported in Python 3.4.  I have nose 1.3.4 module installed for both Python 3.4 and 2.7.  I need to run the nosetests command on some Python code written in 2.7 that uses the Web.py module.  However, when I type nosetests command it automatically uses Python 3.4, so is throwing an error as unable to import the Web.py module in my Python code.  Is there a way to force nosetests to use Python 2.7?  Macbook Pro running OS X Yosemite.\n", " You can use the --use-old-modules option to nosycheck. This will use a different module for each Python version. For example, if you have a module named mymodule.mymodule in both 2 and 3, you can run nosetsests mymod.mysubmodule -- Use the old-module option if the module is not in the current Python. If you want to be sure that the tests are run in a specific Python, use --oldmodule=2.6.5. You will need the Python module name in your command line. The --module-name option is for the command-line. It is used to specify the name of the python module to load. In this case, it is my module, not my.module, and it will be used for all Python versions. --test-only is the same as --only-test. Use this if your tests only need a subset of your code, but not all of it. Also, the test-cases are not run if there is a failure. So, for example:\nnosetest mytest -- --modules=mymod,mysubmodule\n", "Try python2.7 -m nose instead of running nosetests. \u2013  dano 38 mins ago\n", 0.7860164642333984, 0.8046603202819824, 0.7952291369438171, 0.010810810810810811, 7.22381295112861e-232, 890], ["After making changes in a course on the Teacher side, there are options to preview changes and view live version but currently both options are not working properly and we have to append base url (www.abc.com/) in URL every time to preview changes or view live version.\nWe are using edx open code for providing open(free) courses for users from all over the world.\nBut we are stuck here, please assist us so we can move forward.\nThanks in Advance, \nAnjum\n", " I think you need to add a new course to the course list.  Then you can add the new one to your course.html.erb file. Then in your browser, you should be able to see the changes. You can also use the \"live\" version of the site. Just make sure you have the correct course ID in the URL. Also, make a note of course name and course URL in course_list.rb. I'm not sure if this is the right way to do it, but it works for me. Hope this helps.", "if you are in development, ie using settings=devstack ,\ncheck cms/envs/devstack.py \nwhether LMS_BASE = \"your lms url\" normally LMS_BASE = \"localhost:8000\"\nif you are in production, \ncheck /edx/app/edxapp/cms.env.json and put LMS_BASE = \"your lms url\"\n", 0.8258283138275146, 0.8076866865158081, 0.816656768321991, 0.1438848920863309, 9.160713561418769e-232, 438], ["So I want to implement a simple comms protocol where reads & writes are completely asynchronous. That means that client sends some data and then server may or may not respond with an answer. So I can't just call reader.read() because that blocks until at least something is returned. And I may have something more to send in the mean time.\nSo is there a way to check if reader has something to read? \n(please note that I'm talking specifically about the streams version: I'm fully aware that protocols version has separate handlers for reading and writing and does not suffer from this issue)\n", " You can use a callback to signal when the data is available.  The callback will be called when data arrives, and you can then check the callback's result to determine if the read operation is complete.\n\n", "There is no way to ask reader has incoming data or not.\nI guess to create asyncio.Task for reading data from asyncio stream reader in loop.\nIf you need to write data asynchronously feel free to call StreamWriter.write() from any task that have some outgoing data.\nI strongly dont recommend to use protocols directly -- they are low-level abstraction useful for flow control but for application code is better to use high-level streams.\n", 0.8689448237419128, 0.8383190631866455, 0.8533572554588318, 0.1592920353982301, 1.0885393138232049e-231, 204], ["I'm currently designing an application using the Twisted framework, and I've hit a bit of a roadblock in my planning.\nFormal Description of the Problem\nMy application has the following constraints:\n\nData arrive in-order, but asynchronously.  I cannot know when the next piece of my data will arrive\nThe order in which data arrive must be preserved throughout the lifespan of the application process.\nAdditional asynchronous operations must be mapped onto this \"stream\" of data.\n\nThe description of my problem may remind people of the Functional Reactive Programming (FRP) paradigm, and that's a fair comparison.  In fact, I think my problem is well-described in those terms and my question can be pretty accurately summarized thusly:  \"How can I leverage Twisted in such a way as to reason in terms of data streams?\"\nMore concretely, this is what I have figured out:\n\nA datum arrives and is unpacked into an instance of a custom class, henceforth referred to as \"datum instance\"\nThe newly-arrived datum instance is appended to a collections.deque object, encapsulated by a custom Stream class.\nThe Stream class exposes methods such as Stream.map that apply non-blocking computations asynchronously to:\n\nAll elements already present in the Stream instance's deque.\nAll future elements, as they arrive.\n\nResults of the operations performed in item 3 are appended to a new Stream object.  This is because it's important to preserve the original data, as it will often be necessary to map several callable's to a given stream.\n\nAt the risk of beating a dead horse, I want to insist upon the fact that the computations being mapped to a Stream instance are expected to return instances of Deferred.\nThe Question\nIncidentally, this precisely where I'm stuck:  I can implement items 1, 2 & 3 quite trivially, but I'm struggling with how to handle populating the results Stream.  The difficulty stems from the fact that I have no guarantees of stream length, so it's completely possible for data to arrive while I'm waiting for some asynchronous operations to complete.  It's also entirely possible for async operation Oi to complete after Oi + n, so I can't just add deque.append as a callback.\nSo how should I approach this problem?  Is there some nifty, hidden feature of Twisted I have yet to discover?  Do any twisty-fingered developers have any ideas or patterns I could apply?\n", " Twisted is a great framework for doing this sort of thing. It has a lot of built-in support for this, such that you can do things like:\n\n   def get_data(self, data):\n ...\n def map(f, *args, **kwargs):...\n\n def stream(", "I don't know of any neat tricks that will help you with this.  I think you probably just have to implement the re-ordering (or order-maintaining, depending on how you look at it) logic in your Stream.map implementation.\nIf operation i + 1 completes before operation i then Stream.map will probably just have to hold on to that result until operation i completes.  Then it can add results i and i + 1 to the output Stream.\nThis suggests you may also want to support back-pressure on your input.  The re-ordering requirement means you have an extra buffer in your application.  You don't want to allow that buffer to grow without bounds so when it reaches some maximum size you probably want to tell whoever is sending you inputs that you can't keep up and they should back off.  The IProducer and IConsumer interfaces in Twisted are the standard way to do this now (though something called \"tubes\" has been in development for a while to replace these interfaces with something easier to use - but I won't suggest that you should hold your breath on that).\n", 0.820471465587616, 0.816149115562439, 0.8183045983314514, 0.14592274678111586, 1.1096206078226783e-231, 218], ["To achieve sense of nativeness in my Pyinstaller packaged Kivy Python application, I would like to create menu item in OSX menu bar. If using PyQt, I can easily achieve this. But as I am using Kivy, it seems like I have to resort to other ways.\nAny suggestion? I guess it will be in the range of PyObjc or AppleScript. Not sure though.\n", " I think you're going to have a hard time getting this to work. \nThe way I've done it is to use a Python script that calls the menu command. This script is then run by the OS. The menu commands are then called by a menu script. I'm not sure if this is the best way, but it works for me. You can also use the command line to call the script, and then you can call it from the main menu. It's not perfect, because you have no way to know if the user has already run the Python command, so you'll have some problems if you try to do this from within the application. However, this works well enough for my purposes. Also, you could use an external script to run this, if that's what you want. Just make sure that the external Python is called from your main application script and that it's called with the same arguments as the original Python. That way you won't have problems with arguments. Of course, that will make it a lot more complicated. Anyway, the point is that you will have the ability to execute the commands from a script in your application without having to mess with menu items. And you don't need to worry about the arguments, since the argument is passed to the python script as a string. So you just need a way for the app to get the string from somewhere. In my case, a command prompt. There are many ways to accomplish this in Python, depending on what your app needs. For example, there's a library called pygtk that allows you to write a GUI in python. Then you use that library to make a window that looks like a pyqt widget. When you run your program, pygtk will call your command and pass the window to it. Your command will then call a function in that window, which will execute a python command that does whatever you need. Once you've got the function, just call that function from inside your code. Another way is using the Gtk.Main class. Gdk.gdk_main will allow you call GMain.run_command() from python, passing the name of the program you'd like the GUI to be called. Again, all you do is call run_program() with a name that matches the one you gave to GgdK.main. Run your script with Gdks.Run. From there, call Run.py from GRunMain, where you get a reference to your", "\nIf using PyQt, I can easily achieve this. But as I am using Kivy, it\n  seems like I have to resort to other ways\n\nI don't see why you can't just use the PyQt method. Create a separate file that runs the icon that is called by your main Kivy app.\n Like PyQt I am sure that Kivy has an exit function that you could override and have it kill the icon thread on exiting the application. \nOnly potential downside (that I can think of off the top of my head) is that it might run slower than implementing in PyObjC.\n", 0.8180855512619019, 0.839777946472168, 0.8287898898124695, 0.22761194029850745, 6.469494142759201e-232, 2201], ["I need a way to serialize and unserialize dates that are potentially far away in the past, for instance -10000\nI first look at ISO8601, but it does not seem to support years with more than four digits. (Or at least, python libraries I tried don't.)\nThe different solutions I can think of:\n\nchange the year before serializing/deserializing, give it to the parsing/formatting library, and fix it back (sounds hacky)\ndefine my own format, like year:month:day:hour:minute:second (that is reinventing the wheel, since I have to handle timezones, etc.)\nUse a UNIX timestamp without bounds or something equivalent (may overflow in some programming languages, and still the timezone stuff)\nStore dates before -9999 (or 0) differently than those after, since there was no timezone issue/leap years/\u2026 issue at that time. (two different formats at the same place)\n\nDo you see any other way that would be better than these ones? Or recommand one of those?\n", " If you want to be able to parse dates in a format that is not a date, you can use the strptime() function.\n", "You could take a page from the astronomy people. Sky maps they account for long period precession of Earth's spin by establishing epochs. (The sky is different if you're looking now vs 10,000 BC.)\nCreate a new class that has an \"epoch\" number and a facade pattern of your current date class. The new class contains two private fields for epoch and internal-date. Your constructor sets epoch to (year div 10000), and instantiates the internal-date with (year modulo 10000). I hope rest of the facade pattern is as obvious as I think.\n", 0.854353129863739, 0.8134670853614807, 0.8334089517593384, 0.16666666666666666, 1.2761323602556544e-231, 108], ["I'm looking for an implementation of n-grams count vectorization that is more efficient than scikit-learn's CountVectorizer.  I've identified the CountVectorizer.transform() call as a huge bottleneck in a bit of software, and can dramatically increase model throughput if we're able to make this part of the pipeline more efficient.  Fit time is not important, we're only concerned with transform time.  The end output must be a scipy.sparse vector.  If anyone has any leads for potential alternatives it would be much appreciated.\n", " The problem is that the vectorized version of a ngram model is a sparse matrix, so it's not really a vector, but a matrix. The sparse version is much faster, because it avoids the matrix multiplication. However, the sparse vector is still a dense matrix and it takes a lot of time to convert it to a simple sparse array. So, if you want to speed up the conversion, you can use the scicos.linalg.matrix.to_sse() function. This function takes the dense vector and converts it into a sse array, which is faster than the original dense array (but still not as fast as the numpy sparse arrays).\n", "Have you tried HashingVectorizer? It's slightly faster (up to 2X if I remember correctly). Next step is to profile the code, strip the features of CountVectorizer or HashingVectorizer that you don't use and rewrite the remaining part in optimized Cython code (after profiling again).\nVowpal Wabbit's bare-bone feature processing that uses the hashing trick by default might give you a hint of what is achievable.\n", 0.8282302618026733, 0.8286094665527344, 0.8284198045730591, 0.23333333333333334, 8.788180087707819e-232, 591], ["I am a non-programmer who started to learn Python. My Mac OS X Yosemite shipped with Python 2.7.6. I installed Python 3.4.2 too. If I use pip or easy_install in the terminal to install a package, how do I know which Python I installed the package in? It seems Python 3.4.2 shipped with pip and easy_install, but I think Python 2.7.6 may also have some version of pip or easy_install. I know my system can have both versions of Python, but can it have multiple versions of pip or easy_install?\n", " You can check the version installed by typing python -V.\n", "There's an easy way around it - use pip2 or pip2.7 or pip-2.7 for Python 2, and pip3 or pip3.4 or pip-3.4 for Python 3. Both version ship with easy_install, but Python 2 does not contain pip by default - you have to install it yourself.\n", 0.8640764355659485, 0.8307458162307739, 0.847083330154419, 0.12903225806451613, 1.4132082004247135e-231, 58], ["I had python2.6 on my linux box but installed python3.4 to use new modules. I installed it using sudo access. The new version was installed in /usr/local/bin. Without root access, I can use the new python3.4, both by just using python3.4 in the command line or using the shebang in the .py file #!/usr/local/bin/python3\nNow I am trying to install a module, for which I need sudo access. When I am root, and I run python3.4, it says command not found. I ran whereis python and found the path to python2.6 in /usr/bin, but whereis python3.4 as root gives, not found in /usr/bin, which is correct since it is in /usr/local/bin. Again, if I exit from root, I have no trouble using python3.4\nThis seems like a $PATH issue (not sure), can some one help me what I am doing wrong while installing the module for the new python3.4? I was able to install the module, but it was installed in the old python2.6 site-packages.\n", "You need to add the directory containing the python-modules to your $PYTHONPATH. This is done by adding the following line to /etc/environment:\nPREFIX=/usr\nPATH=/home/username/path/to/your/project/lib/site-package/\n", "How about export PATH+=:/usr/local/bin, try it, maybe helpful.\n", 0.8270911574363708, 0.8647722601890564, 0.8455120921134949, 0.0851063829787234, 1.0944305725481359e-231, 215], ["I had python2.6 on my linux box but installed python3.4 to use new modules. I installed it using sudo access. The new version was installed in /usr/local/bin. Without root access, I can use the new python3.4, both by just using python3.4 in the command line or using the shebang in the .py file #!/usr/local/bin/python3\nNow I am trying to install a module, for which I need sudo access. When I am root, and I run python3.4, it says command not found. I ran whereis python and found the path to python2.6 in /usr/bin, but whereis python3.4 as root gives, not found in /usr/bin, which is correct since it is in /usr/local/bin. Again, if I exit from root, I have no trouble using python3.4\nThis seems like a $PATH issue (not sure), can some one help me what I am doing wrong while installing the module for the new python3.4? I was able to install the module, but it was installed in the old python2.6 site-packages.\n", "You need to add the directory containing the python-modules to your $PYTHONPATH. This is done by adding the following line to /etc/environment:\nPREFIX=/usr\nPATH=/home/username/path/to/your/project/lib/site-package/\n", "Well you could have given the location to install Py 3.4 to be in /usr/bin. \nAn easy approach could be to copy the Py 3.4 bin to /usr/bin from /usr/local/bin.\nSecondly You can also install again with the prefix params.\n", 0.8262797594070435, 0.8514807224273682, 0.838690996170044, 0.24390243902439024, 1.0944305725481359e-231, 215], ["I'm using regex to find occurrences in a string python, for example: \"cw cx\", \"cw cx av\", \"cw cx dr wt\" or \"cw cx qw hv eb\". The match can be 2, 3, 4 or 5 pairs. I'm thinking something like this \"([a-zA-Z]{2}) ([a-zA-Z]{2})\" Please help me\n", " You can use a regex like:\n([\\w-]+)([\\d-]+)\n", "You are almost there. Try this: ([a-zA-Z]{2} ){1,4}[a-zA-Z]{2}\n", 0.8553943634033203, 0.8345513343811035, 0.8448443412780762, 0.16666666666666666, 1.4363404288308567e-231, 44], ["I'm sure this is a very newb question, so I apologize in advance. I'm trying to use ipython notebook for a group project. The program we are building is fairly large and pulls in a large number of external datasets. Much of the time, Ipython seems to stop working. I'll try to run a cell or multiple cells and nothing will happen (except a little asterisk * will appear in the brackets [] to the left of the cell). Even if I try to just add a new cell and execute 2+2, nothing will happen. What is going on here? How do I fix this? Thanks!\n", " The problem is that you're trying too hard to make it work. \nThe problem with ipy notebook is not that it doesn't work, but that the notebook itself is too complex.\nIf you want to do something, you have to write a lot of code. If you just want a simple notebook, then you can just use the ipynb notebook. But if you really want something more complex, it's not a good idea to try and make a notebook that works. You'll just end up with a mess. So, if it works, great, just keep doing it. Otherwise, make something else. And if that doesn\u2019t work then make another notebook and try again. It's a bit like trying and trying again to get a car to start. Eventually, the car will start, and you'll be able to drive it, or you won't. That's the way it goes. Sometimes, though, things just won\u2018t start at all. In that case, there's no point trying. Just keep trying until you get it to work!  \n\n", "The asterisk next to a cell [*] indicates that the cell is currently executing. While IPython provides each notebook with it's own kernel, there is only one kernel per notebook. When that kernel is busy executing code (either a cell, or a series of cells) it cannot accept or run any further code until what it is currently doing is finished. New executions sit in a queue, until the kernel is ready.\nIf you wait long enough after trying to execute 2+2 you should find that it will eventually execute (assuming your main code ever exits).\nThe solution to this depends on your code, and how long you're willing to wait to get the results. As a general rule try the following:\n\nUse a smaller data set to test the algorithm, then scale up gradually noting the increase in time. Is it going to be feasible with your full dataset?\nIs your algorithm reading/writing to the disk? Can you avoid it, or pre-load/post-save state?\nIs it possible to split your data into batches?\nIf your algorithm is batchable, can you parallelize it to make best use of your CPU?\n\nYou can interrupt the kernel, however this will not work if the execution is currently out of the kernel's hands e.g. in external C modules (a lot of numpy for example). In these cases you may need to restart completely.\n", 0.8374895453453064, 0.818737804889679, 0.8280074596405029, 0.3532219570405728, 7.93542840843609e-232, 889], ["I am using Ubuntu 14.04 and trying to run snoopy_auth which is a part of the snoopy-ng application I downloaded and installed from their GitHub.\nWhen running, I get an error that is documented on snoopy-ng's GitHub page, which says that it works using version 0.7.8.\nHow can I downgrade sqlalchemy to 0.7.8?\nThe error looks like:\nsnoopy_auth -l\n[+] Available drone accounts:\nTraceback (most recent call last):\n  File \"/usr/bin/snoopy_auth\", line 103, in \n    drones = auth_.manage_drone_account(\"foo\", \"list\")\n  File \"/usr/bin/snoopy_auth\", line 29, in manage_drone_account\n    self.db.create(self.drone_tbl_def )\n  File \"\", line 2, in create\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/deprecations.py\", line 106, in warned\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/interfaces.py\", line 859, in create\n    raise NotImplementedError()\nNotImplementedError\n", " You can't downgrade the version of sqlAlchemy.  You need to install the latest version. You'll need the sqlite3-dev package. Then you'll be able to downgrade to the 0 version, but you won't be using the new sql Alchemy. The new Alchemy will be used for the next release.\n\n", "To get passed this error I just simply ran the command:\nsudo easy_install \"SQLAlchemy==0.7.8\"\nThe virtual environments do seem like the preferred method though, so hopefully I don't run into any additional problems from downgrading system-wide.\n", 0.8575745820999146, 0.8369964361190796, 0.8471605181694031, 0.16842105263157897, 1.0575674736275367e-231, 273], ["I've been installing few Library/Toolkit for Python like NLTK, SciPy and NumPy on my Ubuntu. I would like to try to use Anaconda distribution though. Should I remove my existing libraries before installing Anaconda?\n", " If you're not going to be using the libraries, then you don't need to remove them.\n", "There is no need to remove your system Python. Anaconda sits alongside it. When it installs, it adds a line to your .bashrc that adds the Anaconda directory first in your PATH. This means that whenever you type python or ipython in the terminal, it will use the Anaconda Python (and the Anaconda Python will automatically use all the Anaconda Python libraries like numpy and scipy rather than the system ones). You should leave the system Python alone, as some system tools use it. The important points are:\n\nWhichever Python is first on your PATH is what gets used when you use Python in the terminal. If you create a conda environment with conda and use source activate it will put that environment first on the PATH. \nEach Python (Anaconda or the system) will use its own libraries and not look at the others (this is not true if you set the PYTHONPATH environment variable, but I recommend that you don't).\n\n", 0.8809579610824585, 0.8061477541923523, 0.841894268989563, 0.13333333333333333, 1.331960397810445e-231, 84], ["I have been trying to build a prediction model using a user\u2019s data. Model\u2019s input is documents\u2019 metadata (date published, title etc) and document label is that user\u2019s preference (like/dislike). I would like to ask some questions that I have come across hoping for some answers:\n\nThere are way more liked documents than disliked. I read somewhere that if somebody train\u2019s a model using way more inputs of one label than the other this affects the performance in a bad way (model tends to classify everything to the label/outcome that has the majority of inputs\nIs there possible to have input to a ML algorithm e.g logistic regression be hybrid in terms of numbers and words and how that could be done, sth like:\ninput = [18,23,1,0,\u2019cryptography\u2019] with  label = [\u2018Like\u2019]\nAlso can we use a vector ( that represents a word, using tfidf etc) as an input feature (e.g. 50-dimensions vector) ?\nIn order to construct a prediction model using textual data the only way to do so is by deriving a dictionary out of every word mentioned in our documents and then construct a binary input that will dictate if a term is mentioned or not? Using such a version though we lose the weight of the term in the collection right?\nCan we use something as a word2vec vector as a single input in a supervised learning model?\n\nThank you for your time.\n", "1. There are ways to train a classifier using more than one input. For example, you can train your classifiers using different numbers of features. You can also train classifications using multiple class labels. \n2. Yes, a linear model can be used to predict a label given a set of input features, but it is not the best way. A linear class model is a special case of a nonlinear model. Nonlinear models are better suited for classification problems. The best linear models use the same number of parameters for each class. This is because the linear classification models can only be trained using the data that is already in your training set. If you want to use your data to make predictions, then you need to learn a different model that uses different parameters. In this case, the model will be non-linear. Learning a new model for a classification problem is called transfer learning. It is the process of using your knowledge of your dataset to help you learn new models. To learn transfer models, it's important to know what your problem domain is. Then you will know which models to try. Once you have a good model, use it to solve your problems, and you'll be able to transfer your model to new problems that you haven't seen before. Models that are transferable are called black box models because they don't have", "\nYou either need to under-sample the bigger class (take a small random sample to match the size of the smaller class), over-sample the smaller class (bootstrap sample), or use an algorithm that supports unbalanced data - and for that you'll need to read the documentation.\nYou need to turn your words into a word vector. Columns are all the unique words in your corpus. Rows are the documents. Cell values are one of: whether the word appears in the document, the number of times it appears, the relative frequency of its appearance, or its TFIDF score. You can then have these columns along with your other non-word columns.\n\nNow you probably have more columns than rows, meaning you'll get a singularity with matrix-based algorithms, in which case you need something like SVM or Naive Bayes.\n", 0.8243799209594727, 0.8271468877792358, 0.825761079788208, 0.29551451187335087, 7.344654400628587e-232, 1325], ["When I use R and execute a command in the command window, I can just press up and I get the last executed command in the current command line. What key do I need to press in order to get the same in Python? \nEdit: When I asked this question, I did not know what IDLE or iPython is. I just googled \"download python\" and downloaded it. But now I know I am using IDLE.\n", " I'm not sure what you're trying to do, but I think you'll find that the key you need is Ctrl+Shift+P.  This will bring up the Python prompt, and you can type in whatever you want.\n\n", "If you're in IDLE, use Alt+P and Alt+N to navigate your command history.\n", 0.8664767742156982, 0.885266900062561, 0.8757710456848145, 0.21428571428571425, 1.1200407237786664e-231, 182], ["When I use R and execute a command in the command window, I can just press up and I get the last executed command in the current command line. What key do I need to press in order to get the same in Python? \nEdit: When I asked this question, I did not know what IDLE or iPython is. I just googled \"download python\" and downloaded it. But now I know I am using IDLE.\n", " I'm not sure what you're trying to do, but I think you'll find that the key you need is Ctrl+Shift+P.  This will bring up the Python prompt, and you can type in whatever you want.\n\n", "For anyone else who comes upon this looking for a mac solution, I found that Contrl+Option+n and Contrl+Option+n worked to cycle through previously executed lines in IDLE (Python) 2.7.10 on OS 10.8.5. \nI realize that this should work without the Option, but for months I needed to use both modifier keys to execute the shortcut properly. \n", 0.8733800649642944, 0.8387500643730164, 0.8557148575782776, 0.21153846153846156, 1.1703908019266405e-231, 182], ["The Question:\nGiven a sympy expression, is there an easy way to generate python code (in the end I want a .py or perhaps a .pyc file)?  I imagine this code would contain a function that is given any necessary inputs and returns the value of the expression.\nWhy\nI find myself pretty frequently needing to generate python code to compute something that is nasty to derive, such as the Jacobian matrix of a nasty nonlinear function.  \nI can use sympy to derive the expression for the nonlinear thing I want: very good.  What I then want is to generate python code from the resulting sympy expression, and save that python code to it's own module.  I've done this before, but I had to:\n\nCall str(sympyResult)\nDo custom things with regular expressions to get this string to look like valid python code\nwrite this python code to a file\n\nI note that sympy has code generation capabilities for several other languages, but not python.  Is there an easy way to get python code out of sympy?  \nI know of several possible but problematic ways around this problem:\n\nI know that I could just call evalf on the sympy expression and plug in the numbers I want. This has several unfortuante side effects:\n\ndependency: my code now depends on sympy to run.  This is bad.\nefficiency: sympy now must run every time I numerically evaluate: even if I pickle and unpickle the expression, I still need evalf every time.\n\nI also know that I could generate, say, C code and then wrap that code using a host of tools (python/C api, cython, weave, swig, etc...).  This, however, means that my code now depends on there being an appropriate C compiler.\n\nEdit: Summary\nIt seems that sympy.python, or possibly just str(expression) are what there is (see answer from smichr and comment from Oliver W.), and they work for simple scalar expressions.\nThat doesn't help much with things like Jacobians, but then it seems that sympy.printing.print_ccode chokes on matrices as well.  I suppose code that could handle the printing of matrices to another language would have to assume matrix support in the destination language, which for python would probably mean reliance on the presence of things like numpy.  It would be nice if such a way to generate numpy code existed, but it seems it does not.  \n", " think the best way is probably to use the python-symbolic library. It has a lot of functionality for generating python, c, python and c++ code. You can also use it to convert between python syntax and symbolic syntax. I have used it for", "If you don't mind having a SymPy dependency in your code itself, a better solution is to generate the SymPy expression in your code and use lambdify to evaluate it.  This will be much faster than using evalf, especially if you use numpy. \nYou could also look at using the printer in sympy.printing.lambdarepr directly, which is what lambdify uses to convert an expression into a lambda function. \n", 0.8555891513824463, 0.8421628475189209, 0.8488229513168335, 0.2456140350877193, 1.0680971220864064e-231, 237], ["In my app I need to know how big the canvas is in pixels.\nInstead calling canvas.size returns [100,100] no matter how many pixels the canvas is wide.\nCan you please tell me a way to get how many pixels the canvas is wide and high?\n", " You can get the width and height of the Canvas object by calling getBounds() on it. \n", "There is no position nor size in a Canvas. Canvas act just as a container for graphics instructions, like Fbo that draw within a Texture, so it have a size.\nIn Kivy, Canvas.size doesn't exists, but i guess you called your widget a canvas. By default, a Widget size is 100, 100. If you put it into a layout, the size will be changed, when the layout will known its own size. Mean, you need to listen to the changes of the Widget.size, or use a size you known, like Window.size.\n", 0.8625245690345764, 0.8342344164848328, 0.8481436371803284, 0.12612612612612611, 1.3241479764369575e-231, 86], ["I recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?\nOr is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?\nThanks.\n", " doc_vec is a preprocessing step that converts a document into a vector representation. It is not a part of the training process. \n", "Note that the \"DBOW\" (dm=0) training mode doesn't require or even create word-vectors as part of the training. It merely learns document vectors that are good at predicting each word in turn (much like the word2vec skip-gram mode). \n(Before gensim 0.12.0, there was the parameter train_words mentioned in another comment, which some documentation suggested will co-train words. However, I don't believe this ever actually worked. Starting in gensim 0.12.0, there is the parameter dbow_words, which works to skip-gram train words simultaneous with DBOW doc-vectors. Note that this makes training take longer \u2013 by a factor related to window. So if you don't need word-vectors, you may still leave this off.)\nIn the \"DM\" training method (dm=1), word-vectors are inherently trained during the process along with doc-vectors, and are likely to also affect the quality of the doc-vectors. It's theoretically possible to pre-initialize the word-vectors from prior data. But I don't know any strong theoretical or experimental reason to be confident this would improve the doc-vectors. \nOne fragmentary experiment I ran along these lines suggested the doc-vector training got off to a faster start \u2013 better predictive qualities after the first few passes \u2013 but this advantage faded with more passes. Whether you hold the word vectors constant or let them continue to adjust withe the new training is also likely an important consideration... but which choice is better may depend on your goals, dataset, and the quality/relevance of the preexisting word-vectors. \n(You could repeat my experiment with the intersect_word2vec_format() method available in gensim 0.12.0, and try different levels of making preloaded vectors resistant-to-new-training via the syn0_lockf values. But remember this is experimental territory: the basic doc2vec results don't rely on, or even necessarily improve with, reused word vectors.) \n", 0.8776551485061646, 0.7897877097129822, 0.8314063549041748, 0.08115942028985507, 1.1662621194838564e-231, 131], ["What is the significance of the body of the __iter__ method in a class definition in Python? \nIt is my understanding that an __iter__ method must return an iterator; then shouldn't every __iter__ method only contain a return statement which returns an iterator (defined elsewhere) ? Why is there a body in the __iter__ method in some cases?\n", "__getitem__ is a method that returns a reference to the next item in an iterable.  It is not a function that is called with a single argument.\n", "I don't quite get what you are meaning by body. I suppose you are talking about the definition of the method __iter__.\nWell sometimes you could use yield to return a generator. In this case, you could write the generating logic in the body of __iter__.\nOr returning another iterator might involve some logic. So it's not always simply a return.\n", 0.8673701286315918, 0.847227931022644, 0.8571807146072388, 0.17777777777777776, 1.1537355459208404e-231, 143], ["From what I read the dynamic and big data rendering capabilities of Bokeh are accessible by use of the bokeh-server.\nIn Bokeh documentation there is a brief mention of embedding the bokeh-server in a Flask application using the blueprint api. This option retains all of the Bokeh's dynamic and big data rendering capabilities.\nI would like to do the same for a django1.7 application.\nHas this been done? \nIf so are there any examples?\n", " I'm not sure if this has been implemented, but I've seen a similar approach used in the past. Basically, you create a new project in djangorestframework, and then you can use the djbokehserver to embed the server in your project.   This is the approach I use for my projects. I have a project that uses the Django Rest Framework, which uses a lot of Django's features, such as the admin, forms, etc. So I just use djbookehdserver to get the data I need. Then I can just import the models I want to use in my project, like models.py, models_generated.html, or models/myproject/models.xml. And then I import my models in models, mygenerated_models, in templates/generated/templates/project. Mygeneratedmodels is just a python file that contains all the generated models for the project I am working on. It's not a complete project because it doesn't contain all my djnago models (which are in another project), but it's a good starting point. You can then import your models from myproject in any project you want. The only thing you have to keep in mind is that you will have the ability to import models that are not in this project (such as models you've generated yourself), so you'll have some extra work to make sure you import all your djngo models correctly. But it works great for me. Hope this helps. :)\n", "No need to reinvent.\n\nbokeh-server is a webserver and can listen on arbitrary port.\nyou can have your django webserver listen on some other arbitrary port.\n... and integrate the two:\n\nhave redirects from django to bokeh-server, or\nwebserver in front, e.g. nginx which does reverse-proxy.\n\n\n", 0.8032103180885315, 0.8312195539474487, 0.816974937915802, 0.15753424657534248, 7.023306821658965e-232, 1313], ["Is there an editor that I could install on the Raspbian OS to practice Regex with? If not, what about through Python? If so, is there a good python IDE out there for Raspbian that supports Regexs?\n", " I'm not sure what you mean by \"Regex\" but if you want to learn regex, I recommend you to try the Python regex module. It's a very simple and easy to use module that allows you do to a lot of things with regex.\n", "Python itself supports regexes (via a built-in module). If you're just interested in playing around with them, import re in an interactive shell, and you have access to Python's regular expression engine.\n", 0.8673644065856934, 0.8720011711120605, 0.8696765899658203, 0.24691358024691357, 1.1092698505138592e-231, 211], ["On my desktop PC I have anaconda installed, and on my laptop - to save space - I thought i'd install miniconda and be selective about the modules I install. So I installed a handful, numpy, scipy etc. I didn't install anything which isn't part of the default anaconda install, but I just realized my miniconda install is taking up more space than the anaconda install! (1.8GB vs 2.2GB). (no environments in either)\nThe bulk of the difference comes from the pkgs folder. The miniconda install seems to have the tar.bz2 of all of the installed packages as well as the exploded versions. Are these safe to delete? Will they be deleted automatically after a while? Is there an option to not cache these?\nP.S. I'm developing on both windows and mac (i've tried installed anaconda and miniconda on both mac and windows to see, and I get very similar results). \n", " I've been using the same setup on a mac for a couple of months now.  I think the answer is that the packages are not being cached.\nI've found that if you install a package that is not part or all in the standard anconda package, it will be installed in a separate folder, which is what I found to be the case with numpydb. It's not in my standard path, so it's being installed as a standalone package. This is the only package I know of that has this behavior. If you want to know what the package is, you can just look at the folder where it is installed. For example, if I wanted to install ndlib, I would just type ndk-build ndbuild.exe nn.cpp nnp.h npyd.c npy.py nupy.pdb nuby_py_wrapper.zip nyb_python.egg.nupyd_2.4.1_1-py2_7.3.0.dmg.gz nydlib.a nypy2-2to2numpy.7z nynumpy-1to1numpy-0to0numba-3to3nubu-4to4numd-5to5nunu.bin nzlib-dev.6to6nzip-6.5.9-r1 nzip2d.lib nzone2zip.dll nzz2z.so nwlib2w.o nwx2wx.obj nx2x.x nxml2xml.cfg nws2ws.ini nwk2wk.xml nk2k.k3 nks2ks.conf nl2l.lisp nllib2ll.ll", "You can safely delete the tar.bz2 files. They are only used as a cache. The command conda clean -t will clean them automatically. \n", 0.7615379095077515, 0.8536336421966553, 0.8049601316452026, 0.07894736842105263, 7.251510683086493e-232, 996], ["Are conda's envs a wrapper around pyenv, a similar version or are they a distinct implementation?  They seem identical, but I do not know for sure.  Are they interchangeable?  \n", " They are not the same.\n", "Python virtualenvs (or pyvenvs) are very Python specific. The create an environment that pretends to be a distinct installation of Python, while reusing some data from the base Python. Only Python packages can be installed into a virtualenv, because that's really the only thing that makes sense. \nConda environments are not Python specific. Any kind of package can be a conda package (Python packages, C libraries, R packages, Python itself, ...), and any conda package can be installed into a conda environment. Each conda environment is essentially a completely separate installation of every file from every package (including all of Python itself). This is done efficiently using hard links. \n", 0.8796258568763733, 0.7898527383804321, 0.8323255181312561, 0.05263157894736843, 1.59216298109724e-231, 24], ["I am trying to use the timeit module to time the speed of an algorithm that analyzes data.\nThe problem is that I have to do run some setup code in order to run this algorithm. Specifically, I have to load some documents from a database, and turn it into matrix representation. \nThe timeit module does not seem to allow me to pass in the matrix object, and instead forces me to set this up all over again in the setup parameter. Unfortunately this means that the running time of my algorithm is fuzzed by the running time of pre-processing.\nIs there some way to pass in objects that were created already, to timeit in the setup parameter? Otherwise, how can I deal with situations where the setup code takes a nontrivial amount of time, and I don't want that to fuzz the code block I actually am trying to test?\nAm I approaching this the wrong way?\n", " You can use a function that returns a list of the matrices you want to measure. Then you can call time.timeit(matrix) to get the results. You could also use time(my_matrices) or time_series(myset) if you need a more complex result. If you just want the result, you could use my_result.\n\n", "The time it takes to run the setup code doesn't affect the timeit module's timing calculations.\nYou should be able to pass your matrix into the setup parameter using import, eg \n\"from __main__ import mymatrix\"\n", 0.8581600785255432, 0.8398691415786743, 0.8489160537719727, 0.1935483870967742, 9.986275395842682e-232, 288], ["I'm running Django with Postgres database. On top of application-level security checks, I'm considering adding database-level restrictions. E.g. the application code should only be able to INSERT into log tables, and not UPDATE or DELETE from them. \nI would manually create database user with appropriate grants for this. I would also need a more powerful user for running database migrations.\nMy question is, do people practice things like this? Any advice, best practices on using restricted database users with Django? \nEdit: To clarify, there's no technical problem, I'm just interested to hear other people's experiences and takeaways. One Django-specific thing is, I'll need at least two DB users: for normal operation and for running migrations. Where do I store credentials for the more privileged user? Maybe make manage.py migrate prompt for password? \nAs for the reasoning, suppose my app has a SQL injection vulnerability. With privileged user, the attacker can do things like drop all tables. With a more limited user there's slightly less damage potential and afterwards there's some evidence in insert-only log tables. \n", " The problem is that you're not really doing any kind of security at all. You're just trying to make it so that the database can't be used to do anything bad. This is a bad idea. The database is supposed to be a secure place to store data. If you want to prevent someone from doing bad things, you need to lock down the data in the first place. That's what the security models are for. They're there to protect you from bad people. And if you don't lock the bad guys out, then you'll be doing the wrong thing. So, if your goal is to keep bad actors out of your database, your security model is going to fail. It's not going be good. Your security is just a facade. A facade that's there for your own convenience. But it's still a security problem. There's a lot of ways to get around that. For example, a database that has no restrictions on what you can and can not do with it. Or a system that allows you to create a user that can only do certain things. These are all good ideas. However, they're all bad ideas, because they don\u2019t really protect against bad behavior. In fact, it\u2018s the opposite. Bad behavior is what makes the system fail in a real world. When you lock out bad users, bad data is still being stored. All you\u201cre doing is locking out the users that are doing something bad, but you are not locking them out from the rest of the world, so they can still do bad stuff. Basically, what your doing here is creating a false sense of safety. Creating a safety net that\u201ds there just to catch bad behaviour, when in fact you just created a", "Yes, this is practiced sometimes, but not commonly. The best way to do it is to grant specific privileges on user, not in django. Making such restrictions means that we should not trust application, because it might change some files / data in db in the way that we do not expect it to do so.\nSo, to sum up: create another user able to create / modify data and user another one with restrictions to use normally.\nIt's also quite common in companies to create one user to insert data and another one for employees / scripts to access it.\n", 0.8137855529785156, 0.8413180112838745, 0.827322781085968, 0.23244552058111378, 6.906896747403195e-232, 1549], ["I'm running Django with Postgres database. On top of application-level security checks, I'm considering adding database-level restrictions. E.g. the application code should only be able to INSERT into log tables, and not UPDATE or DELETE from them. \nI would manually create database user with appropriate grants for this. I would also need a more powerful user for running database migrations.\nMy question is, do people practice things like this? Any advice, best practices on using restricted database users with Django? \nEdit: To clarify, there's no technical problem, I'm just interested to hear other people's experiences and takeaways. One Django-specific thing is, I'll need at least two DB users: for normal operation and for running migrations. Where do I store credentials for the more privileged user? Maybe make manage.py migrate prompt for password? \nAs for the reasoning, suppose my app has a SQL injection vulnerability. With privileged user, the attacker can do things like drop all tables. With a more limited user there's slightly less damage potential and afterwards there's some evidence in insert-only log tables. \n", " The problem is that you're not really doing any kind of security at all. You're just trying to make it so that the database can't be used to do anything bad. This is a bad idea. The database is supposed to be a secure place to store data. If you want to prevent someone from doing bad things, you need to lock down the data in the first place. That's what the security models are for. They're there to protect you from bad people. And if you don't lock the bad guys out, then you'll be doing the wrong thing. So, if your goal is to keep bad actors out of your database, your security model is going to fail. It's not going be good. Your security is just a facade. A facade that's there for your own convenience. But it's still a security problem. There's a lot of ways to get around that. For example, a database that has no restrictions on what you can and can not do with it. Or a system that allows you to create a user that can only do certain things. These are all good ideas. However, they're all bad ideas, because they don\u2019t really protect against bad behavior. In fact, it\u2018s the opposite. Bad behavior is what makes the system fail in a real world. When you lock out bad users, bad data is still being stored. All you\u201cre doing is locking out the users that are doing something bad, but you are not locking them out from the rest of the world, so they can still do bad stuff. Basically, what your doing here is creating a false sense of safety. Creating a safety net that\u201ds there just to catch bad behaviour, when in fact you just created a", "For storing the credentials to the privileged user for management commands, when running manage.py you can use the --settings flag, which you would point to another settings file that has the other database credentials.\nExample migrate command using the new settings file:\npython manage.py migrate --settings=myapp.privileged_settings\n", 0.7933447360992432, 0.8107987642288208, 0.8019767999649048, 0.09863013698630137, 6.557507653018412e-232, 1549], ["Hi I am getting the error below when going to the website url on ubuntu server 14.10 running apache 2 with mod_wsgi and python on django.\nMy django application uses python 3.4 but it seems to be defaulting to python 2.7, I am unable to import image from PIL and AES from pycrypto.\n\nImportError at /  cannot import name _imaging Request\n  Method:   GET Request URL:    Django Version:   1.7.3\n  Exception Type:   ImportError Exception Value:    cannot import\n  name _imaging Exception\n  Location: /usr/local/lib/python3.4/dist-packages/PIL/Image.py in\n  , line 63 Python Executable: /usr/bin/python Python\n  Version:  2.7.6 Python Path:  ['/var/www/blabla', \n  '/usr/local/lib/python3.4/dist-packages', \n  '/usr/lib/python2.7', \n  '/usr/lib/python2.7/plat-x86_64-linux-gnu', \n  '/usr/lib/python2.7/lib-tk',  '/usr/lib/python2.7/lib-old', \n  '/usr/lib/python2.7/lib-dynload', \n  '/usr/local/lib/python2.7/dist-packages', \n  '/usr/lib/python2.7/dist-packages',  '/var/www/blabla', \n  '/usr/local/lib/python3.4/dist-packages']\n\n", "You need to install the python-imager package. It is a wrapper around the Pango library. You can install it with pip install -e.\n", "I believe that mod_wsgi is compiled against a specific version of python, so you need a py3.4 version of mod_wsgi. You may be able to get one from your os's package repository or you can build one without too much drama. From memory you'll need gcc and python-dev packages (python3-dev?) to build.\nOK, quick google, for ubuntu 14.10: sudo apt-get install libapache2-mod-wsgi-py3 should install a py3 version of mod_wsgi (will probably want to remove the existing py2 version).\nAdding a shebang line won't do any good as the python interpreter is already loaded before the wsgi.py script is read.\n", 0.8888211250305176, 0.8313665986061096, 0.8591343760490417, 0.18840579710144925, 1.2206867568780193e-231, 129], ["So I have this little problem. When I try to install PyGame for Python 3.4 I download a .whl (wheel?) file and don't know how to use it. Some guys told me something about pip but don't know how to use/install it.\n", "You can use pip to download the wheel file. \nFor example, if you have a folder named pygame, you can run the following command:\npython3.5 -m pip install pyGame wheel\n", "14 y/o? Good for you! You can put the file into your python/scripts folder and run pip install *file* (where *file* is your filename).\n", 0.8630359172821045, 0.8523136973381042, 0.8576412796974182, 0.3508771929824561, 1.1115097088810584e-231, 166], ["Many spreadsheets have formulas and formatting that Python tools for reading and writing Excel files cannot faithfully reproduce. That means that any file I want to create programmatically must be something I basically create from scratch, and then other Excel files (with the aforementioned sophistication) have to refer to that file (which creates a variety of other dependency issues).\nMy understanding of Excel file 'tabs' is that they're actually just a collection of XML files. Well, is it possible to use pandas (or one of the underlying read/write engines such as xlsxwriter or openpyxl to modify just one of the tabs, leaving other tabs (with more wicked stuff in there) intact?\nEDIT: I'll try to further articulate the problem with an example.\n\nExcel Sheet test.xlsx has four tabs (aka worksheets): Sheet1, Sheet2, Sheet3, Sheet4\nI read Sheet3 into a DataFrame (let's call it df) using pandas.read_excel()\nSheet1 and Sheet2 contain formulas, graphs, and various formatting that neither openpyxl nor xlrd can successfully parse, and Sheet4 contains other data. I don't want to touch those tabs at all.\nSheet2 actually has some references to cells on Sheet3\nI make some edits to df and now want to write it back to sheet3, leaving the other sheets untouched (and the references to it from other worksheets in the workbook intact)\n\nCan I do that and, if so, how?\n", " You can use the xsltproc module to do this.  It has a method called \"get_sheet\" that returns a list of all the worksheet names in a work book. Then you can just iterate through that list and call the method on each sheet. For example:\nimport xsl\nfrom xhtml import xmldoc\nworkbook = xmd.Book('test.xml')\nsheet_names = workbooks.get(workbooks[0]).get('sheet1').get(\"sheet2\").get()[1:]\nfor sheet_name in sheetnames: \n   sheet = sheet[sheetname]\n# get the sheet's data\ndata = get_worksheet(sheet)  # get a dataframe from the first sheet\nprint data\n\n", "I'm 90% confident the answer to \"can pandas do this\" is no.  Posting a negative is tough, because there always might be something clever that I've missed, but here's a case: \nPossible interface engines are xlrd/xlwt/xlutils, openpyxl, and xlsxwriter.  None will work for your purposes, as xlrd/wt don't support all formulae, xlsxwriter can't modify existing xlsx files, and openpyxl loses images and charts.\nSince I often need to do this, I've taken to only writing simple output to a separate file and then calling the win32api directly to copy the data between the workbooks while preserving all of my colleague's shiny figures.  It's annoying, because it means I have to do it under Windows instead of *nix, but it works.\nIf you're working under Windows, you could do something similar.  (I wonder if it makes sense to add a native insert option using this approach to help people in this situation, or if we should simply post a recipe.)\n\nP.S.: This very problem has annoyed me enough from time to time that I've thought of learning enough of the modern Excel format to add support for this to one of the libraries.\nP.P.S.: But since ignoring things you're not handling and returning them unmodified seems easy enough, the fact that no one seems to support it makes me think there are some headaches, and where Redmond's involved I'm willing to believe it.  @john-machin would know the details, if he's about..\n", 0.7983275651931763, 0.8045221567153931, 0.8014129400253296, 0.1797752808988764, 9.231787263313528e-232, 546], ["I have an array of graysale image read in from a color one. If I use matplotlib to imshow the grayscale image, it looks just fine. But when I io.imsave it, it's ruined (by an outrageous amount of noise). However, if I numpy.around it first before io.imsave-ing, then it's significantly better, but black and white are still all swapped (dark regions appear white, and bright regions appear dark)\nscipy.misc.imsave, on the other hand, works perfectly.\nThank you.\n", " The problem is that the ndarray is not a true n-dimensional array, so it can't be imaged. \nThe solution is to convert it to a n x n array first, using npy.frombuffer, narray.array, or nmatrix.matrices. Then you can use nim.imshow to display it. This will work for all n arrays, not just nxn arrays. You can also use the array.view method to get a view of the data, which will be a 2D array (or a 3D one if you use a matrix).\n", "I think I've figured out why. By convention, floats in skimage are supposed to be in the range [0, 1].\n", 0.8126529455184937, 0.8475897312164307, 0.8297537565231323, 0.05405405405405406, 9.141066424986713e-232, 426], ["I use Spyder as my Python IDE. In most of what I do I use the editor to write code and execute portions of the program one at a time. I typically go back and forth, change things, add pieces, execute portions... you get it. Is there a way to save to file what Spyder shows in the console portion of the IDE after each portion of code is executed?\n", " You can save the output of your code to a file.\n", "(Spyder dev here) Well, there is a way to save the output shown in our consoles but not after each execution. You have to do it manually, like this:\n\nFor our Python consoles, you need to do a right mouse click over any console and select the option Save history log.\nFor our IPython consoles, you need to do a right mouse click over any console and select the option Save as HTML/XML\n\n", 0.9154900312423706, 0.8272885680198669, 0.8691574335098267, 0.1411764705882353, 1.4740564900137075e-231, 49], ["I'm new in python testing and am having some diffiulties choosing between \nunittest, pytest and nose nose. \nI'm bulding a test lib that requires : installing a package changing the conf restarting deamons and unsintalling the packages on a linux box. Which test framewok will be the best between thoses three?\nAt the end of the test I'd like very much to have :\nn test Suceeded\nn test Skiped\nn test failed\nCan you please provide some advise?\ncheers\n", " py.test is the most popular choice, but it's not the only choice.\n", "That is a very wide question with a lot of resources available. However, I will recommend py.test because getting started is very easy despite having a full set of tools. Nose has a bit more configuration needed than py.test before starting. Unittest is like junit in java, which is not a bad place to start. Anyway, I recommend py.test. \n", 0.8972865343093872, 0.8488581776618958, 0.8724008202552795, 0.10526315789473685, 1.3466263920541852e-231, 67], ["I am iterating a list of links for screen scraping.  The pages have JavaScript so I use Selenium.  I have a defined a function to get the source for each page.\n\nShould I instantiate the WebDriver inside that function, which will happen once per loop?\nOr should I instantiate outside the function and pass the WebDriver in?\nOr assign the WebDriver to a variable that will be visible from inside the function, without explicitly passing it?\n\n", " You should instantiating the webdriver in the loop. The web driver will not be available for the next iteration. If you want to use the same web-driver for all the iterations, you can instantiated it in a method. For example,\n   def get_links(self):\n#  self.driver = web_driver.get(url)\nself.links = self._get_all_urls()\nreturn self\n", "Each instantiation of WebDriver launches a new browser, which is a very costly operation, so option 1 is not what you want to do. \nI would also not do option 3 because it is not good coding practice to depend on global variables when it can easily be avoided. \nThis leaves you option 2: instantiate WebDriver once and pass the instance to your function(s).\n", 0.8354386687278748, 0.8443182706832886, 0.8398550748825073, 0.17741935483870966, 9.889428760587541e-232, 334], ["How to get the content of a row of a Numpy array ? For example I have a Numpy array with 3 rows color=np.array([[255,0,0],[255,255,0],[0,255,0]]) and I want to retrieve the content of the first row [255,0,0].\n", "You can use np.argmin(a, axis=0) to find the index of that row.\n", "Use array indexing as below:\ncolor[0]\n", 0.8671071529388428, 0.8690887689590454, 0.8680968284606934, 0.18181818181818182, 1.2676114249651737e-231, 64], ["Can Python + NLTK be used to identify the subject of a sentence? From what I have learned till now is that a sentence can be broken into a head and its dependents. For e.g. \"I shot an elephant\". In this sentence, I and elephant are dependents to shot. But How do I discern that the subject in this sentence is I. \n", " You can use NLSTK to do this.\n", "English language has two voices: Active voice and passive voice. Lets take most used voice: Active voice.\nIt follows subject-verb-object model. To mark the subject, write a rule set with POS tags. Tag the sentence I[NOUN] shot[VERB] an elephant[NOUN]. If you see the first noun is subject, then there is a verb and then there is an object.\nIf you want to make it more complicated, a sentence- I shot an elephant with a gun. Here the prepositions or subordinate conjunctions like with, at, in can be given roles. Here the sentence will be tagged as I[NOUN] shot[VERB] an elephant[NOUN] with[IN] a gun[NOUN]. You can easily say that word with gets instrumentative role. You can build a rule based system to get role of every word in the sentence.  \nAlso look at the patterns in passive voice and write rules for the same.\n", 0.8596525192260742, 0.7773588299751282, 0.8164372444152832, 0.0375, 1.6119675701300876e-231, 31], ["I am using the Anaconda 2.1.0 distribution of Python on Windows 8.\n\n\n    python --version\n    Python 3.4.1 :: Anaconda 2.1.0 (64-bit)\n\n\nI used pip to install the mechanize package. pip (v 6.0.8) installed mechanize 0.2.5 which is the most recent release.\nBut, while trying to import the package, python throws an error:  \n\n\n    >>> import mechanize  \n    Traceback (most recent call last):\n      File \"\", line 1, in   \n      File \"C:\\Anaconda3\\lib\\site-packages\\mechanize\\__init__.py\", line 122, in   \n    from _mechanize import \\\n    ImportError: No module named '_mechanize'\n\n\nSimilar questions here received replies to check if the installation was done on the PYTHONPATH.\nI also checked sys.path and there seems to be no problem there.\n\n\n    >>> import sys\n    >>> sys.path\n    ['',\n     'C:\\\\Anaconda3\\\\Scripts',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\cssselect-0.9.1-py3.4.egg',\n     'C:\\\\Anaconda3',\n     'C:\\\\Anaconda3\\\\python34.zip',\n     'C:\\\\Anaconda3\\\\DLLs',\n     'C:\\\\Anaconda3\\\\lib',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.2.3-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\runipy-0.1.1-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-12.2-py3.4.egg',\n     'C:\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n     'C:\\\\Users\\\\Kumar Siddharth\\\\.ipython']\n\n\nI am able to import other packages residing in the same directory, for e.g. numpy.\nAm I missing something?\n", " The", "Thanks Joe for pointing that out.\nMechanize is not supported for Python 3.x.\nFor my job, I set up a new Python 2.7 environment through conda and switched to it. It addressed the issue.\n", 0.9997346997261047, 0.8219032883644104, 0.902138888835907, 0.05405405405405406, 1.821831989445342e-231, 4], ["I have latest IronPython version built and running in Ubuntu 14.04 through Mono.\nBuilding Ironpython and running with Mono seems trivial but I am not convinced I have proper sys.paths or permissions for Ironpython to import modules, especially modules like fcntl. \nRunning ensurepip runs subprocess, and wants to import \"fcntl\". There are numerous posts already out there, but mostly regarding windows.\nAs I understand, fcntl is part of unix python2.7 standard library. To start the main problem seems to be that Ironpython has no idea where this is, but I also suspect that since fcntl seems to be perl or at least not pure python, that there is more to the story.\nSo my related sys.path questions are:\n\nIn Ubuntu, where should I install Ironpython (Ironlanguages folder)\nto? Are there any permissions I need to set?\nWhat paths should I add to the sys.path to get Ironpython's standard library found?'\nWhat paths should I add to the sys.path to get Ubuntu's python 2.7 installed modules?\nWhat paths should I add to the sys.path  or methods to get fcntl to import properly in Ironpython\nAny clues on how to workaround known issues installing pip through ensurepip using mono ipy.exe X:Frames ensurepip\n\nThanks!\n", " The problem is that the standard lib is not in the path. You need a path to it. The standard path is /usr/lib/python2/\nThe problem with the fclntls module is the same as the problem of the python standard module. It is in /lib, not /python. This is because the module was written for python 3.2, which is no longer supported. So you need the /Python/ standard directory. If you are using a python version older than 3, you will need /Library/Python. Or you can use the Python standard dir. In this case, the modules are in a subdirectory of /System/Library/, so you just need sys/path.py. For example, if you have a /bin/foo, /foo/bin, etc. you would have to add the following to syspath: /path/to/sys/os/getenv('PATH')\n", "As far as I can see, the fcntl module of cPython is a builtin module (implemented in C) - those modules need to be explicitly implemented for most alternative Python interpreters like IronPython (in contrast to the modules implemented in plain Python), as they cannot natively load Python C extensions.\nAdditionally, it seems that there currently is no such fcntl implementation in IronPython.\nThere is a Fcntl.cs in IronRuby, however, maybe this could be used as a base for implementing one in IronPython.\n", 0.8201891183853149, 0.8411180973052979, 0.8305217623710632, 0.27027027027027023, 8.426611900718831e-232, 721], ["There is a trend of discouraging setting sys.setdefaultencoding('utf-8') in Python 2. Can anybody list real examples of problems with that? Arguments like it is harmful or it hides bugs don't sound very convincing.\nUPDATE: Please note that this question is only about utf-8, it is not about changing default encoding \"in general case\".\nPlease give some examples with code if you can. \n", " I've seen a lot of people complaining about this, and I'm not sure if it's a problem with the way the default is set or the fact that it was set in the first place. I think it has to do with how the encoding is handled in a given context. For example, if the user has set the system encoding to UTF-16, then the Python interpreter will try to convert the string to a Unicode string, which will fail if there is no encoding set. If the interpreter is running on a machine that has a UTF16 encoding, the conversion will succeed, but the output will be in UTF8. This is because the UTF encoding has no concept of a \"default encoding\" and so the \"encodings\" module will not be able to tell the difference between the two. So, in this case, you'll get a unicode string that is in an encoding that the operating system doesn't know how to handle. In this way, setting the sys encoding will make the code fail. However, this is the only way to make this work, so it will work. The other way is to set sys to the correct encoding. But this will also fail, because it won't be the same as the OS encoding (or the machine encoding).\n", "Real-word example #1\nIt doesn't work in unit tests.\nThe test runner (nose, py.test, ...) initializes sys first, and only then discovers and imports your modules. By that time it's too late to change default encoding.\nBy the same virtue, it doesn't work if someone runs your code as a module, as their initialisation comes first.\nAnd yes, mixing str and unicode and relying on implicit conversion only pushes the problem further down the line.\n", 0.8316423892974854, 0.8345718383789062, 0.8331045508384705, 0.2302631578947368, 7.653036010141204e-232, 1124], ["I am trying to submit a form via python and I need to know, should I use the \"id\" value, or the \"name\" value. They are both different.\n", " id is the same as name, but name is a reserved word in python. \n", "You should use the \"name\" attribute. \nFor example using radio buttons, each button will have the same name but different Id. When submitted only the one with a value (the selected one) will be submitted.\n", 0.8787590265274048, 0.8506054282188416, 0.8644530773162842, 0.28571428571428575, 1.3735194321822388e-231, 65], ["In my application, I need to allow only one task at a time per user. I have seen that we can set max_concurrent_requests: 1 in queue.yaml. but this will allows only one task at a time in a queue.\nWhen a user click a button, a task will be initiated and it will add 50 task to the queue. If 2 user click the button in almost same time total task count will be 100. If i give max_concurrent_requests: 1 it will run only one task from any of these user.\nHow do i handle this situation ?\n", "You can use a Queue with a fixed size, and a limit on the number of tasks that can be in the Queues. \nThe Queued Task will only be able to be added to a single Queuet. The Queuers will not be allowed to add more than the limit. This will prevent the task being added twice. You will need a way to determine if the user has already added a Task to this Queuer. In this case you can just return false from the method. Otherwise you will have to check if there is already a Tasks in this queue, if so you just add the Task.\n\n", "You can specify as many queues as you like in queue.yaml rather than just using the default push queue. If you feel that no more than, say, five users at once are likely to contest for simultaneous use of them then simply define five queues. Have a global counter that increases by one and wraps back to 1 when it exceeds five. Use it to assign which queue a given user gets to push his or her tasks to at the time of the request. With this method, when you have six or more users concurrently adding tasks, you are no worse off than you currently are (in fact, likely much better off). \nIf you find the server overloading, turn down the default \"rate: 5/s\" to a lower value for some or all of the queues if you have to, but first try lowering the bucket size, because turning down the rate is going to slow things down when there are not multiple users. Personally, I would first try only turning down the four added queues and leave the first queue fast to solve this if you have performance issues that you can't resolve by tuning the bucket sizes.\n", 0.8578962087631226, 0.825688362121582, 0.8414841890335083, 0.34838709677419355, 9.065216746224771e-232, 522], ["Given a double, what is the most efficient way to calculate the precision and accuracy of the number? Ideally, I'd like to avoid for loops. \n", " The answer is a bit more complicated than that, but I'll try to explain it.\nThe double is an object that has a number of properties, such as the value, the sign, and the scale. The scale is used to determine the accuracy, which is how many decimal places to display. For example, a value of 1.5 is displayed as 1,000, or 1/5, so the display is accurate to 5 decimal points. A value like 0.1 is not displayed, because it is too small to be displayed. So, if you want to know the exact value that is in the double object, you must know how to convert it to a string, then convert that string to an integer, etc. This is called the \"scientific notation\" of a floating point number. In this case, it's the same as displaying the decimal point, except that it has more decimal digits. Thus, 0,1,2,3,4,5 are all displayed exactly as 0 decimal, 1 decimal. However, this is only true for the scientific notation, not for all floating-point formats. Floating-points formats like scientific, scientific-like, exponential, logarithmic, fixed-precision, floating, decimal-fixed, hexadecimal, octal, binary, as well as many others are not exactly the mathematical representation of floating points, they are simply a representation for displaying a decimal number, with a few extra decimal place. These formats are called \"fixed-width\" formats, since they have a fixed width, usually 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 1536, 3072, 6144, 12288, 262144. Note that the width of these formats is determined by the underlying hardware, rather than the floating number itself. Therefore, some of them are wider than others, depending on the hardware. Also, many of those formats have different widths for different numbers, for example scientific and scientific like formats use different decimal width for numbers like 1e-10, 2e10 and so on. Some formats may have the widest width available, while others may be wider or narrower than other formats in a given hardware platform. There are many more formats than those I've mentioned, all of which are available in various hardware platforms. I'm not going to go into the details of all the formats available on all platforms, just the ones I know about. If you're interested in learning more, there are plenty of books on floating numbers available. One of my favorite books is \"The Floating Point Handbook\" by Mark Lutz, available online at Amazon.com. Another good one is by Tim Gowers, \"A Practical Guide to Floating point", "Floating point numbers are stored in binary in python. Accuracy and precision have totally different implications in binary, especially given that computers are constrained to fixed-length representations. You can demonstrate that to yourself simply by noting that the code 1e16+1 == 1e16 returns True.\nYou linked to Wolfram.com where precision and accuracy are discussed in terms of decimal numbers. It means nothing to talk about the position of the decimal point in a number stored in a computer in binary. To represent and manipulate decimal numbers with their real decimal precision and accuracy, you need to use python's Decimal class.\nNote that you must give the Decimal class a string for it to be accurate in decimal terms. If you give it a literal number (e.g. Decimal(1.001)) you are just giving it a float, and the Decimal you create will be the precise decimal representation of python's imprecise floating point representation of 1.001.\nSadly, the Decimal class doesn't have methods to return the number of digits either side of the decimal point. However, assuming your decimal is in decimal form, you can just examine it as a string, using str(x) where x is an instance of Decimal.\n", 0.8071099519729614, 0.838896632194519, 0.8226963877677917, 0.35093167701863354, 6.494441658294486e-232, 2477], ["how can I store in a variable, a string containing the EOF character? In other words, how can I represent EOF in a python string?\nThanks in advance\n", " You can't. Python strings are immutable.\n", "There is no EOF character.... Usually it is just when you run out of data. If you really want one, you could just make some arbitrary string combination up, but you are probably better off just reading each line until the file is out of data. \n", 0.9043257832527161, 0.8313558101654053, 0.8663069605827332, 0.07547169811320756, 1.4941162914151016e-231, 42], ["I am using ConEmu windows emulator and I would like to run one simple command on more terminals at the same time. Is there any way to do that?\n", " Conemu is a console-only application. You can't run a command in a terminal.\n", "Apps+G groups input for all visible panes.\n", 0.8610979914665222, 0.8363097906112671, 0.848522961139679, 0.0, 1.185814295932958e-231, 78], ["Scenaio Outline: Blah de blah\n  When I enter  and  on the input field\n  Then Everything is good\nExamples:\n  | a | b |\n  | 1 | 2 |\n  | \u00a0\u00a0 | 3 |\nThe above scenario throws the following error in BBD Behave\nTest undefined\nPlease define test\nI am not sure how I can come around this.\nAny suggestions? \n", " I think you need to use the \"test\" method.  This method is used to test the behavior of the method you are testing. In this case, you want to check that the value entered is in the correct format. You can use this method to do this by calling the test method on your method that you have defined. For example, if you had defined a method called \"myMethod\" that takes a string as an argument, then you could call the myMethod method and then test that it returns the string \"a\" when you enter \"1\" and \"2\" on an input box. This will test your mymethod method for the expected behavior. If you wanted to make this test more specific, for example if your test is to see if the user can enter a number, but you only want the result to be a letter, this would be done by using the return method instead of calling my method, and testing that my return value is \"b\".\n", "As far as I know you can't do it.  But you can use either an empty string or a placeholder value (e.g. 'N/A') that you can look out for in your step definitions.\n", 0.8230626583099365, 0.8591694831848145, 0.8407285809516907, 0.196078431372549, 7.52042765627792e-232, 861], ["Every time I'm using the django-admin command \u2014 even on TAB\u2013completion \u2014 it throws a RemovedInDjango19Warning (and a lot more if I use the test command). How can I suppress those warnings?\nI'm using Django 1.8 with Python 3.4 (in a virtual environment).\nAs far as I can tell, all those warnings come from libraries not from my code, here are some examples:\n\n\u2026/lib/python3.4/importlib/_bootstrap.py:321: RemovedInDjango19Warning: django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes.\nreturn f(*args, **kwds)\n\u2026/lib/python3.4/site-packages/django/contrib/admin/util.py:7: RemovedInDjango19Warning: The django.contrib.admin.util module has been renamed. Use django.contrib.admin.utils instead.\n\"Use django.contrib.admin.utils instead.\", RemovedInDjango19Warning)\n\u2026/lib/python3.4/site-packages/django/templatetags/future.py:25: RemovedInDjango19Warning: Loading the ``url`` tag from the ``future`` library is deprecated and will be removed in Django 1.9. Use the default ``url`` tag instead.\nRemovedInDjango19Warning)\n\n", " I've found that the problem is that you're using a version of Django that doesn't support the Removed In Django 19 warning. \nThe solution is to install the latest version, which is 1,9, but you'll need to do this on a per-project basis. You can find the version you need by running: pip freeze > requirements.txt. Then you can install it with: easy_install dj-1.10.1-py2.7.egg\n", "Django puts warnings through the standard python warnings module. If your python project throws warnings and they're \"acceptable\" for the moment, just use warnings.filterwarnings() or warnings.simplefilter(). I'm not sure where the \"best\" place for these are, but I've dropped them into my common_settings.py file (For me, this is a unchanging, checked-in file, that is imported by local_settings.py).\nEg:\nwarnings.filterwarnings(action=\"ignore\", category=RemovedInDjango110Warning, module='django.template.utils', lineno=37)\nAlas the comment about silencing the system checks won't work, here, since your example isn't throwing a system-check error.\n", 0.8394374251365662, 0.8230903148651123, 0.8311834931373596, 0.2346368715083799, 1.0190272661982057e-231, 378], ["My google searching has failed me. I'd like to know how to make an executable Python file on OS X, that is, how to go about making an .exe file that can launch a Python script with a double click (not from the shell). For that matter I'd assume the solution for this would be similar between different scripting languages, is this the case?\n", "You can't make a.py file executable, but you can make it run as a command.\nThe easiest way to do this is to use the -c flag to the python command, which will launch the script in a new window. \nFor example, if you wanted to launch your script from a terminal window, you could use this command:\npython -m SimpleHTTPServer -t http://localhost:8001\n", "You can run python scripts through OS X Terminal. You just have to write a python script with an editor, open your Terminal and enter python path_to_my_script/my_script.py\n", 0.8463174700737, 0.8655145168304443, 0.8558083176612854, 0.27999999999999997, 9.622034193354714e-232, 347], ["I apologize in advance for my lack of knowledge concerning character encoding. \nMy question is: are there any inherent advantages/disadvantages to using the 'Unicode' type, rather than the 'String' type,  when storing data in PostgreSQL using SQLAlchemy (or vice-versa)? If so, would you mind elaborating?\n", " The advantage of using unicode is that it is a more human-readable format.\nThe disadvantage is the fact that the encoding is not guaranteed to be UTF-8. This means that you will have to convert the data to UTF8 when you store it in the database. If you use a bytea column, you can use the byte representation of the unicodes, which is guaranteed by the Postgres documentation to have the same encoding as the character representation. However, if you are using a character column (a character string), you have no guarantee that this will be the case. In this case, the only way to guarantee the correct encoding of a unicoded character is to use UTF16. UTF32 is also possible, but it will not be as good as UTF 16. The only other way is using UTF 8, and this is only possible if the column is UTF encoded. So, it's not a good idea to store unicoders in a column. It's better to just use strings. Also, using strings is more efficient, because you don't have a string to encode. You just have an integer, so you only need to do one conversion. And, in some cases, this conversion is faster than using byte strings, especially if your column has a lot of unicods. For example, a UTF string is just a sequence of bytes, while a Unicode string has to first be decoded, then re-encoded. Using a single conversion for both is much faster. Another advantage is, that if a user inserts a new unicoder, he will get a warning, telling him that he has already used the Unicode character. But, when he inserts the string, Postgre will convert it to the UTF encoding, without warning. PostGre is very good at this, as it knows that a given character can be in many different encodings. Thus, there is no need for a conversion, since it already knows the right encoding for the given unicodon. A disadvantage of this method is if there are many unicodus in your database, or if they are not all in UTF. Then, your application will need a way of converting them to a common encoding (e.g. UCS-2, UCS4, UTF, etc.). This is where the use of UTF is really useful. When you insert a UCS2 character, for example (in UTF), Postgis will automatically convert this to UCS 2, even if it has no UCS 4 character in it. Similarly, with UCS 3, all UCS characters will become UCS3, unless you", "In 99.99% of the cases go for Unicode and if possible use Python 3 as it would make your life easier.\n", 0.785195529460907, 0.845213770866394, 0.8141000270843506, 0.0625, 6.074783152240455e-232, 2265], ["I am pretty familiar with building web based apps using python django on my linux machine. But when I decided to try my hand at building desktop applications that can run on windows/linux I didn't know where to begin.\nI know I can surely build windows desktop application on windows machine. But I am pretty comfortable with linux and don't want to get out of that comfort zone. Can anyone guide me as to what tools can I begin with to develop a simple windows desktop application. I would target windows 7 for starters.\nAny guidance is hugely appreciated.\n", " You can use the command line tools to build a windows application, but you will need to know how to use them. \nFor example, you can start a command prompt window, and type \"python my_script.py\" to start the application in the background. You will then be able to see the output of the script in your command window. This is a good way to learn how the tools work. Once you know the basics, then you should be good to go. There are many other ways to do this, such as using the wxPython toolkit. It is also possible to write a small console application that will run in a separate window and display the results in real time. The wxtools library is another good option. These are all good ways of learning how these tools function. If you are looking for a more in-depth tutorial, I recommend the following:\nhttp://python-wx.sourceforge.net/\n", "What you are looking for is a GUI tool-kit with bindings to python. Tkinter is the de facto standard for python GUI and is cross platform. Qt is also a popular choice but the license is more restrictive then Tkinter but will allow you to transition into C++ programming with Qt easier if that is something you may want to do down the road. The choice up to you. \n", 0.8334400653839111, 0.846287727355957, 0.8398147225379944, 0.2807017543859649, 7.709411755499781e-232, 842], ["How to check the current version of sympy and upgrade to latest. I am using mac. The way I installed my current version is using \"pip install sympy\"\n", " You can use the following command to get the latest version:\npydoc sympyd.py --version\n", "Use pip list to list all packages and their versions. You can pipe it to grep to search for the package your interested in:\npip list | grep sympy\nAlternatively to get all information about that package including version you can use pip show:\npip show sympy\nTo upgrade it's simply:\npip install --upgrade sympy\nIf you need write permissions to your python installation directory don't forget to prepend the pip install command with a sudo: e.g. sudo pip install --upgrade sympy\n", 0.8829593658447266, 0.8305000066757202, 0.8559266328811646, 0.18181818181818182, 1.3431700288830542e-231, 88], ["I have a question about REST design in general and specifically what the best way to implement a solution is in Django Rest Framework.  Here it the situation:\nSay I have an app for keeping track of albums that the user likes.  In the browser, the user sees a list of albums and each one has a check box next to it.  Checking the box means you like the album.  At the bottom of the page is a submit button.  \nI want the submit button to initiate an AJAX request that sends tp my API endpoint a list of the ids (as in, the Djano model ids) of the albums that are liked by the user.  \nMy question is, is this a standard approach for doing this sort of thing (I am new to web stuff and REST in particular).  In other words, is there a better way to handle the transmission of these data than to send an array of ids like this?  As a corollary, if this is an alright approach, how does one implement this in Django Rest Framework in a way which is consistent with its intended methodology.  \nI am keeping this question a little vague (not presenting any code for the album serializer, for example) intentionally because I am looking to learn some fundamentals, not to debug a particular piece of code.\nThanks a lot in advance!\n", " You can use a POST request to get the list. You could also use GET. The difference is that POST is for a single request, while GET is used for multiple requests. POST will send a JSON object, which will be serialized to a string. GET will return a response, and you can parse it with a regular expression. If you want to use POST, you'll have to make sure that you send the correct data. For GET, it's up to you to parse the response. I'm not sure what you're trying to do, but if you just want a simple list, I'd recommend POST. It's a bit more work, though. Also, POST can be used to post a form, so you could use it to POST a new album, or to update an existing one. But if that's what your doing, then you should use the POST method. There's no need to reinvent the wheel. :)\n", "Consider the upvote button to the left. When you click it, a request may be sent to stackoverflow.com/question/12345/upvote. It creates an \"action resource\" on the db, so later you can go to your user profile and check out the list of actions you took.\nYou can consider doing the same thing for your application. It may be a better user experience to have immediate action taken like SO, or a \"batch\" request like with gmail's check boxes.\n", 0.8339738249778748, 0.837913990020752, 0.8359392881393433, 0.2926829268292683, 8.259885936748448e-232, 781], ["I have a Python client behind a NAT and a python server with a public IP address. My job is to send a pcap file (the size of a few MB) from the client to a server, as well as a dictionary with some data. \nIs there any easy way of doing this without resorting to third-party libraries (e.g. twisted, tornado)?\nIf not, what's the easiest alternative?\nI thought I could send the pcap file through http so that it would be easier to read it on the server side, and perhaps I could do the same with the dictionary by first pickling it. Would it be a good solution?\n(I have complete control on the server, where I can install whatever)\n", " You can use the Twisted library to do this.\n", "If you can install software on the server, and the server allows HTTP connections, you can write your own simple HTTP server (Python has libraries for doing that).  If not, the answer would depend on what services are available on the server.\n", 0.9165393114089966, 0.8608154058456421, 0.887803852558136, 0.11764705882352941, 1.4875195904069663e-231, 45], ["I have written a piece of software in Python that does a lot of parsing and a lot of writing files to disk. I am starting to write unit tests, but have no idea how to unit test a function that just writes some data to disk, and returns nothing.\nI am familiar with unittest and ddt. Any advice or even a link to a resource where I could learn more would be appreciated.\n", " You can use unix sockets to test your code. \n", "Arguably, the best solution is to split your function into two pieces. One piece to do the parsing, the second to do the writing. Then, you can unit test each piece separately. \nFor the first piece, give it a file and verify the parsing function returns the proper string, and/or throws the proper exception. \nFor the second, give it a string to write, and then verify that the file was written and that the contents match your string. It's tempting to skip the test that writes the data, since it's reasonable to assume that the python open and write functions work. However, the unit testing also proves that the data you pass in is the data that gets written (ie: you don't have a bug that causes a fixed string to be written to the file).\nIf refactoring the code isn't something you can do, you can still test the function. Feed it the data to be parsed, then open the file that it wrote to and compare the result to what you expect it to be.\n", 0.8843070864677429, 0.8074101209640503, 0.8441109657287598, 0.06282722513089005, 1.4204668274596413e-231, 46], ["I refactored my webapp and now my IDE pycharm marks some imports red. Why?\nfrom wtforms import Form, TextField, validators, SelectField\nMy IDE marks the Form, TextField and the SelectField red as they cannot be imported. (\"Unresolved reference\"). What should I do if I need those classes in my project?\n", " You need to add the classes to your project.\n", "Try deleting the libraries from your project if they are in libraries, then re-importing those libraries.\nAlso, I assume you've done this, but make sure the libraries are actually installed and present in a reachable location that is properly mapped.\n", 0.925428032875061, 0.8593677282333374, 0.8911753296852112, 0.1568627450980392, 1.4409104529338234e-231, 46], ["I refactored my webapp and now my IDE pycharm marks some imports red. Why?\nfrom wtforms import Form, TextField, validators, SelectField\nMy IDE marks the Form, TextField and the SelectField red as they cannot be imported. (\"Unresolved reference\"). What should I do if I need those classes in my project?\n", " You need to add the classes to your project.\n", "You need to install it in in your environment(according to the comments you didn't), please try the following:\nSettings -> Project: MyProjectName -> Project Interpreter\nThen click on the green plus and choose your packages\n", 0.9226392507553101, 0.8358113765716553, 0.8770816326141357, 0.3181818181818182, 1.4409104529338234e-231, 46], ["I want to sample from my posterior distribution using the pymc package. \nI am wondering if there is a limit on the number of dimensions such algorithm can handle. My log likelihood is the sum of 3 Gaussians and 1 mixture of Gaussians. I have approx 750 parameters in my model. Can pymc handle such a big number of parameters?\n", " The pysample function in the pyMC package can be used to do this.\n", "I recently ran (successfully) a model with 2,958 parameters. It was on a 8 Gb Windows machine. You should be fine with 750.\n", 0.8478044271469116, 0.8358980417251587, 0.8418091535568237, 0.05405405405405406, 1.3466263920541852e-231, 67], ["I installed zipline package via Enthought Cantopy. Now I try to run a script using it in command prompt, but get error ImportError: No module named zipline.\nI also tried to run the same code using IPython, with the same output.\nI think it is related to python virtual environments, but don't know how to fix that.\n", " You need to install the zipped package in a virtual environment. \nIn the Enscripten environment, you can install zips using pip install -e.\nThen you need a zipping script that will zip the files in the virtualenv. You can use the following command to create a zip file in your virtual env:\npython -m pip freeze > zipfile.zip\n", "I figured it out. The problem was in the version of python I have. I have 32-bit python and Enthought Cantopy with 64 bit python, installed zipline package was under 64 bit python while command prompt was using 32 bit version. Installing 64 bit python fixed the issue.\n", 0.8250133991241455, 0.8365839719772339, 0.8307583928108215, 0.1495327102803738, 1.0034234290939611e-231, 326], ["I have a 3+ million record XLS file which i need to dump in Oracle 12C DB (direct dump) using a python 2.7.\nI am using Cx_Oracle python package to establish connectivity to Oracle , but reading and dumping the XLS (using openpyxl pckg) is extremely slow and performance degrades for thousands/million records.\nFrom a scripting stand point used two ways-\n\nI've tried bulk load , by reading all the values in array and then dumping it using cursor prepare (with bind variables) and cursor fetchmany.This doesn't work well with huge data.\nIterative loading of the data as it is being fetched.Even this way has performance issues.\n\nWhat options and techniques/packages can i deploy as a best practise to load this volume of data from XLS to Oracle DB ?Is it advisable to load this volume of data via scripting or should i necessarily use an ETL tool ?\nAs of now i only have option via python scripting so please do answer the former\n", " can use the cxOracle.py module to do the bulk loading.  It will load the entire file into memory, and will not use a cursor.\n\n", "If is possible to export your excel fila as a CSV, then all you need is to use sqlldr to load the file in db \n", 0.8675487637519836, 0.8569108247756958, 0.8621969223022461, 0.24, 1.1753387737804285e-231, 127], ["I just started setting up a centos server today and noticed that the default version of python on centos is set to 2.6.6. I want to use python 2.7 instead. I googled around and found that 2.6.6 is used by system tools such as YUM so I should not tamper with it. Then I opened up a terminal on my mac and found that I had python 2.6.8 and 2.7.5 and 3.3.3 installed. Sorry for the long story. In short I just want to know how to lookup all the version of python installed on centos so I don't accidentally install it twice.\n", " You can check the python version using the following command:\npython -V\n", "As someone mentioned in a comment, you can use which python if it is supported by CentOS. Another command that could work is whereis python. In the event neither of these work, you can start the Python interpreter, and it will show you the version, or you could look in /usr/bin for the Python files (python, python3 etc). \n", 0.8742270469665527, 0.8489704132080078, 0.8614136576652527, 0.22535211267605634, 1.3498430622113935e-231, 73], ["How can I customize my robot framework log.html and  output so that I can add some external links to my output files like log.html and output.xml file. \n", " You can use the following code to add links in your log file:\n<link href=\"${BASE_PATH}/log.${date:format=%Y-%m-%-d %H:%M:%.2f}${filename}\">\n   <link rel=\"shortcut icon\" href=\"/favicon.ico\" type=\"image/x-icon\" />\n</link>\n", "You want to add top-level metadata.\nAnd that metadata would be an HTML link.\nCreate a suit setup for the master suite (create a file called __init__.robot in \nthe parent test folder)\nAnd in it:\n\n\n*** Settings ***\nDocumentation     The main init phase for all robot framework tests.\nSuite Setup  Setup\n\n*** Keywords ***\n\nSetup\n    Set Suite Metadata  Link to my cool external site  http://www.external.com  top=True\n\n\n", 0.7728711366653442, 0.8182055950164795, 0.7948924899101257, 0.14953271028037382, 1.1574064021798955e-231, 221], ["I have a server application that needs to schedule functions to be called at various times during the week, with a desired accuracy of plus or minus 15 seconds, let's say.  threading.Timer is the simplest solution, but I'm concerned about accuracy when using intervals of several hundred thousand seconds.  Everything I can find concerning Timer's accuracy focuses on timers with comparatively tiny periods.\nTests using timers with intervals on the order of an hour or two yield almost perfect results, but I'd like some assurance this is something I can rely on.\n", " Timers are not perfect, and you should not rely solely on them. Timing is a very important part of the system, so you need to make sure that you are using the right tool for the job. If you use a Timed Event, you will need a good way to measure the accuracy. You can use the built-in accuracy, or you can measure it yourself. The built in accuracy is not very accurate, as it is based on a fixed number of events, which is very small. However, if you measure your own accuracy you'll find that it's quite good. I've used the Timestamps module in the Python profiler to find out how many events I was getting, then I used that to get an accurate measure of my own performance. This is what I use for my tests. It's not the most accurate method, however, it does give you a measure that is fairly accurate. Another method is to use your system clock, this will give a more accurate result, although it will be less accurate than the clock on your machine. There are many other methods, such as the one in this answer, that will help you to determine the best method for your needs. Also, I would recommend that if your application is going to run on multiple platforms, make a copy of your code on each platform, to ensure that the code is as consistent as possible. Having a consistent code base will make it easier to debug problems, especially if the application has a lot of dependencies. Lastly, the more you know about your platform the better you're going be able to troubleshoot problems. For example, on my machine, my application will run fine on Windows, Linux, Mac, FreeBSD, OpenSolaris, AIX, etc. Knowing that I will have problems on those platforms will allow me to fix the problems before they become problems for other platforms. So, knowing your platforms is important. Once you have your method of measuring your performance, use that method to compare your results with the results you get from your other method. Then, once you've found the method that gives you the least error, try using that as your default method and see if it gives the same results. Remember, your goal is accuracy and not speed. Speed is only important if there is no other way of getting the result you want. In this case, speed is irrelevant.", "Accuracy should match the computer clock: milliseconds.\nThe real problem is the jobs you're running.  Do they finish before the period expires?  That's dependent on the job and the machine load.  The Timer can't help with that.\n", 0.8026599884033203, 0.8514238595962524, 0.8263230919837952, 0.10989010989010989, 6.1473979365873355e-232, 2237], ["I am pretty new in programming, just learning python.\nI'm using Komodo Edit 9.0 to write codes. So, when I write \"from math import sqrt\", I can use the \"sqrt\" function without any problem. But if I only write \"import math\", then \"sqrt\" function of that module doesn't work. What is the reason behind this? Can I fix it somehow?\n", " The problem is that the module is imported as math, not as sqr. \n", "When you only use import math the sqrt function comes in under a different name: math.sqrt. \n", 0.897641122341156, 0.891254186630249, 0.894436240196228, 0.13333333333333333, 1.3165594234639305e-231, 66], ["I have a dictionary that is passed to me from a function that I do not have access to the code.  In this dictionary there is a key called 'time'.  I can print d['time'] and it prints the value I expect.  However, when I iterate through the dictionary, this key is skipped.  Also d.keys() does not include it.  If it matters, the other keys are numerical.\nHow would I recreate this?  How do you see hidden keys without knowing the name?  Can this be undone?\nprint type(d) returns <type 'dict'>\n", " If you want to see the keys, you can use the inspect module. For example, if you have the following:\nd = {'a': 1, 'b': 2, 3}\ninspect.getmembers(type(a))\n# [('a', 'type'), ('b', None)]\n", "Python can do pretty much anything in response to item access since any class can redefine __getitem__ (and, for dict subclasses, __missing__). If the documentation doesn't cover it, there is no well-defined way to discover what \"hidden keys\" are available in any given object, short of inspecting the source code.\n", 0.8048465251922607, 0.8367899656295776, 0.8205074667930603, 0.1627906976744186, 1.156100809123877e-231, 185], ["Would it be possible to execute multiple redis pipelines in parallel using the python multiprocessing module to increase redis throughput?\n", " I'm not sure what you mean by \"pipeline\" but if you want to do something like this, you can use the multiproc module.\n", "In order to use it with python's multiprocessing model you will need to create a new connection in each subprocess to ensure each process has it's own connection. Otherwise you can run into contention issues on the client side.\nThat said, if there are commands you need to run as a transaction you will want to use multi/exec yourself as pipelining is not the same thing and does not call it. The simplest way with py-redis is by setting the transaction flag to True when calling pipeline. But only do this if you really need every other client to wait for that pipeline to finish executing. If you do that you've essentially made your application non-threaded as it works like a lock on the database - all other clients can't operate on the database while a MULTI/EXEC is in play.\nIf you must use MULTI/EXEC and still want the concurrency you will need to isolate groups of keys on different servers and run a server per connection needing to lock the DB. If your operations are on keys which have overlap in various processes, this will require either accepting the effects of MULTI/EXEC on the overall performance or redesigning the client code to eliminate the contention.\n", 0.8541849851608276, 0.8034459352493286, 0.8280389308929443, 0.13389121338912136, 1.2573683499841106e-231, 119], ["Why does Queue.Queue have a task_done method while multiprocessing.Queue has no such method?\n", " Queues are designed to be used in a multi-threaded environment.  Queuing is a way to make sure that the tasks are done when the thread that is calling the Queued method is done.\n", "My guess would be: multithreading module was implemented very early, multiprocessing module came in 2.6 version.  \nThe queue design was slightly corrected for multiprocessing and offers better flexibility than the multithreading, because you can choose between Queue, SimpleQueue and JoinableQueue depending on your use cases (speed vs reliability).  \nNow modifing  multithreading like this would have caused backwards incompatibility, since join and task_done methods would have to be removed. Imagine the code needed to be refactored, new tests had to be written, API broken - for me clearly no benefits.\n", 0.8525968790054321, 0.8125381469726562, 0.8320856690406799, 0.12903225806451613, 1.1137303841177933e-231, 179], ["is integer comparison in Python constant time? Can I use it to compare a user-provided int token with a server-stored int for crypto in the way I would compare strings with constant_time_compare from django.utils.crypto, i.e. without suffering timing attacks?\nAlternatively, is it more secure to convert to a string and then use the above function?\n", " Yes, it is constant-time.\n", "The answer is yes for a given size of integer - by default python integers that get big become long and then have potentially infinite length - the  compare time then grows with the size. If you restrict the size of the integer to a ctypes.c_uint64 or ctypes.c_uint32 this will not be the case.\nNote that compare with 0 is a special case, normally much faster, due to the hardware actions many CPUs have a special flag for 0, but if you are using/allowing seeds or tokens with a values of 0 you are asking for trouble.\n", 0.8919594883918762, 0.8019008636474609, 0.8445360660552979, 0.05714285714285714, 1.5459640530794076e-231, 27], ["I am writing a program that should spit out a random sentence of a complexity of my choosing. As a concrete example, I would like to aid my language learning by spitting out valid sentences of a grammar structure and using words that I have already learned. I would like to use python and nltk to do this, although I am open to other ideas.\nIt seems like there are a couple of approaches:\n\nDefine a grammar file that uses the grammar and lexicon I know about, and then generate all valid sentences from this list, then selecting a random answer.\nLoad in corpora to train ngrams, which then can be used to construct a sentence. \n\nAm I thinking about this correctly? Is one approach preferred over the other? Any tips are appreciated. Thanks!\n", " You can use a corpus to help you train a n-gram model. You'll need to load the corpus, but you can load it in a variety of ways. For example:  \n\n  1. Use a dictionary to store the words in the corporal. This will be a list of strings. The corpus will have a word for each word in your dictionary. So you'll have something like this:\n\n word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12 word13 word14 word15 word16 word17 word18 word19 word20 word21 word22 word23 word24 word25 word26 word27 word28 word29 word30 word31 word32 word33 word34 word35 word36 word37 word38 word39 word40 word41 word42 word43 word44 word45 word46 word47 word48 word49 word50 word51 word52 word53 word54 word55 word56 word57 word58 word59 word60 word61 word62 word63 word64 word65 word66 word67 word68 word69 word70 word71 word72 word73 word74 word75 word76 word77 word78 word79 word80 word81 word82 word83 word84 word85 word86 word87 word88 word89 word90 word91 word92 word93 word94 word95 word96 word97 word98 word99 word100 word101 word102 word103 word104 word105 word106 word107 word108 word109 word110 word111 word112 word113 word114 word115 word116 word117 word118 word119 word120 word121 word122 word123 word124 word125 word126 word127 word128 word129 word130 word131 word132 word133 word134 word135 word136 word137 word138 word139 word140 word141 word142 word143 word144 word145 word146 word147 word148 word149 word150 word151 word152 word153 word154 word155 word156 word157 word158 word159 word160 word161 word162 word163 word164 word165 word166 word167 word168 word169 word170 word171 word172 word173", "If I'm getting it right and if the purpose is to test yourself on the vocabulary you already have learned, then another approach could be taken:\nInstead of going through the difficult labor of NLG (Natural Language Generation), you could create a search program that goes online, reads news feeds or even simply Wikipedia,  and finds sentences with only the words you have defined.\nIn any case, for what you want, you will have to create lists of words that you have learned. You could then create search algorithms for sentences that contain only / nearly only these words.\nThat would have the major advantage of testing yourself on real sentences, as opposed to artificially-constructed ones (which are likely to sound not quite right in a number of cases).\nAn app like this would actually be a great help for learning a foreign language. If you did it nicely I'm sure a lot of people would benefit from using it.\n", 0.6880307197570801, 0.8211504220962524, 0.7487195134162903, 0.15841584158415842, 6.785390742171694e-232, 1611], ["Does a linked list have any value in a language which has dynamic arrays, such as Python (which, of course, has the list data structure)?\nI currently understand a list in python to really just be a static array which, as more data is inserted, redefines itself as a new array (larger in size) copying the data from the old into the new (thus making it dynamic). is this correct?\nI also understand how a list and linked list stores data in memory differently (lists in a contiguous manner and linked lists in a non-contiguous manner), but does this provide any major advantages?\n", " Lists in Python are not arrays. They are a data type.\n", "Yes, it does. Removing a link from a linked list is O(1), while it is linear for a dynamic array.\nSuppose you want to build a data structure for an LRU. Typically, you would have a hash-table for \"touch\" operations, plus a sequence array to see what has aged. When an item is accessed, the hash table finds it in the sequence, and moves it to the end. If an item needs to be evicted, then the first item in the sequence is used to find the item in the hash table, which is then removed.\nIn this example, using a linked-list for the sequence operation means that everything works in (expected) O(1) time. With dynamic vectors, everything would be linear. Linked lists still have their uses.\n", 0.8896633386611938, 0.81390780210495, 0.8501012325286865, 0.05633802816901408, 1.3584069082388692e-231, 55], ["Is there any way to determine where a class is coming from in python (especially sklearn)?  I want to determine if a class is from sklearn.linear_models or sklearn.ensemble.  \nAs an example, I would like to be able to determine if Ridge() is a member of sklearn.linear_model.\nThe fit function is a bit different depending on the model so formulas fed to each via patsy need to be different.  \n", " You can use the skLearn.__class__.get_full_name() method to get the class name. This will return the full name of the object. You could then use this to check if the name is in the list of classes that are in skl.models.\n\n", "Use the __module__ attribute, i.e.: Ridge.__module__\nIf you want to know it from an instance of the class: inst.__class__.__module__\nIf you need the module object (not just the name as string): sys.modules[Ridge.__module__]\n", 0.8636237382888794, 0.8544537425041199, 0.8590143322944641, 0.3333333333333333, 1.0645729939000923e-231, 223], ["If I'm scheduling IO bound task in celery and if my server spec was like Quad Core with 8GB RAM, How many workers and concurrency I can use.\nIf CPU bound processes are advised to use 4 workers and 8 concurrency for Quad Core processor. Whats  the spec for IO bound process.\nIn my task I will be performing API calls, manipulating the received data and storing the processed data in server.\n", " I think you're misunderstanding the concept of concurreny. \nConcurrency is the ability to execute tasks concurrently. If you have a task that is CPU-bound, you can execute it in parallel with other tasks. However, if you want to run a CPU task concurrently with IO tasks, then you need to have the IO task execute in a separate process, and the CPU tasks execute on the same machine. This is called \"threading\" and is a fundamental part of the operating system. The operating systems of today use a variety of techniques to make this possible. For example, the OS can make the threads appear to be separate processes, but they're really all running on a single core. Another technique is to allow the tasks to share a common pool of memory, so that the task can be executed in the context of a different process without the overhead of having to copy the data. In this case, each task is given a unique ID, which is used to determine which task to start. Each task has a \"handle\" that allows it to communicate with the other task. When the handle changes, it signals the new task, allowing it the opportunity to finish. A common example of this is when a process is writing to a file, where the process can signal the file to write to, without having the write operation complete. Other common examples are when the processes need a resource, such as a database connection, or a socket, that needs to remain open. These tasks can all be started in separate threads, with each thread having its own handle. Thus, a thread can start a new process that has no knowledge of what the previous process was doing. It can then start the next task on its behalf, knowing that it will finish when it is done. There are many other techniques, all of which are used in modern operating-systems. I won't go into detail on all the techniques that are available, as I don't know what your operating environment is. But I'll give you a few examples. One of my favorite is \"pipe\", which allows you to send data from one process to another, while still allowing the receiving process the chance to process it. Pipe is very common in operating environments that use the \"event loop\" model, in which the kernel keeps track of events that occur, including the events of interest to the application. Events are sent to processes by the system, using a pipe. Once the event is detected, an event loop is started, sending the appropriate events", "That's like asking 'how long is a piece of string' and I'm sure there isn't a single simple answer. Certainly it will be more than 8 threads, with a useful upper limit at the maximum concurrent I/O tasks needed, maybe determined by the number of remote users of your service that the I/O tasks are communicating with. Presumably at some number of tasks 'manipulating the data' will start to load up your processor and you won't be i/o bound any more.\n", 0.8018485307693481, 0.8221633434295654, 0.8118789196014404, 0.16257088846880907, 6.224982160432165e-232, 2421], ["I am trying to arm it using Dronkit-Python and I am able to get it to arm properly through code, however, it requires the controller to be on.\nIs there anyway to bypass this?\n", " You can use the command \"arm\" to do this. \n", "Assuming you mean the RC Controller (not the flight controller), you'll need to turn off the radio failsafe (FS_THR_ENABLE).\n", 0.8903765678405762, 0.8309610486030579, 0.8596433997154236, 0.1935483870967742, 1.4147351699132998e-231, 44], ["There are many posts where I got to use the case of __init__.py and use to define global variable is one of them.\nBut I want to know is there any demerit in defining a local variable in __init__.py ?\n", "I think it's a bad practice to have a global function in the __main__ module. It's not a good practice, and it will make your code hard to maintain. If you really need to do this, you should create a class that has a __getattr__ method that returns the name of the attribute that you want. Then you can use that method to get the value of that attribute. This way, the code is much more maintainable. \n", "Using global variables has its problems in general:\n\nPassing variables as parameters to functions e.g. more flexible and readable than having functions access globals.\nIn case of a network of connected objects, it's usually more flexible for objects to have members referring to other objects then for them to access other objects using globals.\nHaving a module export classes rather than instances allows you to have multiple instances of a class rather than one.\n\nHaving said that, it's my experience that in programming there are no dogma's. A well known lemma in algorithm design is that from each resource (thing in your program) there maybe zero, one or infinitely many. If you typically can have only one instance of an object and it doesn't change identity, then exporting an instance from a module (so defining it in the modules __init__.py) is fine.\nIt's just that at the start of design you will sometimes assume that a certain resource is unique, but later on it turns out that you'll have multiple.\nA typical application of exporting an variable rather than a type is when it's in fact a constant. A good example of this is math.pi, that doesn't tend to change very often...\nNote that since in Python everything is an object, the words 'variable' and 'instance' are used interchangeably here. In addition to that, functions and classes are (usually singleton) objects in their own right in Python.\n", 0.8608171939849854, 0.8133985996246338, 0.83643639087677, 0.26911314984709483, 9.441724190350838e-232, 402], ["I'm following the exact directions of getting started with pinax-project-account. You can see them [here][1]. I just created my virtual environment and installed the requirements. The problem with when I run this command: django-admin.py startproject --template=https://github.com/pinax/pinax-project-account/zipball/master.  I get this error:\nCommandError: couldn't extract file /tmp/django_project_template_e1ulrY_downl\noad/master.html to /tmp/django_project_template_wU3ju6_extract: Path not a re\ncognized archive format: /tmp/django_project_template_e1ulrY_download/master.\nhtml\nI can get this working on my local machine but I'm using python anywhere and it doesn't seem to like this command? \nAny ideas?\n", " You need to specify the path to the zip file. For example, if you're using a virtualenv, you can specify a path like /path/to/project/templates/myproject.zip. If you are using the command line, it's probably not going to work. Try using virtualenvs, or just use the project directory. Also, make sure you have the correct permissions on the directory you want to extract. I think you need the permissions for the virtual directory, but not the actual project.\n\n", "The .html at the end of /tmp/django_project_template_e1ulrY_download/master.html seems suspect to me. I'm guessing that you got an error html page instead of the archive you requested. Check the contents of that file to see what happened.\n", 0.8468686938285828, 0.8464574217796326, 0.8466629981994629, 0.17322834645669294, 9.196617910709463e-232, 462], ["I am working for a company who wants me to test and cover every piece of code I have. \nMy code works properly from browser. There is no error no fault.\nExcept my code works properly on browser and my system is responding properly do I need to do testing? Is it compulsory to do testing?\n", " You don't need testing. You need a test plan. A test is a plan to make sure that the code you are testing is correct. If you have a good test, you will be able to find any problems in your code. Having a bad test will make you waste time and money. It will also make your boss think that you don\u2019t know what you doing. So, if you want to be a great programmer, make a list of all the tests you need and make them a part of your test plans. Then, when you find a problem, fix it. This will save you time, money and effort. Also, it will help you to learn faster. Learning is the key to a successful career. Good luck!\n", "Whether it\u2019s compulsory depends on organization you work for. If others say it is, then it is. Just check how tests are normally written in the company and follow existing examples.\n(There\u2019re a lot of ways Django-based website can be tested, different companies do it differently.)\nWhy write tests?\n\nRegression testing. You checked that your code is working, does it still work now? You or someone else may change something and break your code at some point. Running test suite makes sure that what was written yesterday still works today; that the bug fixed last week wasn\u2019t accidentally re-introduced; that things don\u2019t regress.\nElegant code structuring. Writing tests for your code forces you to write code in certain way. For example, if you must test a long 140-line function definition, you\u2019ll realize it\u2019s much easier to split it into smaller units and test them separately. Often when a program is easy to test it\u2019s an indicator that it was written well.\nUnderstanding. Writing tests helps you understand what are the requirements for your code. Properly written tests will also help new developers understand what the code does and why. (Sometimes documentation doesn\u2019t cover everything.)\nAutomated tests can test your code under many different conditions quickly, sometimes it\u2019s not humanly possible to test everything by hand each time new feature is added.\nIf there\u2019s the culture of writing tests in the organization, it\u2019s important that everyone follows it without exceptions. Otherwise people would start slacking and skipping tests, which would cause regressions and errors later on.\n\n", 0.8548282384872437, 0.8305224180221558, 0.8425000905990601, 0.2957393483709273, 8.757692451095618e-232, 618], ["I have a somewhat intricate project setup consisting of several components that work together. Each component is a separate Python project that is hosted as a uWSGI application behind an Nginx proxy. The components interact with each other and with the outside world through the proxy.\nI noticed myself about to cut-and-paste some code from one component to another, as they perform similar functions, but interact with different services. Obviously, I want to avoid this, so I am going to pull out common functionality and put it into a separate 'library' project, to be referenced by the different components.\nI am running these apps in a virtual environment (using virtualenv), so it should theoretically be easy to simple drop the library project into .env/includes.\nHowever, I have a bit of a strange setup. First of all, I am running the project from /var/www (i.e. uWSGI hosts the apps from here), but the projects actually are present in another source controlled directory. For various reasons, I don't want to move them, so I created symlinks for the project directories in /var/www. This works fine. However, now I have a potential problem, namely, where do I put the library project (which is currently in the same directory as the other components), which I also want to symlink?\nDo I symlink it in .env/includes? And if so, how should I reference the library from my other components? Do I reference it from sys.path or as a sibling directory? Is Nginx/uWSGI with virtualenv following the symlinks and taking into account the actual directory or is it blindly assuming that everything is in /var/www?\nI have not tried either approach because there seems to be a massive scope for problems, so I wanted to get some input first. Needless to say, I am more than a little confused.\n", "ou can sylink the libraries in your environment. \nFor example, if you have the following directory structure:\nproject1\n   lib\nindex.html\nlib/\nbin/  # this is the sylink\ntest.py\netc. etc.\n\n", "I solved the problem quite easily by symlinking the package of interest in .env/lib/python2.7/site-packages. I originally tried to symlink the entire project folder but that didn't work as it couldn't find the package.\nIt seems that my uWSGI/Nginx just follows the virtualenv's version of pythonpath, so whatever I configure there is used.\nIt will be a bit of a pain to have to remember to symlink every package, but at least I only have to do it once for each package.\nI'm using PyDev, and it was masking the issue because I was using the default Python interpreter, not the one in virtualenv. Once I changed that, it was easier to solve.\n", 0.8310983180999756, 0.821707010269165, 0.8263760209083557, 0.10457516339869281, 1.141743008095917e-231, 188], ["I'm fairly sure this is a duplicate, but suppose I have a pandas DataFrame and I've sorted the rows based on the values of some column.  Originally the indices were the integers 0, 1, \u2026, n-1 but now they're out of order.  How do I reassign these indices to be in the proper order for the new sorted DataFrame?\n", " You can use the sorted method to re-sort the DataFrames.\n", "df.reset_index(drop=True, inplace=True)\n", 0.8614805340766907, 0.8092231750488281, 0.8345346450805664, 0.0, 1.253537017077218e-231, 58], ["I'm learning about Python's boolean logic and how you can shorten things down.  Are the two expressions in the title equivalent?  If not, what are the differences between them?\n", " The two are not equivalent.\nThe first expression is a bit shorter, but it's not shorter than the second. The second expression has a lot more parentheses, and it has more braces. It's a little bit more complex, so it takes a while to get used to. But it is shorter. And it works. :)\n", "number % 2\nis equal to (shorthand for) \nnumber % 2 != 0 \nbecause 1 evaluates to True and 0 to False.\n", 0.8239548802375793, 0.796773374080658, 0.8101361393928528, 0.10666666666666667, 9.718756419071616e-232, 284], ["So I have some code that attempts to find a resource on HDFS...if it is not there it will calculate the contents of that file, then write it.  And next time it goes to be accessed the reader can just look at the file.  This is to prevent expensive recalculation of certain functions\nHowever...I have several processes running at the same time on different machines on the same cluster.  I SUSPECT that they are trying to access the same resource and I'm hitting a race condition that leads a lot of errors where I either can't open a file or a file exists but can't be read.\nHopefully this timeline will demonstrate what I believe my issue to be\n\nProcess A goes to access resource X\nProcess A finds resource X exists and begins writing\nProcess B goes to access resource X\nProcess A finishes writing resource X\n...and so on\n\nObviously I would want Process B to wait for Process A to be done with Resource X and simply read it when A is done.\nSomething like semaphores come to mind but I am unaware of how to use these across different python processes on separate processors looking at the same HDFS location.  Any help would be greatly appreciated\nUPDATE: To be clear..process A and process B will end up calculating the exact same output (i.e. the same filename, with the same contents, to the same location).  Ideally, B shouldn't have to calculate it.  B would wait for A to calculate it, then read the output once A is done.  Essentially this whole process is working like a \"long term cache\" using HDFS.  Where a given function will have an output signature.  Any process that wants the output of a function, will first determine the output signature (this is basically a hash of some function parameters, inputs, etc.).  It will then check the HDFS to see if it is there.  If it's not...it will write calculate it and write it to the HDFS so that other processes can also read it.\n", " I think you're looking for a lock. I don't know what you mean by \"processes on a different machine on same machine cluster\" but if you want to lock a shared resource, you can use a semaaphore. You can lock the resource in a thread, and then unlock it in another thread. This will prevent the other threads from accessing the shared resources. If you need to do this on multiple machines, I recommend using a mutex. The lock will be released when the thread that locked the lock is finished. In your case, the semanage will lock resource A, while the mutange will unlock resource B. Then, when you lock A again, it won't allow B access to resource x. Once you unlock B, resource y will become available. So, if B is locked, A will not be able to", "(Setting aside that it sounds like HDFS might not be the right solution for your use case, I'll assume you can't switch to something else. If you can, take a look at Redis, or memcached.)\nIt seems like this is the kind of thing where you should have a single service that's responsible for computing/caching these results. That way all your processes will have to do is request that the resource be created if it's not already. If it's not already computed, the service will compute it; once it's been computed (or if it already was), either a signal saying the resource is available, or even just the resource itself, is returned to your process.\nIf for some reason you can't do that, you could try using HDFS for synchronization. For example, you could try creating the resource with a sentinel value inside which signals that process A is currently building this file. Meanwhile process A could be computing the value and writing it to a temporary resource; once it's finished, it could just move the temporary resource over the sentinel resource. It's clunky and hackish, and you should try to avoid it, but it's an option.\nYou say you want to avoid expensive recalculations, but if process B is waiting for process A to compute the resource, why can't process B (and C and D) be computing it as well for itself/themselves? If this is okay with you, then in the event that a resource doesn't already exist, you could just have each process start computing and writing to a temporary file, then move the file to the resource location. Hopefully moves are atomic, so one of them will cleanly win; it doesn't matter which if they're all identical. Once it's there, it'll be available in the future. This does involve the possibility of multiple processes sending the same data to the HDFS cluster at the same time, so it's not the most efficient, but how bad it is depends on your use case. You can lessen the inefficiency by, for example, checking after computation and before upload to the HDFS whether someone else has created the resource since you last looked; if so, there's no need to even create the temporary resource.\nTLDR: You can do it with just HDFS, but it would be better to have a service that manages it for you, and it would probably be even better not to use HDFS for this (though you still would possibly want a service to handle it for you, even if you're using Redis or memcached; it depends, once again, on your particular use case).\n", 0.8417755961418152, 0.8109530806541443, 0.8260769248008728, 0.28192371475953565, 8.420521413912345e-232, 745], ["The title says it all. For example 1<2<3 returns True and 2<3<1 returns False.  \nIt's great that it works, but I can't explain why it works... I can't find anything about it in the documentation. It's always: expression boolean_operator expression, not two boolean operators). Also: a<b returns a boolean, and boolean boolean_operator expression does not explain the behaviour. \nI'm sure the explanation is (almost) obvious, but I seem to miss it. \n", " It is because the order of the operators is important.\n", "Your multiple operators all have the same precedence, so now it is going to work through them serially. 1<2<3 goes to 1<2 which is T, then 2<3 is T. 2<3<1 has two parts, 2<3 is T, but 3<1 is F so the entire expression evaluates to F.\n", 0.8786764144897461, 0.8027629852294922, 0.8390060067176819, 0.1846153846153846, 1.3717641321977202e-231, 56], ["I'm very confused as to what np.exp() actually does. In the documentation it says that it: \"Calculates the exponential of all elements in the input array.\" I'm confused as to what exactly this means. Could someone give me more information, or even point me to the right direction to learn more.\nThanks a lot\n", " np exp(a) returns the value of a raised to a power. \nIn other words, it returns a number that is the product of the elements of array a multiplied by the power of that element. For example, if a is an array of integers, np(2*a).  \n\n", "It calculates ex for each x in your list.\n", 0.8183110356330872, 0.8703976273536682, 0.8435510396957397, 0.10526315789473684, 9.735488397697274e-232, 233], ["I'm very confused as to what np.exp() actually does. In the documentation it says that it: \"Calculates the exponential of all elements in the input array.\" I'm confused as to what exactly this means. Could someone give me more information, or even point me to the right direction to learn more.\nThanks a lot\n", " np exp(a) returns the value of a raised to a power. \nIn other words, it returns a number that is the product of the elements of array a multiplied by the power of that element. For example, if a is an array of integers, np(2*a).  \n\n", "The exponential function is e^x where e is a mathematical constant called Euler's number, approximately 2.718281. This value has a close mathematical relationship with pi and the slope of the curve e^x is equal to its value at every point. np.exp() calculates e^x for each value of x in your input array.\n", 0.8434160947799683, 0.8383541107177734, 0.8408775329589844, 0.339622641509434, 1.0726520105754455e-231, 233], ["I was going through the documentation of spark. I got a bit confused with rdd.repartition() function and the number of partitions we pass during context initialization in sc.parallelize().\nI have 4 cores on my machine, if I sc.parallelize(data, 4) everything works fine, but when I rdd.repartition(4) and apply rdd.mappartitions(fun) sometimes the partitions has no data and my function fails in such cases.\nSo, just wanted to understand what is the difference between these two ways of partitioning.\n", " The difference is that the first one is a blocking call, while the second one will be non-blocking. The first will return a Future, which is nonblocking, and will not block the calling thread. \n", "By calling repartition(N) spark will do a shuffle to change the number of partitions (and will by default result in a HashPartitioner with that number of partitions). When you call sc.parallelize with a desired number of partitions it splits your data (more or less) equally up amongst the slices (effectively similar to a range partitioner), you can see this in ParallelCollectionRDD inside of the slice function.\nThat being said, it is possible that both of these sc.parallelize(data, N) and rdd.reparitition(N) (and really almost any form of reading in data) can result in RDDs with empty partitions (its a pretty common source of errors with mapPartitions code so I biased the RDD generator in spark-testing-base to create RDDs with empty partitions). A really simple fix for most functions is just checking if you've been passed in an empty iterator and just returning an empty iterator in that case.\n", 0.8503332138061523, 0.8017369508743286, 0.8253203630447388, 0.14659685863874347, 1.0790771125155735e-231, 195], ["I need to install cx_Oracle, and when I open up the .exe, it doesn't find my conda installation because it isn't in the registry. How do I...\n\nAdd to registry or\nSpecify a path\n\nThanks!\n", "You can add the cx-Oracle installation to the Windows registry by using the following command:\nreg add \"HKCU\\Software\\Classes\\CLSID{4D4E4F4B-4A4C-11D2-B9B4-00805FBC2BEB}\" /v cxOracle /t REG_EXPAND_SZ /d \"C:\\Program Files\\Conda\\bin\\cx_oracle\"\n", "The relevant registry key is in (python 2.7 on windows xp, others version might be similar) HKEY_LOCAL_MACHINE\\SOFTWARE\\Python\\PythonCore\\2.7\\InstallPath (if it is not there, create the key). Change it so that the anaconda Python is the default python installation.\nThen you just run that exe installer of cx_Oracle. Then the installer should be able to locate python installation from registry.\n", 0.7546188831329346, 0.8321678638458252, 0.7914983630180359, 0.2018348623853211, 1.1688275677693198e-231, 242], ["Currently, I am trying to implement a basic logistic regression algorithm in Python to differentiate between A vs. B.\nFor my training and test data, I have ~50,000 samples of A vs. 1000 samples of B. Is this a problem if I use half the data of each to train the algorithm and the other half as testing data (25000 train A, 500 train B and so on for testing accuracy).\nIf so, how can I overcome this problem. Should I consider resampling, doing some other \"fancy stuff\".\n", " You can use a different data set for training, and a separate data for the testing. \n", "How much of a problem it is depends on the nature of your data. The bigger issue will be that you simply have a huge class imbalance (50 As for every B). If you end up getting good classification accuracy anyway, then fine - nothing to do. What to do next depends on your data and the nature of the problem and what is acceptable in a solution. There really isn't a dead set \"do this\" answer for this question. \n", 0.8659794330596924, 0.8304805755615234, 0.8478586077690125, 0.20833333333333334, 1.2490278180519265e-231, 86], ["I am doing a mini-project on Web-Crawler+Search-Engine. I already know how to scrape data using Scrapy framework. Now I want to do indexing. For that I figured out Python dictionary is the best option for me. I want mapping to be like name/title of an object (a string) -> the object itself (a Python object).\nNow the problem is that I don't know how to store dynamic dict in MySQL database and I definitely want to store this dict as it is!\nSome commands on how to go about doing that would be very much appreciated!\n", " You can use a Python dict to represent your data.  You will need to use the MySQL module to get the data into a MySQL table.\n", "If you want to store dynamic data in a database, here are a few options.  It really depends on what you need out of this.\nFirst, you could go with a NoSQL solution, like MongoDB.  NoSQL allows you to store unstructured data in a database without an explicit data schema.  It's a pretty big topic, with far better guides/information than I could provide you.  NoSQL may not be suited to the rest of your project, though.\nSecond, if possible, you could switch to PostgreSQL, and use it's HSTORE column (unavailable in MySQL).  The HSTORE column is designed to store a bunch of Key/Value pairs.  This column types supports BTREE, GIST, GIN, and HASH indexing.  You're going to need to ensure you're familiar with PostgreSQL, and how it differs from MySQL.  Some of your other SQL may no longer work as you'd expect.\nThird, you can serialize the data, then store the serialized entity.  Both json and pickle come to mind.  The viability and reliability of this will of course depend on how complicated your dictionaries are.  Serializing data, especially with pickle can be dangerous, so ensure you're familiar with how it works from a security perspective.  \nFourth, use an \"Entity-Attribute-Value\" table.  This mimics a dictionaries \"Key/Value\" pairing.  You, essentially, create a new table with three columns of \"Related_Object_ID\", \"Attribute\", \"Value\".  You lose a lot of object metadata you'd normally get in a table, and SQL queries can become much more complicated. \nAny of these options can be a double edged sword.  Make sure you've read up on the downfalls of whatever option you want to go with, or, in looking into the options more, perhaps you'll find something that better suits you and your project.\n", 0.8690126538276672, 0.7958652973175049, 0.8308321237564087, 0.1345565749235474, 1.2508498911928379e-231, 126], ["Everything in Python is an object, and almost everything has attributes and methods. Now, according to Object Oriented Programming, every object created in Python must be an instance of a common parent class. However, this logic just doesn't make sense to me.\nCan someone clear this up for me?\n", " Objects are not objects in the sense that they are instances of other objects. Rather, they're instances in that the methods and attributes of the object are the same as those of its parent. This is the way that Python works. Objects can be created, but they can't be instances. \n", "\naccording to Object Oriented Programming, every object created in Python must be an instance of a common parent class\n\nThis is not true. It happens that, in Objective-C, Java (and maybe C# too?), things tend to derive from a single superclass, but this is an implementation detail - not a fundamental of OO design.\nOO design just needs a common-enough method to find the implementation of a method you wish to call on the object on which you wish to call it. This is usually fundamental to how the language works (C++, C#, Java, Objective-C, Python, etc all do it their own way that makes sense for their language).\nIn C++, this is done for static types by the linker and for dynamic types (through virtual inheritance) by a vector table -- no need for a common base class.\nIn Objective-C, this is done by looking up something in a hash-map on the object's class's structure, then calling a specific method to get the signature of the desired method. This code is nuanced, so everything generally derives from a single, common base-class.\nPython technically shouldn't require this, but I think they've made an implementation choice to make everything be a class and every class derive from a common base class.\n", 0.8560653328895569, 0.8070740103721619, 0.8308480978012085, 0.20740740740740743, 1.0235787270834931e-231, 281], ["So I'm beginning to learn Python and I'm using powershell & Notepad++. The issue I'm having is I noticed when I'm making programs, I get a lot of indentation errors and then I have to go and convert all tabs to spaces, which solves the issue.\nMy question is, is there anyway to get around this? For instance when you hit tab, it just inserts 4 spaces instead? Because it's really annoying having to consistently do this. I have errors all the time when I have tabs instead of spaces.\nAny help is appreciated, thanks guys!\n", " I think you're looking for the \\t character.  It's a tab character, so it won't affect the indenting.\n\n", "There's such an option.\nSettings > Preferences > Tab Settings > Replace by space.\n", 0.8513680100440979, 0.8444973826408386, 0.8479187488555908, 0.12121212121212123, 1.2064448882743937e-231, 104], ["I have class imbalance problem and want to solve this using cost sensitive learning. \n\nunder sample and over sample \ngive weights to class to use a modified loss function \n\nQuestion \nScikit learn has 2 options called class weights and sample weights. Is sample weight actually doing option 2) and class weight options 1). Is option 2) the the recommended way of handling class imbalance. \n", " Sample weights are not the same as class weighted loss functions. Sample weight is the proportion of the data that is used for training a class. Class weighted losses are the loss for a given class when the class is not used.\n", "sample_weight and class_weight have a similar function, that is to make your estimator pay more attention to some samples. \nActual sample weights will be sample_weight * weights from class_weight.\nThis serves the same purpose as under/oversampling but the behavior is likely to be different: say you have an algorithm that randomly picks samples (like in random forests), it matters whether you oversampled or not.\nTo sum it up:\nclass_weight and sample_weight both do 2), option 2) is one way to handle class imbalance. I don't know of an universally recommended way, I would try 1), 2) and 1) + 2) on your specific problem to see what works best.\n", 0.861513614654541, 0.8096119165420532, 0.8347567915916443, 0.24203821656050953, 1.016500342875109e-231, 227], ["A Python module is just a .py source file. A Python package is simply a collection of modules.\nSo why do we need programs such as pip to 'install' Python modules? Why not just download the files, put them in our project's folder and import them?\nWhat exactly does it mean to 'install' a module or a package? And what exactly does pip do?\nAre things different on Windows and on Linux?\n", "Python is a dynamic language. It's not a static language like C++. Python's modules are not installed in the same way as a library. For example, on a Windows machine, you can't just copy a Python library into your project folder. You have to compile it. On a Linux machine you have a different way of doing this. The way you do this on the Linux machines is to use the python-dev package. This package contains a Makefile that you use to build the library for you. Then you just import the module into the project. \n", "\nSo why do we need programs such as pip to 'install' Python modules? Why not just download the files, put them in our project's folder and import them?\n\nIt's just meant to facilitate the installation of softwares without having to bundle all the dependencies nor ask the user to download the files.\nYou can type pip install mysoftware and that will also install the required dependencies. You can also upgrade a software easily.\n\nWhat exactly does it mean to 'install' a module or a package? And what exactly does pip do?\n\nIt will copy the files in a directory that is in your Python path. This way you will be able to import the package without having to copy the directory in your project.\n", 0.8624743223190308, 0.8510571718215942, 0.856727659702301, 0.39823008849557523, 9.230295980288585e-232, 516], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " You need a migration for the new version.\n", "Delete all the migration folder from your app and delete the database then migrate your database......\nif this does not work delete django_migration table from database and add the \"name\" column in django_content_type table ALTER TABLE django_content_type ADD COLUMN name character varying(50) NOT NULL DEFAULT 'anyName'; and then run $ python manage.py migrate --fake-initial\n", 0.8999972939491272, 0.7959372401237488, 0.8447747826576233, 0.05797101449275363, 1.4853527680635705e-231, 43], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " You need a migration for the new version.\n", "Well, I found the issue. I have auditlog installed as one my apps. I removed it and migrate works fine.\n", 0.9003623127937317, 0.8701902627944946, 0.8850191831588745, 0.07142857142857144, 1.4853527680635705e-231, 43], ["I just upgraded my django from 1.7.1 to 1.8.4. I tried to run python manage.py migrate but I got this error:\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" does not exist\nI dropped my database, created a new one, and ran the command again. But I get the same error. Am I missing something? Do I need to do something for upgrading my django?\nEDIT:\nI downgraded back to 1.7.1 and it works. Is there a way to fix it for 1.8.4?\n", " You need a migration for the new version.\n", "Here's what I found/did. I am using django 1.8.13 and python 2.7. The problem did not occur for Sqlite. It did occur for PostgreSQL. \nI have an app the uses a GenericForeignKey (which relies on Contenttypes). I have another app that has a model that is linked to the first app via the GenericForeignKey. If I run makemigrations for both these apps, then migrate works.\n", 0.8947794437408447, 0.8217426538467407, 0.8567072153091431, 0.07692307692307691, 1.4853527680635705e-231, 43], ["I am looking for a way to limit how a python file to be called. Basically I only want it to be executable when I call it from a bash script but if ran directly either from a terminal or any other way I do not want it to be able to run. I am not sure if there is a way to do this or not but I figured I would give it a shot.\n", " You can use the -c option to the python command to specify a command that will be run when the file is called, and you can specify the command in the.py file itself. For example, if you want to call a script named myscript.py, you could do:\npython myfile.txt -i mycommand.ini\n", "There is no meaningful way to do this.\nUNIX process architecture does not work this way. You cannot control the execution of a script by its parent process. \nInstead we should discuss why you want to do something like this, you are probably thinking doing it in a wrong way and what good options there is to address the actual underlying problem.\n", 0.8325034379959106, 0.8389219045639038, 0.8357003927230835, 0.27350427350427353, 9.985582024868538e-232, 277], ["I'm trying to send a cross domain PUT request from AngularJS frontend to Django backend. It's all fine when I'm running on the same domain (frontend at localhost:8000 and backend at localhost:8001), I'm getting my csrftoken from $cookies and can send a successful request. The problem begins when I switch the backend to an external QA server. I get empty $cookies, no sessionid nor csrftoken cookies at all. I ran out of ideas and that's why I'm asking for help here, thanks in advance.\n", " The issue is that you're not sending the proper headers. You're sending a PATCH request, but you need to set the PORT and the COOKIE_DOMAIN. Also, you should set a CORS policy on your front end. \n", "You won't be able to get cookies on other domain because all cookies are set per domain, this is for security reasons.\nIf you want to access session and cookies in other domain, you must copy them. You can do it by sending some request with special token (for validation) and create view in django that will fetch data from some storage, based on that token and populate user cookies, so on next request they will be available.\n", 0.8612692356109619, 0.8412109613418579, 0.8511219620704651, 0.22033898305084745, 1.0980828380673523e-231, 197], ["I'm using Tkinter to create a window in python and a canvas to display graphics in the window. This is working fine so far.\nBut I have a two dimensional list containing colours that I would like to directly place on the canvas.\nExample\nI have a class defined (named CRGB) that has three variables: r, g and b. These are the red, green and blue values of a colour, and are integers between 0 and 255.\nI also have a two-dimensional list, which contains CRGB objects with the colour data.\nI then have a Canvas (defined in a variable called screenCanvas) which is the same size as the 2D list.\nHow would I transfer the pixels from the 2D list to the canvas?\nNotes: I would like the code to work on Mac AND Windows, and not use any external libraries (libraries not included in Python by default.)\n", " You can use the Tcl/Tk bindings to do this.  You'll need to use a Ttk object to get the data from your list and then use that data to draw on your canvas, but it's not too hard.\n\n", "The canvas has no way to draw an individual pixel, except to draw a line that is exactly one pixel long and one pixel wide.\nIf you only need to place pixels, and don't need the other features of the canvas widget, you can use a PhotoImage object. An instance of PhotoImage has methods for setting individual pixels.\n", 0.8710045218467712, 0.8562101721763611, 0.8635439872741699, 0.32, 1.1337861261109773e-231, 180], ["I have been programming in python for about two years; mostly data stuff (pandas, mpl, numpy), but also automation scripts and small web apps.  I'm trying to become a better programmer and increase my python knowledge and one of the things that bothers me is that I have never used a class (outside of copying random flask code for small web apps).  I generally understand what they are, but I can't seem to wrap my head around why I would need them over a simple function.  \nTo add specificity to my question: I write tons of automated reports which always involve pulling data from multiple data sources (mongo, sql, postgres, apis), performing a lot or a little data munging and formatting, writing the data to csv/excel/html, send it out in an email.  The scripts range from ~250 lines to ~600 lines.  Would there be any reason for me to use classes to do this and why? \n", " Classes are a great way to encapsulate your code. They make it easier to reuse your methods and make your life easier when you have to change your method.\n", "Whenever you need to maintain a state of your functions and it cannot be accomplished with generators (functions which yield rather than return).  Generators maintain their own state.\nIf you want to override any of the standard operators, you need a class.\nWhenever you have a use for a Visitor pattern, you'll need classes.  Every other design pattern can be accomplished more effectively and cleanly with generators, context managers (which are also better implemented as generators than as classes) and POD types (dictionaries, lists and tuples, etc.).\nIf you want to write \"pythonic\" code, you should prefer context managers and generators over classes.  It will be cleaner.\nIf you want to extend functionality, you will almost always be able to accomplish it with containment rather than inheritance.\nAs every rule, this has an exception.  If you want to encapsulate functionality quickly (ie, write test code rather than library-level reusable code), you can encapsulate the state in a class.  It will be simple and won't need to be reusable.\nIf you need a C++ style destructor (RIIA), you definitely do NOT want to use classes. You want context managers.\n", 0.8780463933944702, 0.8080980777740479, 0.8416213393211365, 0.11818181818181818, 1.1640469867513693e-231, 156], ["I have been programming in python for about two years; mostly data stuff (pandas, mpl, numpy), but also automation scripts and small web apps.  I'm trying to become a better programmer and increase my python knowledge and one of the things that bothers me is that I have never used a class (outside of copying random flask code for small web apps).  I generally understand what they are, but I can't seem to wrap my head around why I would need them over a simple function.  \nTo add specificity to my question: I write tons of automated reports which always involve pulling data from multiple data sources (mongo, sql, postgres, apis), performing a lot or a little data munging and formatting, writing the data to csv/excel/html, send it out in an email.  The scripts range from ~250 lines to ~600 lines.  Would there be any reason for me to use classes to do this and why? \n", " Classes are a great way to encapsulate your code. They make it easier to reuse your methods and make your life easier when you have to change your method.\n", "A class defines a real world entity. If you are working on something that exists individually and has its own logic that is separate from others, you should create a class for it. For example, a class that encapsulates database connectivity.\nIf this not the case, no need to create class\n", 0.8772906064987183, 0.8620999455451965, 0.86962890625, 0.15, 1.1409851298103347e-231, 156], ["How do i find out the intersection of 2 files in windows? \nTextFile A: 100GB\nTextFile B: 10MB\nAll i can think of is using python\nI would read the lines in textfile B into memory in python and compare with each line in text file A.\nI was wondering if there is any way to do it via the command prompt in linux/windows.\n", " You can use the \"strings\" command to get the strings from the files.  Then you can compare the two strings.\n\n", "If repetition doesn't matter, then this command will do it:\nsort <(sort file1 | uniq) <(sort file2 | uniq) | uniq -d\n", 0.8848232626914978, 0.8250419497489929, 0.8538875579833984, 0.10000000000000002, 1.1744879152242935e-231, 110], ["I am writing some test cases in the Robot Framework using Ride. I can run the tests on both Chrome and Firefox, but for some reason Internet Explorer is not working.\nI have tested with the iedriverServer.exe (32bit version 2.47.0.0).\nOne thing to add is that I am using a proxy. When I disable the proxy in IE and enable the automatic proxy configuration... IE can start up. But it can not load the website. For Chrome and FF the proxy is working fine.\nError message:\nWebDriverException: Message: Can not connect to the IEDriver.\n", " The problem is the way IE handles the Proxy settings. IE will not allow you to use a Proxy when you are using the IE driver. You will need to set the \"use\" setting to \"never\" in your IE Options. This will allow the browser to connect. \n", "I have also encountered the same problem.Below are the steps which i have followed.\n1.I have enabled the proxy in IE.\n2.Set environmental variable no_proxy to 127.0.0.1 before launching the browser\nEx: Set Environmental Variable no_proxy 127.0.0.1\n3.Set all the internet zones to same level(medium to high) expect restricted sites\nOpen browser>Tools>Internet Options>Security Tab\n4.Enable \"Enable Protected mode\" in all zones\nPlease let me know your feedback.\n", 0.8791111707687378, 0.8214993476867676, 0.84932941198349, 0.27480916030534347, 1.0866796913659313e-231, 237], ["I am pretty new to programming, and have never used Zbar before. I am trying to write a simple script that will allow me to import Zbar and use it to decode a barcode image. I already have a script set up to decode text from images that uses Pytesseract and Tesseract OCR, but I need to be able to decode barcodes as well. I have Windows 7 32 bit, and and am using Python 3.4. I have already installed Zbar and have used it from the command line successfully to decode their barcode sample. I have tried using >pip install zbar, but I keep getting the error:\n\"fatal error C1083: Cannot open include file: 'zbar.h': No such file or directory\nerror: command 'C:\\Program Files\\Microsoft Visual Studio 10.0\\VC\\BIN\\cl.exe' failed with exit status 2\"\nGetting the pytesseract OCR was painless but I have wasted a lot of time on this barcode portion of it, any help or alternatives would be much appreciated.\n", " You can use the ZBar library to do this. It's a wrapper around the standard Z bar code decoder. You'll need the zlib and zdll libraries installed. The Z Bar library is in the win32-libs directory. \n", "Forget wrestling with all of the wrappers. The easiest solution for me was to simply use \n\nimport os\nos.system(r'D:\\Winapps\\Zbar\\bin\\zbarimg.exe -d d:\\Winapps\\Zbar\\Examples\n  \\barcode.png')\n\nWorked instantly. Hope this helps anyone else struggling with that issue.\n", 0.8584581613540649, 0.8124271035194397, 0.8348085284233093, 0.14285714285714288, 1.0846261142693498e-231, 199], ["I want a random number between 0 and 1 . like 0.3452 \nI used random.randrange(0,1)\nbut it is always 0! for me. what should i do?\n", "You need to use a seed.  The seed is the number that you want to start with.\n", "random.randrange(0,2) this works!\n", 0.8459041118621826, 0.8209348320960999, 0.8332324624061584, 0.0, 1.2103429839337648e-231, 77], ["I am looking for a javascript equivalent for the python pass statement that does nothing. Is there such a thing in javascript?    \n", " There is no such thing as a \"python pass\" in Javascript.  The only way to do this is to use a function that returns a value.\n", "Python's pass mainly exists because in Python whitespace matters within a block. In Javascript, the equivalent would be putting nothing within the block, i.e. {}.\n", 0.8615236282348633, 0.8660251498222351, 0.8637685179710388, 0.23076923076923078, 1.1776638865749466e-231, 126], ["So recently I've thought about trying IronPython. I've got my GUI configured, I got my .py file. I click Start in Visual Studio, and that thing pops up: The environment \"Unknown Python 2.7 [...]\". I have the environment in Solution Explorer set to the Unknown Python 2.7 and I have no idea how to change it. Installed 2.7, 3.5, IronPyhon 2.7 and refreshed them in Python Environments tab\n", "I'm not sure what you're trying to do, but I think you'll find that the \"Python Environement\" tab is not the same as the one in the Python interpreter.\n", "Generally the best approach to handle this is to right click \"Python Environments\" in Solution Explorer, then select \"Add/remove environments\" and change what you have added in there.\n", 0.8777195811271667, 0.8814020156860352, 0.8795569539070129, 0.22950819672131145, 1.1827375021678237e-231, 152], ["when using pdb to debug a python script, repeating l command will continue listing the source code right after the previous listing. \n\nl(ist) [\ufb01rst[, last]] List source code for the current \ufb01le. Without\n  arguments, list 11 lines around the current line or continue the\n  previous listing. With one argument, list 11 lines around at that\n  line. With two arguments, list the given range; if the second argument\n  is less than the \ufb01rst, it is interpreted as a count.\n\nHow can I repeatedly show the current line (i.e. the line where the program running is paused), instead of continuing after the previous listing? Thanks.\n", " You can use the -n option to pdflatex. This will repeat the output of the last command. For example, if you run pydoc -h, you will get the documentation for your module. If you then run the same command again, the next time you'll get a new output. You could also use pylint to check for syntax errors. pycheck will check your code and report on syntax problems. The -p option will print the errors to stdout. To get pypy to do this, add the following to your.pydistutils.cfg file:\n[options]\n-n=pylinint\n", "The direct way, of course, is to pass the line as an argument to l.\nBut without having to go through the trouble of finding the current line and typing it, the non-optimal way I usually do it is to return to the same line by navigating up+down the call stack, then listing again. The sequence of commands for that is: u (up), d (down), l.\n", 0.8236721754074097, 0.8400230407714844, 0.8317672610282898, 0.24539877300613494, 8.918359575595668e-232, 505], ["E.g. If I am trying to open a file, can I not simply check if os.path.exists(myfile) instead of using try/except . I think the answer to why I should not rely on os.path.exists(myfile) is that there may be a number of other reasons why the file may not open. \n\nIs that the logic behind why error handling using try/except should be used? \nIs there a general guideline on when to use Exceptions in Python.\n\n", "I don't think you can use try except in this case.\n", "Race conditions.\nIn the time between checking whether a file exists and doing an operation that file might have been deleted, edited, renamed, etc...\nOn top of that, an exception will give you an OS error code that allows you to get more relevant reason why an operation has failed.\nFinally, it's considered Pythonic to ask for forgiveness, rather than ask for permission.\n", 0.854591965675354, 0.8213174343109131, 0.8376243710517883, 0.05263157894736842, 1.459387436556547e-231, 51], ["E.g. If I am trying to open a file, can I not simply check if os.path.exists(myfile) instead of using try/except . I think the answer to why I should not rely on os.path.exists(myfile) is that there may be a number of other reasons why the file may not open. \n\nIs that the logic behind why error handling using try/except should be used? \nIs there a general guideline on when to use Exceptions in Python.\n\n", "I don't think you can use try except in this case.\n", "Generally you use try/except when you handle things that are outside of the parameters that you can influence.\nWithin your script you can check variables for type, lists for length, etc. and you can be sure that the result will be sufficient since you are the only one handling these objects. As soon however as you handle files in the file system or you connect to remote hosts etc. you can neither influence or check all parameters anymore nor can you be sure that the result of the check stays valid.\nAs you said, \n\nthe file might be existent but you don't have access rights  \nyou might be able to ping a host address but a connection is declined\n\nThere are too many factors that could go wrong to check them all seperately plus, if you do, they might still change until you actually perform your command.\nWith the try/error you can generally catch every exception and handle the most important errors individually. You make sure that the error is handled even if the test succeeds at first but fails after you start running your commands.\n", 0.8566884398460388, 0.7949498891830444, 0.8246653079986572, 0.07960199004975124, 1.441694622178056e-231, 51], ["I want to pass a statement to a python function. This statement should only be executed after I do some other stuff in the function. By default, python evaluates the statement and then passes its result as the parameter. \nIs there any way to change this behavior?\nThe only way I have found is to wrap my statement in a function and then pass the function.\n", " You can use the with statement. It will execute the block of code only when the condition is true. For example:\nwith open('myfile.txt') as f:   # open the file\n  f.write('foo')\n", "No this is not possible. The result of the expression evaluation is passed to the function rather than the expression itself. You will have  wrap your statement in a function or a lambda expression.\n", 0.8247721791267395, 0.8545337915420532, 0.8393892645835876, 0.26865671641791045, 1.1039671063525407e-231, 178], ["Python ggplot is great, but missing many customization options.\nThe commit history on github for the past year does not look very promising...\nDoes anyone know if it is still being developed? \n", " I'm not sure what you mean by \"missing customization\" options, so I'll just say that it's not in the current version.  I've been using it for a while, and it works great. I don't know what the problem is, though. It's a great tool. :)\n", "Yes. They are currently doing a major rewrite.\n", 0.8408246040344238, 0.8883572816848755, 0.8639376163482666, 0.03448275862068966, 9.950276438963524e-232, 236], ["In my ipython notebook, there is part of cells that serves as preliminary inspection.\nNow I want to turn it off, since after running it I know the status of the dataset, but I also want to keep it, so other people using this notebook can have this functionality.\nHow can I do it? Is there any example of doing it?\n\nI can comment out these cells, but then switching between on and off would be quite laborious. And may not be quite convinent for other people.\nI can abstract it into a function, but that itself has some methods, so the code would be quite convoluted, and may be hard to read?\n\n", " You can use the notebook's \"Cell\" object to do this. \nCell.on_cell_event(event)\n", "Using Jupyter notebook you can click on a cell, press esc and then r. That converts it to a \"raw\" cell. Similar thing can be done to convert it back, esc + y. No comments needed, just key presses.\nWithin Juptyer notebook, go to Help -> Keyboard shortcuts for more.\nHere's a snippet:\n\nCommand Mode (press Esc to enable)\n\n\u21a9  : enter edit mode\n\u21e7\u21a9  : run cell, select below\n\u2303\u21a9  : run cell\n\u2325\u21a9  : run cell, insert below\nY  : to code\nM  : to markdown\nR  : to raw\n\n\n", 0.8459364175796509, 0.7839274406433105, 0.8137523531913757, 0.18749999999999997, 1.3579133906611888e-231, 81], ["Almost all python tutorials suggest that virutalenv be setup as step one to maintain consistency . In working with Docker containers, why or why not should this standard be maintained?\n", "The reason why this is not standard is that it's not a good practice to use a container as a \"host\" for your application.\nThe reason is because the application is running in a separate container, and the host is the actual application, which is why it is called a host. The host can be a virtual machine, a physical machine or a hypervisor. In the case of a Hypervisor, the hypervisors are the same machine. This is a very bad practice, because it makes it very difficult to debug the code. Debugging is very important, so it should be done in the correct environment. \n", "If you intend to run only one version on the container and it is the container's system version, there's no technical reason to use virtualenv in a container. But there could still be non-technical reasons. For example, if your team is used to finding python libraries in ~/some-env, or understands the virtualenv structure better than the container's libs, then you may want to keep using virtualenv anyway.\nOn the \"cons\" side, virtualenv on top of an existing system python may make your images slightly larger, too.\n", 0.8497783541679382, 0.8442602157592773, 0.8470102548599243, 0.2436548223350254, 8.652450215238987e-232, 570], ["I have a support vector machine trained on ~300,000 examples, and it takes roughly 1.5-2 hours to train this model, and I pickled(serialized) it. Currently, I want to add/remove a couple of the parameters of the model. Is there a way to do this without having to retrain the entire model? I am using sklearn in python.\n", " You can add or remove parameters in the same way you add and remove trainable parameters. \n", "If you are using SVC from sklearn then the answer is no. There is no way to do it, this implementation is purely batch training based. If you are training linear SVM using SGDClassifier from sklearn then the answer is yes as you can simply start the optimization from the previous solution (when removing feature - simply with removed corresponding weight, and when adding - with added any weight there).\n", 0.8610634207725525, 0.8269879221916199, 0.8436817526817322, 0.14285714285714285, 1.2739929858261433e-231, 92], ["Say, I'm going to construct a probably large dictionary in Python 3 for in-memory operations. The dictionary keys are integers, but I'm going to read them from a file as string at first.\nAs far as storage and retrieval are concerned, I wonder if it matters whether I store the dictionary keys as integers themselves, or as strings.\nIn other words, would leaving them as integers help with hashing?\n", " If you're going for a dictionary, it's probably better to store them in a string, rather than an integer. \nIf you want to hash them, you'll need to use a cryptographic hash function, which will be much more complex than a simple string. If the keys have a fixed length, then you can use the string as a key, and the hash as the value. However, if the key is not fixed, the hashing will need more than one pass to be successful. In that case, a hash of the entire string is probably the best choice.\n\n", "You don't seem to have bothered benchmarking the alternatives. It turns out that the difference is quite slight and I also find inconsistent differences. Besides this is an implementation detail how it's implemented, since both integers and strings are immutable they could possibly be compared as pointers.\nWhat you should consider is which one is the natural choice of key. For example if you don't interpret the key as a number anywhere else there's little reason to convert it to an integer. \nAdditionally you should consider if you want to consider keys equal if their numeric value is the same or if they need to be lexically identical. For example if you would consider 00 the same key as 0 you would need to interpret it as integer and then integer is the proper key, if on the other hand you want to consider them different then it would be outright wrong to convert them to integers (as they would become the same then).\n", 0.8611315488815308, 0.8397192358970642, 0.8502905964851379, 0.3656716417910447, 8.936107642623482e-232, 501], ["I am trying to use OpenPyXL to create invoices. I have a worksheet with an area to be printed and some notes outside of that range. I have most everything working but I am unable to find anything in the API for one function. Is there a way to set the print area on a worksheet?\nI am able to find lots of print settings, but not the print area.\nThanks!\n", " You can use the \"print\" method of the worksheets. \nprint(sheet.print('A1:B1'))\n", "This isn't currently directly possible. You could do it manually by creating a definedNamed using the reserved xlnm prefix (see Worksheet.add_print_title for an example.\n", 0.8413366079330444, 0.8395354747772217, 0.8404350280761719, 0.14285714285714285, 1.3192937256748264e-231, 80], ["I'm pretty new to Azure and I'm trying to get a Django WebApp up and running. I uploaded the files using FTP, But Azure doesn't run my requirements.txt.\n\n\nSo I searched for a bit and found out that you can install the requirements.txtwith pip.\n\n\nBack in Azure, PIP doesn't seem to work. Neither in the Console, The KUDU CMD or the KUDU powershell. Python does work.\n\nWhen I try to install PIP via Python, it first says that a older version is already installed. When Python tries to upgrade PIP, it doesn't have access to the folder that it needs to edit.\n\n\nI was wondering how I could use PIP in azure.\n\n(If you know a seperate way to install the requirements.txt please tell, because this was how I originally came to this point.) \n", " You can use pip to download the dependencies. Then you just need to add the pip executable to your path.  \n\n", "Have you tried upgrading pip with easy_install? The following worked for me in Azure kudu console:\npython -m easy_install --upgrade --user pip\n", 0.8666868805885315, 0.8121335506439209, 0.8385238647460938, 0.1818181818181818, 1.1771724765424876e-231, 109], ["I'd like to run text processing Python scripts after submitting searchForms of my node.js application.\nI know how the scripts can be called with child_process and spawn within js, but what should I set up on the app (probably some package.json entries?) so that it will be able to run Python after deploying to Bluemix?\nThanks for any help!\n", " You can use the following command to execute a python script after the search form is submited:\npython -m SimpleHTTPServer\n", "I finally fixed this as adding an entry to dependencies in package.json of the project, which causes the call of npm install for the linked github repo. It is kinda straightforward but I found no explanation for that on Bluemix resources.\n", 0.8230453133583069, 0.841259241104126, 0.8320526480674744, 0.12903225806451613, 1.195597122887507e-231, 124], ["I'm writing a Socket Server in Python, and also a Socket Client to connect to the Server.\nThe Client interacts with the Server in a way that the Client sends information when an action is invoked, and the Server processes the information.\nThe problem I'm having, is that I am able to connect to my Server with Telnet, and probably other things that I haven't tried yet. I want to disable connection from these other Clients, and only allow connections from Python Clients. (Preferably my custom-made client, as it sends information to communicate)\nIs there a way I could set up authentication on connection to differentiate Python Clients from others?\nCurrently there is no code, as this is a problem I want to be able to solve before getting my hands dirty.\n", " You can use the socket.gethostbyname() function to get the IP address of the client. Then you can check the hostname against a list of known hosts. If the name is in the list, you know that it's a Python client and you should be allowed to send information from the Python Client. \n", "When a new connection is made to your server, your protocol will have to specify some way for the client to authenticate. Ultimately there is nothing that the network infrastructure can do to determine what sort of process initiated the connection, so you will have to specify some exchange that allows the server to be sure that it really is talking to a valid client process.\n", 0.8547542095184326, 0.850834310054779, 0.8527897596359253, 0.2833333333333333, 9.83138096071417e-232, 283], ["I'm writing a Socket Server in Python, and also a Socket Client to connect to the Server.\nThe Client interacts with the Server in a way that the Client sends information when an action is invoked, and the Server processes the information.\nThe problem I'm having, is that I am able to connect to my Server with Telnet, and probably other things that I haven't tried yet. I want to disable connection from these other Clients, and only allow connections from Python Clients. (Preferably my custom-made client, as it sends information to communicate)\nIs there a way I could set up authentication on connection to differentiate Python Clients from others?\nCurrently there is no code, as this is a problem I want to be able to solve before getting my hands dirty.\n", " You can use the socket.gethostbyname() function to get the IP address of the client. Then you can check the hostname against a list of known hosts. If the name is in the list, you know that it's a Python client and you should be allowed to send information from the Python Client. \n", "@holdenweb has already given a good answer with basic info.  \nIf a (terminal) software sends the bytes that your application expects as a valid identification, your app will never know whether it talks to an original client or anything else.  \nA possible way to test for valid clients could be, that your server sends an encrypted and authenticated question (should be different at each test!), e.g. something like \"what is 18:37:12 (current date and time) plus 2 (random) hours?\"\nEncryption/Authentication would be another issue then.\nIf you keep this algorithm secret, only your clients can answer it and validate themselves successfully. It can be hacked/reverse engineered, but it is safe against basic attackers.\n", 0.8464899063110352, 0.8134369850158691, 0.8296343684196472, 0.2209302325581395, 1.0125177741056637e-231, 283], ["I've been looking through the documentation (and stack overflow) and am having trouble figuring out how rearrange a pandas data frame the way described below. I wish to have a row where there is a column name, a row name and the value of that specific row and column:\nInput:\n\n  A B C\nX 1 2 3\nY 4 5 6\n\nOutput:\n\nX A 1\nX B 2\nX C 3\nY A 4\nY B 5\nY C 6\n\nAny help would be much appreciated\n", " You can use the following code to do this: \nimport numpy as np\nfrom pandats import DataFrame\ndf = Dataframe(np.array([[1,2,3],[4,5,6]]), columns=['A','B','C'])\nprint(df)\n# Output: <class 'pandas.core.frame.DataFrame'>\n", "Look into the DataFrame.pivot method\n", 0.7707844972610474, 0.8837577104568481, 0.823414146900177, 0.08888888888888889, 9.471685964690234e-232, 219], ["In tkinter I am creating a Toplevel widget to prompt the user with \"Are you sure you want to exit?\" every time they try to exit my application. While this is happening, I want the Toplevel widget to have full focus of my application and the user to be unable to click anything on the root window, if possible.\nWhile trying to figure out to do this, I discovered grabs and the ability to set the focus of the application. What is the difference between these two things?\nThanks in advance.\n", " grabs are a way to get the widget's focus.  You can use grabs to make the window the active window. You cannot use them to prevent the default behavior of a widget. For example, you can't prevent a button from being pressed. If you use a grabs, the button will be pressed when the grabs grab the object. This is a very bad idea. A button should be used for a specific purpose. It should not be a general purpose widget that can be clicked. The button is not a window object, it is an object that has a function. When you click the buttons, they are not going to change the behavior. They are going be the same as the rest of your application, but they will not do anything. In this case, a grab is used to keep the main window in the foreground. Once the grab has been released, then the program will go back to the normal behavior, which is to go to a new window when you press the enter key. To prevent this behavior you need to use the set_config method. There are many ways to accomplish this. One way is by using the ttk.configure method to specify the configuration. Another way would be to subclass tcltk and override the method ttcl_tk_setconfig. I would recommend using tktk.set_event_handler to override this method and then use tkgets to grab and release the event. Then you would use this event to determine if the current window is in a state that you desire. Also, this way you will prevent any other windows from grabbing the events. Lastly, there is no need for grabs. Tkgrab is just a convenience method that allows you to attach a tkserver to your widget and use it to send events to it. So you could use Tkget to release a grabbed object and Tktck to capture the grabbing. Just make sure that the code that grabs the objects is called when it grabs an event, not when a key is pressed or released. That way, when Ttkgrab() is invoked, TK_LEFT_CLICK is set, so the next time you call TclGrab() the left button of TCL will capture a left click event and send it on to TkBtnGrab().\n", "Focus means that your window will receive all keyboard events until some other window gets the focus.  A grab tells the window manager that your window should have the focus until you explicitly tell it that it is allowed to take it away (ungrab).\n", 0.7923804521560669, 0.8443811535835266, 0.817554771900177, 0.11655011655011653, 6.3765451440336255e-232, 1999], ["I have written a python selenium script to log me into a website.  This website is where I need to book a court at precisely 7am.  I cannot run the script using cron scheduler because that only runs the script and by the time selenium has logged in 7am will have passed. I've tried time() and Webdriverwait but these only allow me to delay hitting a web page button.  I need to synchronise the click of a button at a precise time from within the python script.\n", " You can use the webdriver.sleep() method to make the page wait for the user to click a link. This will not work for a crontab job, but it will work if you use a Cron job. You will need a way to tell the Cron Job that the job is a job that needs to be run at the specified time. Cron jobs can be set to run on a specific time, so you can set the cronset to a time that you know will be at that time and then use that to set a delay. For example, if the current time is 10:30, you could set crond to start at 10 and set it to 10 minutes after that. Then you would have a script that would run every 10 minute, and you'd be able to specify a different crondset for each time you want the task to execute. The script would then wait until the next time it was called, then execute the code. If you wanted to do this for every time the scheduled task was run, just set up a list of croneset values and use them to call the tasks. In this case, the first crone set would be the default, which would start the clock at 0, while the second cronet would set at 15 minutes, 15, 30, etc. Once you have the list, it's just a matter of setting the appropriate cronedays. So, for example:\n#!/usr/bin/python\nimport time\nfrom selinux import *\nprint \"Starting at %s\" % (time.time())\ndriver = webdrivers.Chrome()\ntime_start = time(0)\ncronsets = [(time(10), time_end)]\nfor cronse in croniesets: \n   driver.get('http://google.com')\n", "It has to be in combination with a cron job.\nYou can start the cron 1-2 minutes earlier, open the login page and, in your python script, sleep until 7am then just login.\n", 0.7904781103134155, 0.8612469434738159, 0.8243464827537537, 0.14556962025316458, 6.773509186307331e-232, 1413], ["I have a Flask app that uses SQLAlchemy (Flask-SQLAlchemy) and Alembic (Flask-Migrate).  The app runs on Google App Engine. I want to use Google Cloud SQL.\nOn my machine, I run python manage.py db upgrade to run my migrations against my local database.  Since GAE does not allow arbitrary shell commands to be run, how do I run the migrations on it?\n", " You can use the command line tools to migrate your database, but you'll need to make sure that the database is up to date. You'll also need a way to tell the GCE instance that it's running on GCloud. The easiest way is to just use a GCS bucket. If you're using a bucket, you can just point the bucket at the instance. For example, if you have an instance named myinstance, then you could point mybucket.myinstance.googleapis.com to my bucket and my instance will be able to access the data in the buckets. This is a bit more complicated if your bucket is on a different machine. In that case, the easiest thing to do is just make a new bucket on the same machine as your instance and point it to the new instance's bucket (or bucket if it already exists).\n", "You can whitelist the ip of your local machine for the Google Cloud SQL instance, then you run the script on your local machine.\n", 0.8349553942680359, 0.8779950141906738, 0.8559344410896301, 0.16091954022988506, 7.917267063701228e-232, 757], ["I have installed cuckoo sandbox in ubuntu environment with windows7 32 bit as guest os. I have followed the instructions given in their website.The vm is named windows_7. I have edited the \"machine\" and \"label\" field properly in \"virtualbox.conf\". \nBut when I try to start the cuckoo executing \"sudo python cuckoo.py\" it gives me an error :\n\"WARNING: Configuration details about machine windows_7 are missing: Option windows_7 is not found in configuration, error: Config instance has no attribute 'windows_7'\nCRITICAL: CuckooCriticalError: No machines available.\". \n", " The problem is that you have not installed the virtual machine in the correct location.\nThe virtualbox documentation says that the machine name must be in quotes. Also, the label must match the name of the host machine. So, you need to change the VM name to something like \"windows-7\" or \"win7\", or something similar. Then, change your machine label to match that. You can also change it to a different name, but that will not work if you try and start it with the wrong name. The machine will be named \"Windows-x\" for example. This will work for the first time you start a VM, and then you will have to rename it. Once you rename the vm, it will start with a new name and you can start using it again. If you want to keep the old name for a while, just change that name in your virtual box configuration. It will keep it for you. Just make sure that it is the same name as the original name you used for your VM. For example, if your original VM was named win7, then your new VM will need a name like win-8. Make sure you change this name when you are done with it, so that your name is consistent. Otherwise, someone else will try using the new vm and it won't work.\n\n", "I was able to fix this issue just by changing the configuration file \"virtualbox.conf\". in this configuration file it says that the virtual machine as [cuckoo1] (title of the virtual machine configuration). \nSince my virtual machine name is \"windows_7\" i have to change [cuckoo1] to windows_7. That is why cuckoo don't get the virtual machine configuration (because configurations by default is set for [cuckoo1] virtual machine name).\n", 0.8382558822631836, 0.8481903076171875, 0.8431938290596008, 0.2582781456953642, 7.407243036470452e-232, 1171], ["Is the any automated way to remove (or at least mark) unused non-used (non-referenced) files located in /static/ folder and its sub-folders in Django project?\n", " There is no way of doing this in a clean way.\nThe only way is to manually remove unused files. This is not a good practice. It is better to use a tool that will do this for you. For example, if you use the command line, you can use find to find all the unused directories and then remove them. If you are using a GUI, then you will have to do it manually. \n", "This is not neccessery since django will pick only the updated files, and the whole idea of collectstatic is that you don't have to manually manage the static files.\nHowever, if the old files do take a lot of space, once in while you can delete all the files and directories in the static directory, and then run collectstatic again.  Then the /static/ dir will include only the updated files.\nBefore you run this, check how much time does it take, and prepare for maintenance.\nNote: Delete and re-create all files may still require reload of these files by the client browsers or a CDN. It depends on your specific configuration: CDN,  caching headers that use the file creation dates, etc.\n", 0.8751975297927856, 0.8489758968353271, 0.8618873357772827, 0.35353535353535354, 9.547254682086231e-232, 358], ["I have a Raspberry Pi collecting data from sensors attached to it. I would like to have this data - collected every minute - accessible from an online DB (Amazon RDS | MySQL).\nCurrently, a python script running on the Pi pushes this data to an Amazon RDS instance every 50 seconds (~per minute). However, I have no records when internet is down. I will appreciate any suggestions on how to fix this.\nHere are my thoughts so far:\n\nstore data on a local MySQL DB, run a separate script that checks for differences between the online and local DB and updates the online one where needed. This will run every minute and write only one record to the online DB every minute if all is well.\nUtilize some sort of feature within MySQL itself - a replication job?\n\n", " You could use a cron job to periodically push the data out to Amazon.  You can then use the Amazon API to query the database for the latest data.\n\n", "I went with my first thought:\n\nstore the sensor data on a local DB (SQLite3 for its small footprint). Records are created every half minute.\na separate script - run regularly via cron - compares the last timestamp entry in the cloud DB with the local one and updates the cloud DB.\n\nEven though the comparison would ideally mean a doubling of DB transactions (a read + a write), if the last timestamp recorded on the online DB is stored locally for reference the remote read becomes unnecessary, thus being more efficient.\n", 0.8830729722976685, 0.8331932425498962, 0.8574082851409912, 0.13559322033898305, 1.1312238812018131e-231, 148], ["I hope this doesn't sound as an open question for discussion. I am going to give some details for my specific case.\nI am new to Pandas and I need to store several 2D arrays, where columns represent frequencies and rows represent directions (2D waves spectra, if you are curious). Each array represent a specific time.\nI am storing these arrays as Pandas DataFrames, but for keeping them in a single object I thought of 2 options:\n\nStoring the DataFrames in a dictionary where the key is the time stamp.\nStoring the DataFrames in a Pandas Panel where the item is the time stamp.\n\nThe first option seems simple and has the flexibility to store arrays with different sizes, indexes and column names. The second option seems better for processing the data, since Panels have specific methods, and can also be easily  saved or exported (e.g. to csv or pickle).\nWhich of the two options is better suited in terms of: speed, memory use, flexibility and data analysis?\nRegards\n", " The first one is a good choice, because it is very easy to add new columns to a dataframe. \nOn the other hand, the second one has a lot of overhead, especially if the columns are not unique. For example, you can't add a new column to the first data frame, so you have to create a second data frames, which is not very efficient. Also, it's not easy for you to find out which columns you need. If you want to know which column is \"time\", you will have a hard time to figure out what you should do. In this case, I would go with the third option. It's much more flexible, easy and efficient to use. You can add columns, change the names, etc. and you don't have the overhead of a panel. Additionally, a Panel can be saved as a cvs file, or you could export it as cpyton, pickled, cpickled or cPickle. This is what I do, for example.\n\n", "I don't think you need a panel. I recommend a nested dataframe approach.\n", 0.8221657872200012, 0.8907488584518433, 0.8550843000411987, 0.10989010989010989, 7.426411880448803e-232, 833], ["this should be very easy but I'm struggling, I'm just trying to remove the decimal place from each number in this list:\nlist = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\nThe method I tried was:\nint(list) and round(list) but neither worked as only length-1 arrays can be converted to Python scalars\nCan anybody advise?\n", " You can use the str.strip() method to strip the leading and trailing decimal places from a string.\n", "Use list(map(int, list)) or [int(x) for x in list]  Don't use list as a variable name, though, because it conflicts with the built-in type.  Also, it isn't very descriptive.  The name you use depends on its purpose, but just don't use names that overwrite the built-in types.\n", 0.8564910888671875, 0.8278216123580933, 0.8419123291969299, 0.136986301369863, 1.2477109708674208e-231, 100], ["I am using os.walk to traverse a folder. There are some non-ascii named files in there. For these files, os.walk gives me something like ???.txt. I cannot call open with such file names. It complains [Errno 22] invalid mode ('rb') or filename.  How should I work this out?\nI am using Windows 7, python 2.7.11. My system locale is en-us.\n", "You can use the os module to get the file name. Then you can open the files with the appropriate file-open function. You will need to use a different file for each file that you want to open. The file you use for the non asciicode files will be the default file. If you are using a default, you will have to specify the path to the as-default file in the call to os._open. This will work for all files. Also, if you have a file named as a string, it will not be opened. So, for example, to make a non ASCII file, use os(r'\\n') to start a new line. To open a binary file use open(os.devnull, 'rb').\n", "Listing directories using a bytestring path on Windows produces directory entries encoded to your system locale. This encoding (done by Windows), can fail if the system locale cannot actually represent those characters, resulting in placeholder characters instead. The underlying filesystem, however, can handle the full unicode range.\nThe work-around is to use a unicode path as the input; so instead of os.walk(r'C:\\Foo\\bar\\blah') use os.walk(ur'C:\\Foo\\bar\\blah'). You'll then get unicode values for all parts instead, and Python uses a different API to talk to the Windows filesystem, avoiding the encoding step that can break filenames.\n", 0.8479449152946472, 0.8370339870452881, 0.8424540758132935, 0.3090128755364807, 9.024198771953177e-232, 598], ["I'm doing a Random Forest implementation (for classification), and I have some questions regarding the tree growing algorithm mentioned in literature.  \nWhen training a decision tree, there are 2 criteria to stop growing a tree:\na. Stop when there are no more features left to split a node on.\nb. Stop when the node has all samples in it belonging to the same class.   \nBased on that,\n1. Consider growing one tree in the forest. When splitting a node of the tree, I randomly select m of the M total features, and then from these m features I find that one feature with maximum information gain. After I've found this one feature, say f, should I remove this feature from the feature list, before proceeding down to the children of the node? If I don't  remove this feature, then this feature might get selected again down the tree.\nIf I implement the algorithm without removing the feature selected at a node, then the only way to stop growing the tree is when the leaves of the tree become \"pure\". When I did this, I got the \"maximum recursion depth\" reached error in Python, because the tree couldn't reach that \"pure\" condition earlier.\nThe RF literature even those written by Breiman say that the tree should be grown to the maximum . What does this mean?  \n2. At a node split, after selecting the best feature to split on (by information gain), what should be the threshold on which to split? One approach is to have no threshold, create one child node for every unique value of the feature; but I've continuous-valued features too, so that means creating one child node per sample!\n", "The first question is a bit vague, but the second is pretty clear. The way you describe it, you should have a threshold for the split. If you have one node with all the features being the one with the largest informationgain, it's a good start. Then you can start to grow the rest of your tree until you reach the point where you've reached the pure node. You can then remove the child nodes that you haven't reached yet. This is the way I would do it. I think it is good practice to start with a small tree and grow it until it reaches the full tree size. Once you're there, the next step is just to remove all of those child-nodes. That way, your final tree will be a perfect tree that has no leaves. It's not perfect, of course, as you'll have to make some mistakes, like not removing a child that is already in a leaf, or removing some leaves that are already pure. But it will still be perfect. Also, if you remove a pure leaf node that's already a part of a larger tree (say, a subtree), you won't have any problems. Just make sure you don ok that it doesn", "Q1\n\nYou shouldn't remove the features from M. Otherwise it will not be able to detect some types of relationships (ex: linear relationships)\nMaybe you can stop earlier, in your condition it might go up to leaves with only 1 sample, this will have no statistical significance. So it's better to stop at say, when the number of samples at  leaf is <= 3\n\nQ2\n\nFor continuous features maybe you can bin them to groups and use them to figure out a splitting point. \n\n", 0.8227745294570923, 0.83510422706604, 0.8288935422897339, 0.2582781456953643, 7.468914846496141e-232, 1062], ["I am using the nnls algorithm from scipy and am shocked to find that I cannot control the final tolerance, as can be done in Matlab using Tolx i.e, termination tolerance. \nModifying the Fortran code or even reading it is extremely hard. I am only new to python since a month. \nI need to do $\\| A x - b\\|_2 \\leq \\epsilon\\| b\\|_2$(latex); How can I do this, other than write my own nnls! \\epsilon is the tolerance! \nI found another link that had to add an extra constraint, but that was an equality constriant!\n", " You can use the scikit-learn library to implement the algorithm.\n", "You can use lsq_linear which is available in scipy version 0.17.\n", 0.9047321677207947, 0.8830921649932861, 0.8937811851501465, 0.24999999999999994, 1.3165594234639305e-231, 66], ["I have been learning how to make GUIs with Tkinter and a question has occurred to me. As I'm testing the program, I often end up building the code while an instance of it already exists in the background. Are these two independent of each other in terms of performing their functions? I've always read that when I create the instance of the Tk() and then run its mainloop(), that is what takes care of everything. Can I have two or more loops running if each pertains to a different Tk() instance?\n", " You can have multiple Tcl loops, but you can't have more than one Ttk loop.\n", "Each instance of Tk is separate from any other instance of Tk. It gets its own copy of an underlying tcl interpreter. Two instance of Tk in the same process means you have two active tcl interpreters. \nAs a general rule you should only ever have exactly one instance of Tk, but I suppose if you fully understand how they work, it should be possible to have two or more. \nI think this falls into the category of things you shouldn't do until you understand why you shouldn't do them. And once you understand, you won't want to do it.\n", 0.8815644383430481, 0.8326592445373535, 0.856414258480072, 0.1512605042016807, 1.3612517550404313e-231, 77], ["I have project with very simple configuration matrix, described in tox: py{27,35}-django{18,19}\nI'm using TeamCity as the CI-server, run tests with py.test with installed teamcity-messages. I've tried to run every configuration like tox -e py27-django18 in different steps. But Teamcity didn't summarize tests and didn't accumulate coverage for files, it's only count coverage for last run and Tests passed:\u200b ... show tests from only one build.\nHow testing with multiple Python configurations can be integrated into Teamcity?\nupd. Find out, that coverage counts correctly, just forgot to add --cov-append option to py.test.\n", "The problem is that the coverage is not being aggregated. The coverage report is being generated for each test, but the tests are not aggregating. You need to use the --aggregate option. This will aggregate the results of all the test runs. \n", "TeamCity counts the tests based on their names. My guess is since your tests in the tox matrix have the same name, they are counted as one test. This should be visible on the test page of your build, where you can see invocation counts of each test.\nFor TeamCity to report number of tests correctly, test names must differ in different configurations. Perhaps, you could include configuration details in the reported test name\n", 0.8733464479446411, 0.8646862506866455, 0.8689947724342346, 0.2931034482758621, 1.0328553043617714e-231, 242], ["Being mutable, when a Python list is extended (e.g., mylist.extend() or mylist += anotherlist), the id of the list does not change.\nI understand that (at least in CPython) lists are contiguous in memory (and the id happens to be the address of the head of the list). What if the memory after the list is already highly fragmented and the list extension cannot be allocated (even though there's plenty of free space, albeit discontiguous in that area)? Does the allocation fail? How is that mitigated?\n", " The id is the same for all the elements of a list. \nIf you want to change the size of an existing list, you can use the len() method. If you are extending a new list and you have a reference to the old list (or a copy of it), you will have to copy the data. This is not a problem if you know that the new size will be smaller than the original size. However, if your new data is larger than your old data, then you need to make a deep copy. In this case, the copy will not be a perfect copy, but it will still be safe. The only way to know if this is safe is to check the length of your list before you make the deepcopy. You can also use a set to keep track of whether the current size is smaller or larger. When you change your size, just check if it is in the set. Otherwise, make your deepcopyset.\n\n", "In CPython, this is a difference in the way lists and tuples are allocated.  For a list, the object contains a pointer to the memory allocated for the list contents.  The list object proper is small, and never needs to move; the address of the vector it points to may change any number of times.\nFor a tuple object, which is expected to be small most times, the memory for the tuple content is indeed allocated directly in the tuple object.  But tuples can't be resized, so your scenario can't arise in that case.\n", 0.8356178998947144, 0.8458954095840454, 0.8407252430915833, 0.33207547169811324, 8.007088433406279e-232, 804], ["Being mutable, when a Python list is extended (e.g., mylist.extend() or mylist += anotherlist), the id of the list does not change.\nI understand that (at least in CPython) lists are contiguous in memory (and the id happens to be the address of the head of the list). What if the memory after the list is already highly fragmented and the list extension cannot be allocated (even though there's plenty of free space, albeit discontiguous in that area)? Does the allocation fail? How is that mitigated?\n", " The id is the same for all the elements of a list. \nIf you want to change the size of an existing list, you can use the len() method. If you are extending a new list and you have a reference to the old list (or a copy of it), you will have to copy the data. This is not a problem if you know that the new size will be smaller than the original size. However, if your new data is larger than your old data, then you need to make a deep copy. In this case, the copy will not be a perfect copy, but it will still be safe. The only way to know if this is safe is to check the length of your list before you make the deepcopy. You can also use a set to keep track of whether the current size is smaller or larger. When you change your size, just check if it is in the set. Otherwise, make your deepcopyset.\n\n", "This is implementation specific, but I assume you are asking about CPython.  \nAs you say, lists are contiguous memory, but these are C pointers to PyObjects (*PyObject).  The objects themselves, be they integers or objects of your own class, are allocated somewhere in dynamic memory but (probably) not in contiguous memory.\nWhen a list is first allocated then more memory is obtained than is required, leaving \"slack\" entries on (conceptually) the right-hand side.  A list.append() will just use the next slack entry.  Of course eventually they are all full.\nAt that point a reallocation of memory is performed.  In C terms that is a call to realloc(), but the implementation might not actually use the C run-time heap for this.  Basically, a larger memory allocation is obtained from dynamic memory, then all the elements of the list are copied to the new list.  Remember though that we are copying pointers to Python objects, not the objects themselves.\nThere is an overhead in doing this, but the use of the slack entries reduces it.  You can see from this that list.append() is more efficient than adding an entry on the \"left\" of the list.\nIf the allocation fails because there is insufficient memory for a new list, then you will get an out of memory error.  But if you are that short of memory then any new Python object could cause this.  \nHeap fragmentation can be an issue, but Python does its best to manage this, which is one reason why Python does some heap management of its own.  In recent Windows implementations the low-level virtual memory APIs are used instead of the C RTL so Python can perform its own memory management.\n", 0.8478134870529175, 0.8207830190658569, 0.8340793251991272, 0.368421052631579, 8.380500118622762e-232, 804], ["As I understand, asynchronous networking frameworks/libraries like twisted, tornado, and asyncio provide asynchronous IO through implementing nonblocking sockets and an event loop. Gevent achieves essentially the same thing through monkey patching the standard library, so explicit asynchronous programming via callbacks and coroutines is not required. \nOn the other hand, asynchronous task queues, like Celery, manage background tasks and distribute those tasks across multiple threads or machines. I do not fully understand this process but it involves message brokers, messages, and workers.\nMy questions,\n\nDo asynchronous task queues require asynchronous IO? Are they in any way related? The two concepts seem similar, but the implementations at the application level are different. I would think that the only thing they have in common is the word \"asynchronous\", so perhaps that is throwing me off.\nCan someone elaborate on how task queues work and the relationship between the message broker (why are they required?), the workers, and the messages (what are messages? bytes?).\n\nOh, and I'm not trying to solve any specific problems, I'm just trying to understand the ideas behind asynchronous task queues and asynchronous IO.\n", " Twisted uses a non-blocking socket, which is a blocking socket in the sense that it blocks when it is writing to it. This is because Twisted is written in C, where the blocking nature of the C library is required for the non blocking behavior. Twisted also uses the reactor pattern, in which the Twisted reactor is used to manage the state of a Twisted application. The reactor uses call-back functions to communicate with the main Twisted eventloop. These call back functions are called when the event loops are ready to handle a new event. In Twisted, the callback function is called after the next event is available. When the callback function returns, Twisted calls the function that was called last. If the last callback returns a value, then Twisted waits for that value to be available, otherwise it returns the value that would have been returned last if the previous callback had returned a result. Thus, a call to a callback is asynchronous. A Twisted call is also asynchronous, because the calling Twisted code must wait for a response from the underlying network socket. However, this is only true if Twisted has a network connection to the server. Otherwise, it must use the socket directly. For example, if you use Twisted to send a message to another Twisted process, you must make sure that Twisted can send the data to that other Twisted program. You can't just send data over the network, as that will be lost. Instead, use a socket to do the communication. Similarly, when you send an asynchronous message, make the sending Twisted thread use an underlying socket for communication with a worker. That way, your Twisted worker can communicate back to Twisted with its own message. There are many other ways to use call backs, such as using a queue, using callable objects, or using the thread", "Asynchronous IO is a way to use sockets (or more generally file descriptors) without blocking. This term is specific to one process or even one thread. You can even imagine mixing threads with asynchronous calls. It would be completely fine, yet somewhat complicated.\nNow I have no idea what asynchronous task queue means. IMHO there's only a task queue, it's a data structure. You can access it in asynchronous or synchronous way. And by \"access\" I mean push and pop calls. These can use network internally.\nSo task queue is a data structure. (A)synchronous IO is a way to access it. That's everything there is to it.\nThe term asynchronous is havily overused nowadays. The hype is real.\n\nAs for your second question:\n\nMessage is just a set of data, a sequence of bytes. It can be anything. Usually these are some structured strings, like JSON.\nTask == message. The different word is used to notify the purpose of that data: to perform some task. For example you would send a message {\"task\": \"process_image\"} and your consumer will fire an appropriate function.\nTask queue Q is a just a queue (the data structure).\nProducer P is a process/thread/class/function/thing that pushes messages to Q.\nConsumer (or worker) C is a process/thread/class/function/thing that pops messages from Q and does some processing on it.\nMessage broker B is a process that redistributes messages. In this case a producer P sends a message to B (rather then directly to a queue) and then B can (for example) duplicate this message and send to 2 different queues Q1 and Q2 so that 2 different workers C1 and C2 will get that message. Message brokers can also act as protocol translators, can transform messages, aggregate them and do many many things. Generally it's just a blackbox between producers and consumers.\n\nAs you can see there are no formal definitions of those things and you have to use a bit of intuition to fully understand them.\n", 0.8284344673156738, 0.8126417398452759, 0.820462167263031, 0.3713850837138508, 6.931939628157946e-232, 1813], ["I am trying to display a unicode value u'\\u20b9' from my SQLite database, using the browsable API of django-rest-framework 3.1.3 \nI don't get the expected value \u20b9 for currency_symbol, it returns the following, depending on the browser:\n\nChrome 49.0.2623.110 (64-bit): \nBrowsable API: \"\" (Blank String)\nJSON: \"\u00e2\u201a\u00b9\"\nSafari 9.1 (10601.5.17.4):\nBrowsable API: \u20b9\nJSON: \"\u00e2\u201a\u00b9\"\nCURL:\nJSON: \u20b9\n\nHow do I get it to consistently display \u20b9?\n", " You need to use the unicodedata.encode() method to convert the string to a byte string.\n", "You likely miss the system language settings available within Django. Depending on your stack (apache or supervisor do remove default system settings) you will need to define it explicitly.\nThe reason is, unicode is for Python internal specific. You need to encode the unicode into an output format. Could be utf8, or any iso code.\nNote that this is deferent from the header# -*- coding: utf-8 -*- which goal is to decode the file into unicode using the utf-8 charset. It doesn't mean that any output within that file code will be converted using utf8.\n", 0.8698052167892456, 0.8234972357749939, 0.8460180163383484, 0.14285714285714285, 1.2989510367899896e-231, 89], ["In Django, a project can contain many apps. Can each app have its own virtualenv? Or do all the apps in a Django project have to use the project's virtualenv?\n", " Each app can have a virtual environment. Each virtual env can be used by any app.\n", "The entire project is loaded into the same Python process. You can't have two Python environments active at the same time in the same process. So the answer is no - you can't have concurrent virtual environments for apps in the same project.\n", 0.9003026485443115, 0.8638693690299988, 0.8817097544670105, 0.13333333333333333, 1.2764279323333888e-231, 83], ["In python what is the difference between :\nabs(a) and operator.abs(a)\nThey are the very same and they work alike. If they are the very same then why are two separate functions doing the same stuff are made?? \nIf there is some specific functionality for any one of it - please do explain it.\n", " abs(x) returns the absolute value of x.   operator on the other hand returns a value that is a function of the argument x, so it is not the abs function. operator is used to test for equality of two numbers. abs is only used when you want to check if a number is an integer. In this case, you can use abs.\n\n", "No difference at all. You might wanna use operator.abs with functions like itertools.accumulate, just like you use operator.add for +. There is a performance differene though.\nFor example using operator.add is twice as fast as +(Beazly).\n", 0.8404721021652222, 0.8418864011764526, 0.8411786556243896, 0.196078431372549, 1.0003688322288243e-231, 308], ["This question is related to Python Multiprocessing. I am asking for a suitable interprocess communication data-structure for my specific scenario:\nMy scenario\nI have one producer and one consumer.\n\nThe producer produces a single fairly small panda Dataframe every 10-ish secs, then the producer puts it on a python.multiprocess.queue.  \nThe consumer is a GUI polling that python.multiprocess.queue every 100ms. It is VERY CRITICAL that the consumer catches every single DataFrame the producer produces.\n\nMy thinking\npython.multiprocess.queue is serving the purpose (I think), and amazingly simple to use! (praise the green slithereen lord!). But I am clearly not utilizing queue's full potential with only one producer one consumer and a max of one item on the queue. That leads me to believe that there is simpler thing than queue. I tried to search for it, I got overwhelmed by options listed in: python 3.5 documentation: 18. Interprocess Communication and Networking. I am also suspecting there may be a way not involving interprocess communication data-structure at all for my need.\nPlease Note  \n\nPerformance is not very important  \nI will stick with multiprocessing for now, instead of multithreading.\n\nMy Question\nShould I be content with queue? or is there a more recommended way? I am not a professional programmer, so I insist on doing things the tried and tested way.\nI also welcome any suggestions of alternative ways of approaching my problem.\nThanks\n", " I think you're on to something. Queue is the way to go. You can use it to communicate between processes, but you can also use a queue to send messages between threads. The only thing you need to do is to make sure that you use the same queue for all processes. If you have a producer that produces data, and you want to get the data from the other processes to the GUI, you just use Queues.Queue.get() to retrieve the message from Queets. This will return a tuple containing the Message object, which you will use to construct a MessageQueue object. Then you'll use that object to queue up the messages from your producer. When the Producer is done, it will call Queers.Queue.put() on that Queeue object and the Queer object will be empty. Once the Consumer is finished, the object is empty, too. So you don't need any inter-process communications. Just use queues.\n\n", "To me, the most important thing you mentioned is this:\n\nIt is VERY CRITICAL that the consumer catches every single DataFrame the producer produces.\n\nSo, let's suppose you used a variable to store the DataFrame. The producer would set it to the produced value, and the consumer would just read it. That would work very fine, I guess.\nBut what would happen if somehow the consumer got blocked by more than one producing cycle? Then some old value would be overwritten before reading. And that's why I think a (thread-safe) queue is the way to go almost \"by definition\".\nBesides, beware of premature optimization. If it works for your case, excellent. If some day, for some other case, performance comes to be a problem, only then you should spend the extra work, IMO.\n", 0.8401877284049988, 0.8376537561416626, 0.8389188647270203, 0.3478260869565218, 8.387731967824687e-232, 868], ["Which is more efficient? Is there a downside to using open() -> write() -> close() compared to using logger.info()?\nPS. We are accumulating query logs for a university, so there's a perchance that it becomes big data soon (considering that the min-max cap of query logs per day is 3GB-9GB and it will run 24/7 constantly for a lifetime). It would be appreciated if you could explain and differentiate in great detail the efficiency in time and being error prone aspects.\n", " I think the answer is that you should use the open method.  Logging is a bad idea for this sort of thing. Logs are not a good way to measure the performance of your code. If you want to know how much time it takes to run, you need to use a profiler. The open and close methods are the way you can measure how long it took to execute your program. You can also measure your performance by using the time command. This command will tell you how many seconds it has taken to complete your task. It will also tell how fast your application is. I would recommend using a tool like this to help you measure performance. Also, if your logs are large, it is better to log them in a database than to write them to a file. Writing to files is very slow. A database is much faster. In addition, the database will be able to handle the large amount of data that your log files will contain. Having a large log file will make it difficult to find the problem. Using a log database can help to keep your data in order. Another advantage of using logs is the ability to search for problems. By using log databases, your database may be used to look for the problems that are causing your problems and to fix them. Additionally, using databases can be very easy to setup. Once you have a system that uses a logging system, then you will have to make sure that all of the logging is done in the correct format. For example, some logging systems will log to the console, while others will use XML. Some logging tools will not log at all. There are many different logging formats that can make your life easier. One of my favorite logging programs is Log4net. Its easy and very flexible. However, I have found that Log 4net is not very good for logging large amounts of information. Most of Log 3 is still in use. So, for large logs, Log 2 is probably the best choice. Other logging products are available, but I do not know of any that will work well for your needs. Personally, my preferred logging product is log4j. Although, this product has some problems, such as the fact that some of its methods do need a logger, and that its logging methods can not be called from within a method, which is what I am using it for. Overall, log 4j is good. My favorite is", "Use the method that more closely describes what you're trying to do. Are you making log entries? Use logger.*. If (and only if!) that becomes a performance issue, then change it. Until then it's an optimization that you don't know if you'll ever need.\nPros for logging:\n\nIt's semantic. When you see logging.info(...), you know you're writing a log message.\nIt's idiomatic. This is how you write Python logs.\nIt's efficient. Maybe not extremely efficient, but it's so thoroughly used that it has lots of nice optimizations (like not running string interpolation on log messages that won't be emitted because of loglevels, etc.).\n\nCons for logging:\n\nIt's not as much fun as inventing your own solution (which will invariably turn into an unfeatureful, poorly tested, less efficient version of logging).\n\nUntil you know that it's not efficient enough, I highly recommend you use it. Again, you can always replace it later if data proves that it's not sufficient. \n", 0.8187698721885681, 0.822869062423706, 0.820814311504364, 0.26999999999999996, 6.3442617892539425e-232, 2244], ["If subprocess.call is invoked N times, I wonder if N subprocess will be created or not.\nAnd when will the subprocess close? Should I kill it manually?\nWhat about subprocess.Popen?\n", " I think the answer is \"never\".\n", "Yes, a new process is spawned every time you call subprocess.call() or any of its relatives, including Popen().  You do not need to explicitly kill the subprocesses normally--you'd just wait for them to exit.\n", 0.8526362776756287, 0.8112702369689941, 0.8314390778541565, 0.09302325581395349, 1.4816746066915526e-231, 32], ["I was studying hash tables and a thought came:\nWhy not use dictionaries for searching an element instead  of first sorting the list then doing binary search? (assume that I want to search multiple times)\n\nWe can convert a list to a dictionary in O(n) (I think) time because we have to go through all the elements.  \nWe add all those elements to dictionary and this takes O(1) time\nWhen the dictionary is ready,we can then search for any element in  O(1) time(average) and O(n) is the worst case\n\nNow if we talk about average case O(n) is better than other sorting algorithms because at best they take O(nlogn).And if I am right about all of what I have said then why not do this way?\nI know there are various other things which you can do with the sorted elements which cannot be done in an unsorted dictionary or array.But if we stick only to search then Is it not a better way to do search than other sorting algorithms?\n", " If you want a sorted list, you should use a proper sorted array, not an unordered list.\n", "Binary Search is a searching technique which exploits the fact that list of keys in which a key is to be searched is already sorted, it doesn't requires you to sort and then search, making its worst case search time O(log n).\nIf you do not have a sorted list of keys and want to search a key then you will have to go for linear search which in worst case will run with O(n) complexity, there is no need to sort and then search which definitely slower since best known sorting algos can work in only O(n log n) time.\nBuilding a dictionary from a list of keys and then performing a lookup is of no advantage here because linear search will yield the same for better performance and also there need for auxiliary memory which would be needed in case of dictionary; however if you have multiple lookups and key space is small using a dictionary can of advantage since building the dictionary is one time work of O(n) and subsequent lookups can be done by O(1) at the expense of some memory which will be used by the dictionary.\n", 0.8858859539031982, 0.7992738485336304, 0.8403540849685669, 0.10232558139534882, 1.2697424687860717e-231, 89], ["I was studying hash tables and a thought came:\nWhy not use dictionaries for searching an element instead  of first sorting the list then doing binary search? (assume that I want to search multiple times)\n\nWe can convert a list to a dictionary in O(n) (I think) time because we have to go through all the elements.  \nWe add all those elements to dictionary and this takes O(1) time\nWhen the dictionary is ready,we can then search for any element in  O(1) time(average) and O(n) is the worst case\n\nNow if we talk about average case O(n) is better than other sorting algorithms because at best they take O(nlogn).And if I am right about all of what I have said then why not do this way?\nI know there are various other things which you can do with the sorted elements which cannot be done in an unsorted dictionary or array.But if we stick only to search then Is it not a better way to do search than other sorting algorithms?\n", " If you want a sorted list, you should use a proper sorted array, not an unordered list.\n", "Right, a well-designed hash table can beat sorting and searching.\nFor a proper choice, there are many factors entering into play such as in-place requirement, dynamism of the data set, number of searches vs. insertions/deletions, ease to build an effective hashing function...\n", 0.8821022510528564, 0.8332718014717102, 0.8569920659065247, 0.12903225806451613, 1.2543488061808038e-231, 89], ["I have a python script which takes a while to finish its executing depending on the passed argument. So if I run them from two terminals with different arguments, do they get their own version of the code? I can't see two .pyc files being generated.\nTerminal 1 runs: python prog.py 1000 > out_1000.out\nBefore the script running on terminal 1 terminate, i start running an another; thus terminal 2 runs:  python prog.py 100 > out_100.out\nOr basically my question is could they interfere with each other? \n", "If you're running the same script on two different machines, you'll get two separate.pyd files.  If the two machines are on different operating systems, they'll be on separate processes.\n\n", "Each Python interpreter process is independent. How the script reacts to itself being run multiple times depends on the exact code in use, but in general they should not interfere.\n", 0.8572676181793213, 0.8703001737594604, 0.8637347221374512, 0.15873015873015872, 1.0889844332430006e-231, 188], ["I have a python script which takes a while to finish its executing depending on the passed argument. So if I run them from two terminals with different arguments, do they get their own version of the code? I can't see two .pyc files being generated.\nTerminal 1 runs: python prog.py 1000 > out_1000.out\nBefore the script running on terminal 1 terminate, i start running an another; thus terminal 2 runs:  python prog.py 100 > out_100.out\nOr basically my question is could they interfere with each other? \n", "If you're running the same script on two different machines, you'll get two separate.pyd files.  If the two machines are on different operating systems, they'll be on separate processes.\n\n", "I you are writing output to the same file in disk then yes, it will be overwritten. However, it seems that you print the output to the standard output and then redirect them to a file. So that is not the case here. \nNow answer to your question is simple: there is no interaction between two different executions of the same code. When you execute a program or a script, first OS will load the code to the memory and then execute. Subsequent changes to the code has nothing to do with the code that is already running. Technically a program that is running is called a process. When you run a code on two different terminals there will be two different processes on the OS one for each of them and there is no way for the two processes to interfere unless you explicitly do that (IPC or inter-process communication) which you are not doing here. \nSo in summary you can run your code simultaneously on different terminals and they will be completely independent.\n", 0.884122371673584, 0.8348054885864258, 0.858756422996521, 0.18181818181818182, 1.1001549658594769e-231, 188], ["I know how to use ctypes to call a function from a C++ .dll in Python by creating a \"wrapper\" function that casts the Python input types to C. I think of this as essentially recreating the function signatures in Python, where the function body contains the type cast to C and a corresponding .dll function call.\nI currently have a set of C++ .dll files. Each library contains many functions, some of which are overloaded. I am tasked with writing a Python interface for each of these .dll files. My current way forward is to \"use the hammer I have\" and go through each function, lovingly crafting a corresponding Python wrapper for each... this will involve my looking at the API documentation for each of the functions within the .dlls and coding them up one by one. My instinct tells me, though, that there may be a much more efficient way to go about this.\nMy question is: Is there a programmatic way of interfacing with a Windows C++ .dll that does not require crafting corresponding wrappers for each of the functions? Thanks.\n", "an use the c_str() function to get the C string representation of a given Python object.  You could then use this string to create a wrapper function for the given function. For example, if you have the following function in a.cpp file:\nint foo(int x)\n{\n   return x;\n}\n", "I would recommend using Cython to do your wrapping. Cython allows you to use C/C++ code directly with very little changes (in addition to some boilerplate). For wrapping large libraries, it's often straightforward to get something up and running very quickly with minimal extra wrapping work (such as in Ctypes). It's also been my experience that Cython scales better... although it takes more front end work to stand Cython up rather than Ctypes, it is in my opinion more maintainable and lends itself well to the programmatic generation of wrapping code to which you allude.\n", 0.8077404499053955, 0.81941819190979, 0.8135374188423157, 0.16216216216216214, 1.0528113717254573e-231, 269], ["Is it possible to get an approximate solution to a mixed integer linear programming problem with PuLP? My problem is complex and the exact resolution takes too long.\n", " I think the answer is no. The reason is that the solution is not unique. For example, if you have a solution that is a linear combination of the two solutions, then you can't get a unique solution. Also, the optimal solution may not be unique, depending on the constraints. \n", "You probably do not mean Linear Programming but rather Mixed Integer Programming. (The original question asked about LPs).\nLPs usually solve quite fast and I don't know a good way to find an approximate solution for them. You may want to try an interior point or barrier method and set an iteration or time limit. For Simplex methods this typically does not work very well.\nMIP models can take a lot of time to solve. Solvers allow to terminate earlier by setting a gap (gap = 0 means solving to optimality). E.g.\nmodel.solve(GLPK(options=['--mipgap', '0.01']))\n", 0.8637765645980835, 0.8179695010185242, 0.840249240398407, 0.21192052980132448, 1.0630863641127427e-231, 276], ["Assume I have a float in the interval [0.0, 1.0] that is represented as a string. We will call this value floatstr. Let us also assume that this value represents an integer in the interval [0, 255].\nThe formula for converting floatstr to the integer is (in python):\nint(round(float(floatstr)*255))\nWhat is the minimum number of decimal points required in floatstr to represent this value accurately? How is this minimum number calculated, if there is a formula for doing so?\n", " The minimum is 255, because 255 is an upper bound for the number that can be represented by a single byte.\n", "Two digits obviously won't cut it. You'd only be able to represent 100 distinct values. How about 3 digits?\nSay we have a number x/255, and we display that to 3 digits after the decimal point, effectively rounding it to some number y/1000. Multiplying y/1000 by 255 and rounding it will produce x if x/255 is the closest multiple of 1/255 to y/1000.\nIf x/255 = y/1000, then it's obviously the closest multiple of 1/255. Otherwise, x/255 must be within a distance of 1/2000 of y/1000 to round to y/1000, so the closest multiple of 1/255 on the other side of y/1000 must be at least a distance 1/255 - 1/2000 away, further than x/255. Thus, x/255 is the closest multiple of 1/255 to y/1000, and 3 digits are enough. Similarly, for any denominator d with n digits, n decimal places should be enough (and if d is a power of 10, n-1 decimal places should do it).\n(I've neglected the impact of implicit floating-point rounding error in this derivation. For small denominators, floating-point rounding error should not change this analysis.)\n", 0.8596194982528687, 0.7847366333007812, 0.8204730153083801, 0.10526315789473682, 1.2761323602556544e-231, 108], ["I am playing around with some audio processing in python.  Right now I have the audio as a 2x(Large Number) numpy array.  I want to combine the channels since I only want to try some simple stuff.  I am just unsure how I should do this mathematically.  At first I thought this is kind of like converting an RGB image to gray-scale where you would average each of the color channels to create a gray pixel.  Then I thought that maybe I should add them due to the superposition principal of waves (then again average is just adding and dividing by two.)  Does anyone know the best way to do this?\n", " You can use the ndarray.sum() method to get the sum of each channel.\n", "i handle this by using Matlab.python can do the same. (left-channel+right-channel)/2.0\n", 0.8776395916938782, 0.8753101825714111, 0.8764733672142029, 0.1935483870967742, 1.3483065280626046e-231, 70], ["I am playing around with some audio processing in python.  Right now I have the audio as a 2x(Large Number) numpy array.  I want to combine the channels since I only want to try some simple stuff.  I am just unsure how I should do this mathematically.  At first I thought this is kind of like converting an RGB image to gray-scale where you would average each of the color channels to create a gray pixel.  Then I thought that maybe I should add them due to the superposition principal of waves (then again average is just adding and dividing by two.)  Does anyone know the best way to do this?\n", " You can use the ndarray.sum() method to get the sum of each channel.\n", "To convert any stereo audio to mono, what I have always seen is the following:\nFor each pair of left and right samples:\n\nAdd the values of the samples together in a way that will not overflow\nDivide the resulting value by two\nUse this resulting value as the sample in the mono track -  make sure to round it properly if you are converting it to an integer value from a floating point value\n\n", 0.8594465255737305, 0.8207682967185974, 0.8396622538566589, 0.1590909090909091, 1.2973344175574298e-231, 70], ["I'm a new user of Python\nI installed Python35 for Windows. Hello.py runs fine in a terminal.\nWhen trying to run the same in Geany the path is not found.\nThese are the settings in Geany:\nCompile: C:\\Python35\\python -m py_compile \"%f\"\nExecute: C:\\Python35\\python  \u00a8%f\u00a8\nWhat I'm doing wrong?\n", " You need to install the Python interpreter for the current user. This is done by typing \"python\" at the command prompt. If you're using a virtualenv, you can also use \"virtualenv\" to create a separate environment for your application. For example, if you have a project called \"myproject\" in your virtual environment, then you would type \"py myproject. Myproject will be in the myvirtualenvironment. You can then import my project in any other project. \n", "\nExecute: C:\\Python35\\python \u00a8%f\u00a8\n\nThis string contains the diaeresis character (\u00a8) (U+00A8) instead of the double quote (\") (U+0022).\n", 0.8178805708885193, 0.768051028251648, 0.792182981967926, 0.08421052631578947, 8.820444599897338e-232, 455], ["I have a single-threaded python3 program that I'm trying to convert to use many threads.  I have a tree-like data structure that gets read from and written to.  Potentially many threads will want to read and write at the same time.\nOne obvious way about this is to have a single lock for the entire data structure: no one can read while a write is happening, no more than one write can happen at a time, and no write can happen when there are pending reads.\nHowever, I'd like to make the locking more fine-grained for greater performance.  It's a full 16-ary tree, and when fully populated has about 5 to 6 million leafs (mostly well-balanced in practice, but no guarantee).  If I wanted the finest-grained locking, I could lock the parents of the leafs.  That would mean over 100 thousand locks.\nI must admit, I haven't tried this yet.  But I thought I'd ask first: are there any hardware limitations or performance reasons that should stop me from creating so many lock objects?  That is, should I consider just locking down to, say, depth 2 from the root (256 locks)?\nThanks for any insights.\nEDIT:\nMore details: \nI don't know how many cores yet as we're still experimenting as to just how much computing power we'll need, but I'd hazard a guess that just a handful of cores will be used.\nI'm aiming for around 50,000 threads.  There's async I/O, and one thread per socket.  During a bootstrapping phase of the code, as many threads as possible will be running simultaneously (as limited by hardware), but that's a one-time cost.  The one we're more interested in is once things are up and running.  At that point, I'd hazard a guess that only several thousand per second are running.  I need to measure the response time, but I'd guess it's around 10ms per wake period.  That's a few 10s of threads active at a time (on average).\nNow that I write that out, maybe that's the answer to my question.  If I only need a few 10s of threads reading or writing at a time, then I don't really need that fine-grained locking on the tree.\n", " If you're using a lock-free algorithm, you can use a hash tree to implement the lock. The hash function is a simple function that takes a key and returns a number. If the key is in the hash bucket, the bucket is locked. Otherwise, it is unlocked. This is the basic idea behind lock free algorithms. You can implement this in a variety of ways, such as using the Python lock object, using an array, or using some other lock mechanism. I've written a lot of lockfree", "Premature Optimization\nThis is a classic example of premature optimization.  Without knowing how much time your threads spend blocking, presumably waiting for other writes to happen, it's unclear what you have to gain from creating the added complexity of managing thousands of locks.  \nThe Global Interpreter Lock\nThreading itself can be a premature optimization.  Is your task easily threadable?  Can many threads safely work in parallel?  Tasks that require a large amount of shared state (i.e. many and frequent locks) are typically poor candidates for high thread counts.  In python, you're likely to see even less benefit because of the GIL.  Are your threads doing a lot of IO, or calling out to external applications, or using python modules written in C that properly releases the GIL?  If not, threading might not actually give you any benefit.  You can sidestep the GIL by using the multiprocessing module, but there's an overhead to passing locks and writes across process boundaries, and ironically, it might make your application much slower\nQueues\nAnother option is to use a write queue.  If threads don't actually need to share state, but they all need to write to the same object (i.e. very little reading from that object), you can simply add the writes to a queue and have a single thread process the writes, with no need for any locks.\n", 0.8448801636695862, 0.8065549731254578, 0.8252728581428528, 0.2866043613707165, 9.331094331957976e-232, 465], ["Can someone explain the difference between runspider and crawl commands? What are the contexts in which they should be used?\n", " RunSpider is a spidering tool that crawls a web page and saves the HTML in a file. Crawler is the name of the command that runs the spider.\n", "The main difference is that runspider does not need a project. That is, you can write a spider in a myspider.py file and call scrapy runspider myspider.py.\nThe crawl command requires a project in order to find the project's settings, load available spiders from SPIDER_MODULES settings, and lookup the spider by name.\nIf you need quick spider for a short task, then runspider has less boilerplate required. \n", 0.873350203037262, 0.8402894735336304, 0.8565009236335754, 0.36363636363636365, 1.2161665472637584e-231, 141], ["Is there any way by which 3D models can be represented as 3D arrays? Are there any libraries that take .obj or .blend files as input and give an array representation of the same?\nI thought that I would slice object and export the slices to an image. I would then use those images in opencv to build arrays for each slice. In the end I would combine all the arrays of all the slices to finally get a 3D array representation of my .obj file. But I gave up halfway through because it is a painfully long process to get the image slices aligned to each other.\nIs there any other index based representation I could use to represent 3D models in code?\nA 3D array would be very convenient for my purposes.\n", "ere are a number of ways to do this. One way is to use a library that has a function that takes a.OBJ file as an input, and returns an object that can represent the object. For example, the Blender object can do that. Another way would to have a separate function for every object, which would return an index for the given object in the array. Then you could just call that function on every.BLEND file you have. This is the approach I took. It works, but it's a bit of a pain. \n", "If I understand correctly, you want to create a voxel representation of 3D models? Something like the visible human displays?\nI would use one of the OBJ file loaders recommended above to import the model into an OpenGL program. Rotate and scale to whatever alignment you want along XYZ.\nThen draw the object with a fragment shader that discards any pixel with Z < 0.001 or Z >= 0.002 (or whatever resolution works - I'm just trying to explain the method). This gives you the first image slice, which you store or save. Clear and draw again this time discarding Z < 0.002 or Z >= 0.003 \u2026 Because it's the same model in the same position, all your slices will be aligned.\nHowever, are you aware that OBJ (and nearly all other 3D formats) are surface descriptions, not solid? They're hollow inside like origami models. So your 3D array representation will be mostly empty.\nHope this helps.\n", 0.8565139770507812, 0.8106168508529663, 0.8329336047172546, 0.28125, 9.533966891110756e-232, 480], ["User A shares a folder link. \nI want to use that shared link to copy that folder to my business dropbox account.\nCatch is I don't want a method which downloads the folder to my server and uploads it to my dropbox account. I want a method by which I can pass that shared link as a parameter and make the api call and then dropbox copies the folder to my dropbox account at there end.\nIs there a way using dropbox-api to copy directly to my dropbox account.\nThanks\n", " You can use the dropBox API to get a link for a shared folder. You will need to specify the sharedFolderId parameter. The API will then return a URL that you can then use to download the file. This is a good way to do it, but you will have to make sure that the link is valid. If you are using a third party library, make that sure it is not returning a 404. Also make certain that it returns a valid link, otherwise you may end up with a broken link that will not work. For example, if you use a library that returns 404, you'll have a problem. Make sure you check the return code of the library. It should be a 200, 404 or something like that. Then you just need a call to the API that gets the URL and downloads it. There is no need for you to upload the files to your server. Just make a request to that URL, and it will download it for your user. That is the way it works. Hope this helps. Good luck!\n", "Unfortunately, the Dropbox API doesn't offer a way to add a folder from a Dropbox shared link directly to an account via the API, without downloading and re-uploading it. We'll consider this a feature request for a way to do so.\n", 0.8247906565666199, 0.8678505420684814, 0.8457728624343872, 0.2164502164502165, 7.95673669817954e-232, 907], ["I am a beginner in panda3d, recently i came across blender 2.77 for modelling. I was disapointed to find that it cannot export egg files supported by panda3d, i searched online and found yabee and chicken but after installing those addons to blender also, i did'nt find an egg file exporter in the exporter's list.\nI tried using obj and dae files in blender and then converting them to egg through obj2egg(did not load mtl files) and dae2egg(i cannot see any other color than white in pview).\nI have no idea of where i am wrong.\nI am sure i have done the conversion correctly. Any help will be appreciated.\n", " Blender 2 is not a good choice for egg-based models. Blenders 2 and 3 are not compatible with each other. Egg-models are only supported in Blends 2, 3 and 4. You can use Blend2 for Egg Models, but you will have to convert them manually. There is no egg export in 2 or 3. If you want to export Egg models, you have two choices:\n1) Use Blende2 Egg Model Export plugin. It is a plugin for Blenda2. This plugin is only for export of Egg model. The plugin can be installed in your Blendas2 folder. After installing the plugin, Blending2 will detect the EggModel and will export it. \n2) You have a choice between Blendo Egg Export and Blenendo Model export. Both are available in 3D Studio Max. In Blendi Egg export, the egg model is imported into Blendra, which is the default Blendar in Max, and the model will then be exported to Blent. For Bleno Egg, it is import into the Blenta, then export to the final Blento. To import the eggs, use the import egg button in Object Inspector. Then you can export the models to a Blente file. Note that Bleni Egg is for import and export only. Models imported from Blene Egg will not be exportable. Also, if you import a model from a.blend file, that model must be in a folder named Blenes Egg. Otherwise, a warning will appear. Model imported with Blenne Egg can not export models with the same name. So, import your models into a new folder, name it Blanes Egg and import them into your project. When you export your model, make sure that the name of the folder is Blane Egg or Blan Egg (or Bla Egg if the file is named bla.blende.egg). Blanche Egg does not support Egg files. However, there is an Egg file export plugin in BBEdit. Use this plugin to import Egg File Models. Beware that this will import all the files, not just the ones you specify. Once you are done importing", "I am also new at Panda3d and I solved a Problem similar to yours just a few hours ago.\nThere are two ways to solve your Problem:\n\nDownload another version of Blender the last Blender-Version working with YABEE is 2.66.\nJust export your model as a .x - file(DirektX) this works great with Panda\n\nPS: Sorry for my english I am a German\n", 0.8001464605331421, 0.826662540435791, 0.813188374042511, 0.13202933985330076, 7.019467442542122e-232, 1815], ["I am a beginner in panda3d, recently i came across blender 2.77 for modelling. I was disapointed to find that it cannot export egg files supported by panda3d, i searched online and found yabee and chicken but after installing those addons to blender also, i did'nt find an egg file exporter in the exporter's list.\nI tried using obj and dae files in blender and then converting them to egg through obj2egg(did not load mtl files) and dae2egg(i cannot see any other color than white in pview).\nI have no idea of where i am wrong.\nI am sure i have done the conversion correctly. Any help will be appreciated.\n", " Blender 2 is not a good choice for egg-based models. Blenders 2 and 3 are not compatible with each other. Egg-models are only supported in Blends 2, 3 and 4. You can use Blend2 for Egg Models, but you will have to convert them manually. There is no egg export in 2 or 3. If you want to export Egg models, you have two choices:\n1) Use Blende2 Egg Model Export plugin. It is a plugin for Blenda2. This plugin is only for export of Egg model. The plugin can be installed in your Blendas2 folder. After installing the plugin, Blending2 will detect the EggModel and will export it. \n2) You have a choice between Blendo Egg Export and Blenendo Model export. Both are available in 3D Studio Max. In Blendi Egg export, the egg model is imported into Blendra, which is the default Blendar in Max, and the model will then be exported to Blent. For Bleno Egg, it is import into the Blenta, then export to the final Blento. To import the eggs, use the import egg button in Object Inspector. Then you can export the models to a Blente file. Note that Bleni Egg is for import and export only. Models imported from Blene Egg will not be exportable. Also, if you import a model from a.blend file, that model must be in a folder named Blenes Egg. Otherwise, a warning will appear. Model imported with Blenne Egg can not export models with the same name. So, import your models into a new folder, name it Blanes Egg and import them into your project. When you export your model, make sure that the name of the folder is Blane Egg or Blan Egg (or Bla Egg if the file is named bla.blende.egg). Blanche Egg does not support Egg files. However, there is an Egg file export plugin in BBEdit. Use this plugin to import Egg File Models. Beware that this will import all the files, not just the ones you specify. Once you are done importing", "Common error when using YABEE: Not only do you need to copy it into the addons directory, you also need to activate it in Blender.\n", 0.7859898805618286, 0.8538629412651062, 0.8185217976570129, 0.07588075880758809, 6.477220260876524e-232, 1815], ["I used Spyder, run Python 2.7.\nJust found interesting things:\n\nhash(-1) and hash(-2) both return -2, is there a problem? I though hash function on different object should return different values. I read previous posts that -1 is reserved as an error in Python.\nhash('s') returns 1835142386, then hash(1835142386) returns the same value. Is this another problem?\n\nThanks.\n", " - is a special character in python. It's used to represent negative numbers. -4 is -, -5 is +, etc. So, if you're trying to get the hash of a negative number, you'll need to use the - character. \n", "-1 is not \"reserved as an error\" in Python. Not sure what that would even mean. There are a huge number of programs you couldn't write simply and clearly if you weren't allowed to use -1.\n\"Is there a problem?\" No. Hash functions do not need to return a different hash for every object. In fact, this is not possible, since there are many more possible objects than there are hashes. CPython's hash() has the nice property of returning its argument for non-negative numbers up to sys.maxint, which is why in your second question hash(hash('s')) == hash('s'), but that is an implementation detail.\nThe fact that -1 and -2 have the same hash simply means that using those values as, for example, dictionary keys will result in a hash conflict. Hash conflicts are an expected situation and are automatically resolved by Python, and the second key added would simply go in the next available slot in the dictionary. Accessing the key that was inserted second would then be slightly slower than accessing the other one, but in most cases, not enough slower that you'd notice.\nIt is possible to construct a huge number of unequal objects all with the same hash value, which would, when stored in a dictionary or a set, cause the performance of the container to deteriorate substantially because every object added would cause a hash collision, but it isn't something you will run into unless you go looking for it.\n", 0.8757096529006958, 0.8110237717628479, 0.8421263694763184, 0.16494845360824742, 1.1084923698537105e-231, 197], ["I'm adding authentication to an existing pyramid project. The simplest form that I'm currently trying (will be expending later) is for all pages to raise HTTPForbidden. The exception view is /login, which will ask for login details and, on success, return HTTPFound with request.referer as the location.\nSo far so good, this does what I want, which is bringing users back to the page they were trying to access when the login page interrupted them. Let's call this Page A.\nThe login page is a simple HTML form with a submit button.\nHowever if the user mistypes username or password, I want to return to the login page with an error message saying \"Wrong password\" or similar. When that happens, request.referer is now the login page instead of Page A.\nHow do I 'store' Page A (or rather its URL) so that, when the user eventually succeeds in logging in, they find themselves back on Page A? Is the session used for things like this, and are there non-session ways of implementing it? I don't (yet) have a session for this simple page, and am trying to avoid adding different components in one pass.\n", "You can use a custom 404 handler to redirect to a 404 page. \n", "I recommend you to pass a parameter like login/?next=pageA.html\nIf the login fails, you could then forward your parameter next to /login again, even if the referrer points now to /login.\nThen when the user will successfully log in, you could redirect if to pageA.html that will be held in your next parameter.\nYou will indeed need to check if your parameter next is a valid one, as someone could copy-paste or try to tamper with this parameter.\n", 0.8888392448425293, 0.8347983956336975, 0.8609716892242432, 0.125, 1.3785844126745443e-231, 61], ["I'm wondering if there's a good way for me to incorporate Python scripts into my current website. \nI have a personal website & my own server that I have been working with for a while. So far it's just been html / css / javascript. I have made a Python script in the past that uses another website's API to retrieve something that I would like to display on my website. I've only used Python from the terminal to take input and spit out results. Is there a way for me to run a Python script from javascript through Ajax to get some content back? \nI don't really want to use a framework like Django or Flask because I feel as though those are mostly for entire projects. I only want to use one Python script on one page for my website. Is this even something I should do? Any advice would be great.\n", " You can use the wsgiref library to make a request to a server and get the response. You will need to parse the XML response to extract the data you need. This is a lot of work, but it is the only way to do it. If you are not familiar with XML, you can look at the documentation for wget. It is not as easy as it sounds. There are a ton of libraries out there that do this, so you will have to find one that works for you. The wgirefer library is one of them. Once you have the information you want, just call the function you desire. For example, if you wanted to call a function that would return a list of all the links on your site, call wgets.get('http://google.com/').\n", "If you are using only javascript and don't feel like a framework is the solution, you'd better rewrite your python script using javascript. These two languages have a lot in common and most of the stuff are transferable. Calling python from javascript would most likely not going to work that great. Again, unless you share your python script(which is encouraged in SO because text only question does not quite fit in here), all answers are opinion based.\n", 0.8277270197868347, 0.835090160369873, 0.831392228603363, 0.23853211009174313, 8.69359474305344e-232, 675], ["I'm wondering if there's a good way for me to incorporate Python scripts into my current website. \nI have a personal website & my own server that I have been working with for a while. So far it's just been html / css / javascript. I have made a Python script in the past that uses another website's API to retrieve something that I would like to display on my website. I've only used Python from the terminal to take input and spit out results. Is there a way for me to run a Python script from javascript through Ajax to get some content back? \nI don't really want to use a framework like Django or Flask because I feel as though those are mostly for entire projects. I only want to use one Python script on one page for my website. Is this even something I should do? Any advice would be great.\n", " You can use the wsgiref library to make a request to a server and get the response. You will need to parse the XML response to extract the data you need. This is a lot of work, but it is the only way to do it. If you are not familiar with XML, you can look at the documentation for wget. It is not as easy as it sounds. There are a ton of libraries out there that do this, so you will have to find one that works for you. The wgirefer library is one of them. Once you have the information you want, just call the function you desire. For example, if you wanted to call a function that would return a list of all the links on your site, call wgets.get('http://google.com/').\n", "I completly agree with you about Django, but I think you can give a chance to Flask, it is really light and I can be used for many porpouses. Anyway if you want to call a python scripts you need a way to call it. I think you need a \"listener\" for the script for example a service or a web service (for this reason I think Flask can be an really easy solution).\nBe careful about calling the script, a web service can be reachable from the frontend but this can not be done from a \"standard\" script.\nMy suggestion is take a look at Flask is more light that you think.\n", 0.8352009654045105, 0.8318516612052917, 0.8335229158401489, 0.3665338645418327, 8.566646876284017e-232, 675], ["I am working on a python project where I will need to work with a matrix whose size is around 10000X10000X10000.\nConsidering that:\n\nThe matrix will be dense, and should be stored in the RAM.\nI will need to perform linear algebra (with numpy, I think) on that matrix, with around O(n^3) where n=10000 operations (that are parallelizable).\n\nAre my requirements realistic? Which will be the hardware requirements I would need to work in such way in a decent time?\nI am also open to switch language (for example, performing the linear algebra operations in C) if this could improve the performances.\n", " If you are going to be doing linear operations on the matrix in O (n) time, then you will have to use a lot of memory.  If the size of the matrices is large, you can use the sparse matrix library to speed up the operations. If your matricies are sparse, the library will not be able to do the same thing. You will probably have a performance bottleneck in your code. The library is not going be a bottleneck, but it will slow you down. So, if you have the choice, use sparse matrice. It will make your life easier. Also, it is a good idea to make the code as simple as possible. This will help you to find the bottlenecks. For example: if your matrix is sparse and you want to get the first element, just do a sparse operation on it. Then, when you get to the point where you need the second element of your sparse array, do another sparse multiplication on your first sparse element. That will save you a ton of time. Another good thing is to try to keep your data in memory as much as you possibly can. Having the data on disk will speed things up. However, keep in mind that the more data you put on memory, your performance will go down, so you should try not to put too much data there. Keep in my mind, that if the performance is going down because you're putting too many data points on RAM, this is the time when the bottleneck is in place. In this case, try using a cache. A cache will allow you not only to save a bunch of RAM but also to reduce the number of operations you do on data. I'm not sure if there is such a library for python, though. There are some libraries for C, for example. But, in python you'll have more choices. Try to think of a way to optimize your algorithm. Use a profiler. Find out what is taking the most time and try and find out why. Sometimes, a simple profiling will tell you what you don't know. And, of course, make sure you use good algorithms. Good algorithms will always be faster than bad algorithms, no matter what language you code in.", "Well, the first question is, wich type of value will you store in your matrix?\nSuposing it will be of integers (and suposing that every bytes uses the ISO specification for size, 4 bytes), you will have 4*10^12 bytes to store. That's a large amount of information (4 TB), so, in first place, I don't know from where you are taking all that information, and I suggest you to only load parts of it, that you can manage easily.\nOn the other side, as you can paralellize it, I will recommend you using CUDA, if you can afford a NVIDIA card, so you will have much better performance.\nIn summary, it's hard to have all that information only in RAM, and, use paralell languajes.\nPD: You are using wrong the O() stimation about algorith time complexity. You should have said that you have a O(n), being n=size_of_the_matrix or O(nmt), being n, m and t, the dimensions of the matrix.\n", 0.8253480195999146, 0.8291325569152832, 0.8272359371185303, 0.3321428571428572, 6.989326195204702e-232, 1985], ["According to Python 2.7.12 documentation, 3.4.2.3. Invoking Descriptors\u00b6:\n\nThe default behavior for attribute access is to get, set, or delete\n  the attribute from an object\u2019s dictionary. For instance, a.x has a\n  lookup chain starting with a.__dict__['x'], then\n  type(a).__dict__['x'], and continuing through the base classes of\n  type(a) excluding metaclasses.\n\nBut why metaclasses are excluded?\nIf you continuously call type(self), no matter what self is, an instance object or a type object, you'll eventually get <type 'type'>. So I can't understand why metaclasses enjoy this \"privilege\".\n\nBy the way, I'm a little confused by this quotation: For instance objects, object.__getattribute__ are used, so I think the lookup chain should look like this:\n\na.__dict__['x']\ntype(a).__dict__['x']\nb.__dict__[x] for b in type(a).__mro__\ntype(b).__dict__[x] for b in type(a).__mro__\nc.__dict__[x] for c in type(b).__mro__\n......\n\nAm I right?\n", " The documentation is wrong. The default behaviour for accessing attributes is get. If you want to set or get an attribute, use set/getattr. \n", "This is because the attribute lookup searches all bases of type(a)(type(a).__mro__), rather than all types of type(a)(type(type(a))).\nAlso, type(self) is not called continuously, so the lookup chain looks like this:\n\na.__dict__['x']\ntype(a).__dict__['x']\nb.__dict__[x] for b in type(a).__mro__\nraise AttributeError\n\nAs @jsbueno wisely pointed out in the comment, the second step is actually included in the third one. This is because for any class, let's say class C, C itself is exactly the first item in C.__mro__.\n", 0.8557062149047852, 0.783781111240387, 0.8181660175323486, 0.1, 1.1557614282537517e-231, 142], ["Suppose if A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nThen A[0][:] prints [1, 2, 3]\nBut why does A[:][0] print [1, 2, 3] again ? \nIt should print the column [1, 4, 7], shouldn't it?\n", "The first argument to map() is the key, the second is a list of the values.\n", "[:] matches the entire list.\nSo A[:] is the same as A. So A[0][:] is the same as A[0].\nAnd A[0][:] is the same as A[0].\n", 0.8624575734138489, 0.8160055875778198, 0.8385887742042542, 0.35555555555555557, 1.2340561512781763e-231, 76], ["Suppose if A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nThen A[0][:] prints [1, 2, 3]\nBut why does A[:][0] print [1, 2, 3] again ? \nIt should print the column [1, 4, 7], shouldn't it?\n", "The first argument to map() is the key, the second is a list of the values.\n", "[:] is equivalent to copy.\nA[:][0] is the first row of a copy of A.\nA[0][:] is a copy of the first row of A.\nThe two are the same.\nTo get the first column: [a[0] for a in A]\nOr use numpy and np.array(A)[:,0]\n", 0.8632890582084656, 0.8190263509750366, 0.8405753970146179, 0.30303030303030304, 1.3933116309307982e-231, 76], ["Hendrix is a WSGI compatible server written in Tornado. I was wondering if it can be used to run an app written in Falcon ? \n", "Falcon is not a web server. It is an HTTP server, which is built on top of Tornado, a Python HTTP library.\n", "So found the solution. Created a python file according to hendrix's docs. And imported my app's wsgi callable there.\n", 0.8664787411689758, 0.8499550819396973, 0.8581374287605286, 0.09302325581395349, 1.2125989918281952e-231, 107], ["I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.\nI know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.\n", " You could use a simple regex to match the data type. \nFor example, if you have a column called \"type\" in your spreadsheet and you want it to be a string, you could do this:\n=REGEXEXTRACT(A1,\"([A-Za-z][Aa][aA][aa][aaa][aaaa][]+)\")\nThis will return a value of \"string\" if the cell contains the string \"A\", \"B\", or \"C\" and a blank otherwise. If you wanted to return \"date\", you would change the regex pattern to \"(?:[A|B|C])(?:(?:0[1-9]|1[0-2]))\"\n", "In openpyxl you'll have to go cell by cell.\nYou could use Excel's builtin Data Validation or Conditional Formatting, which openpyxl supports, for this. Let Excel do the work and talk to it using xlwings.\n", 0.7794182300567627, 0.8408833742141724, 0.8089849948883057, 0.272, 9.434216000664909e-232, 445], ["I am working on a project that requires me to read a spreadsheet provided by the user and I need to build a system to check that the contents of the spreadsheet are valid. Specifically I want to validate that each column contains a specific datatype.\nI know that this could be done by iterating over every cell in the spreadsheet, but I was hoping there is a simpler way to do it.\n", " You could use a simple regex to match the data type. \nFor example, if you have a column called \"type\" in your spreadsheet and you want it to be a string, you could do this:\n=REGEXEXTRACT(A1,\"([A-Za-z][Aa][aA][aa][aaa][aaaa][]+)\")\nThis will return a value of \"string\" if the cell contains the string \"A\", \"B\", or \"C\" and a blank otherwise. If you wanted to return \"date\", you would change the regex pattern to \"(?:[A|B|C])(?:(?:0[1-9]|1[0-2]))\"\n", "I ended up just manually looking at each cell. I have to read them all into my data structures before I can process anything anyways so it actually made sense to check then.\n", 0.7704485058784485, 0.8555291891098022, 0.810762882232666, 0.09917355371900825, 8.956981099776172e-232, 445], ["In an environment where Docker Containers are used for each application, where are Python's shared libraries stored? Are they stored separately within each Docker Container, or shared by the host O/S?\nAdditionally I'm wondering if it would be best practice to use a virtual environment regardless?\n", " The Python standard library is stored in a directory called lib/python2.7/site-packages.\nThe Python interpreter is located in /usr/bin/\n", "Just like everything else in a Docker Container, your libraries are inside the container. Unless you mount a host volume, or a volume from another container of course. On the plus side, though, they're copy-on-write, so if you're not making changes to the libraries in your container (why would you do that anyway?) then you can have 100 running containers from the same image and they don't require any extra disk space.\nSome people advocate for using a virtualenv within the container - there are pros and cons to the approach, and I don't think there's a one-sized-fits-all answer, though I would lean for not having a virtualenv.\n", 0.8194544911384583, 0.8097786903381348, 0.8145878911018372, 0.07142857142857144, 1.1532775870540082e-231, 137], ["I am trying to invoke python script from C application using system() call\nThe python script has #!/usr/bin/python3 on the first line.\nIf I do system(python_script), the script does not seem to run.\nIt seems I need to do system(/usr/bin/python3 python_script).\nI thought I do not need to specify the interpreter externally if I have #!/usr/bin/python3 in the first line of the script.\nAm I doing something wrong?\n", " You need the /usr directory to be in your path. \n", "Make sure you have executable permission for python_script.\nYou can make python_script executable by \nchmod +x python_script\nAlso check if you are giving correct path for python_script\n", 0.8892707228660583, 0.8370965123176575, 0.8623952269554138, 0.0975609756097561, 1.430389201127128e-231, 50], ["When you need to access an object's attributes dynamically in Python, you can just use the builtin functions hasattr(object, attribute) or getattr(object, attribute).\nHowever this seems like an odd order for the syntax to take. It's less readable and intuitive as it messes up the regular sentence structure for English.\n\nif hasattr(person, age):\n  if has attribute Person age\n\nWhere having it as a method of the object would be much more readable:\n\nif person.hasattr(age):\n  if Person has attribute age\n\nIs there a particular reason for not implementing it this way? I could imagine there are cases where you're not sure if the object is even a proper object, rather than just None, but surely in those cases of uncertainty you could just use the builtin function anyway for extra safety.\nIs there some other drawback or consideration I'm not thinking of that makes adding these not worth it?\n", " I think the reason is that it's not really a good idea to have a function that returns a value that is not a valid attribute. \n", "Its part of the language design. I guess your find some docs about the more complicated thoughts behind it, but the key points are like\n\nYou suggest to use a function of an object for a builtin function on all objects. Why should this function be specific to this object?\nSemantics: the getattr function works on objects, not as part of an object.\nNamespace: The functions of an object are defined by you, not by the language. Internal functions are of the form __getattr__ and you will find this function on your object ;-). And getattr uses it internally, so you can even override it (if you know, what you're doing).\n\n", 0.8648634552955627, 0.8201014995574951, 0.8418879508972168, 0.13043478260869568, 1.1988328686372911e-231, 128], ["When you need to access an object's attributes dynamically in Python, you can just use the builtin functions hasattr(object, attribute) or getattr(object, attribute).\nHowever this seems like an odd order for the syntax to take. It's less readable and intuitive as it messes up the regular sentence structure for English.\n\nif hasattr(person, age):\n  if has attribute Person age\n\nWhere having it as a method of the object would be much more readable:\n\nif person.hasattr(age):\n  if Person has attribute age\n\nIs there a particular reason for not implementing it this way? I could imagine there are cases where you're not sure if the object is even a proper object, rather than just None, but surely in those cases of uncertainty you could just use the builtin function anyway for extra safety.\nIs there some other drawback or consideration I'm not thinking of that makes adding these not worth it?\n", " I think the reason is that it's not really a good idea to have a function that returns a value that is not a valid attribute. \n", "You'll find quite a few similar examples - like len(obj) instead of obj.length(), hash(obj) instead of obj.hash(), isinstance(obj, cls) instead of obj.isinstance(cls). You may also have noticed that addition is spelled obj1 + obj2 instead of obj1.add(obj2), substraction spelled obj1 - obj2 instead of obj1.sub(obj2) etc...  The point is that some builtin \"functions\" are to be considered as operators rather than really functions, and are supported by \"__magic__\" methods (__len__, __hash__, __add__ etc). \nAs of the \"why\", you'd have to ask GvR but historical reasons set asides, it at least avoids a lot of namespace pollution / name clashes. How would you name the length of a \"Line\" or \"Rectangle\" class if length was already a \"kind of but not explicitely reserved\" name ? And how should introspection understand that Rectangle.length() doesn't mean Rectangle is a sizeable sequence-like object ?\nUsing generic \"operator\" functions (note that proper operators also exist as functions, cf the operator module) + \"__magic__\" methods make the intention clear and leaves normal names open for \"user space\" semantic.\nwrt/ the \"regular sentence structure for English\", I have to say I don\n\nI could imagine there are cases where you're not sure if the object is\n  even a proper object, rather than just None\n\nNone is a \"proper\" object. Everything in Python (well, everything you can bind to a name) is a \"proper\" object. \n", 0.8504347205162048, 0.7764111757278442, 0.811738908290863, 0.12878787878787878, 1.1988328686372911e-231, 128], ["As part of a bigger set of tests I need to extract all the formulas within an uploaded Excel workbook.  I then need to parse each formula into its respective range references and dump those references into a simple database.  For example, if Cell A1 has a formula =B1 + C1 then my database would record B1 and C1 as referenced cells.\nCurrently I read formulas one at a time using openpyxl and then parse them.  This is fine for smaller workbooks, but for large workbooks it can be very slow.  It feels entirely inefficient.\nCould pandas or a similar module extract Excel formulas faster?  Or is there perhaps a better way to extract all workbook formulas than reading it one cell at a time?\nAny advice would be highly appreciated.\n", " I think you're looking for the xlrd module. It's a Python module that allows you to read and write Excel files. You can use it to get the data you need, and it will also allow you do some simple parsing of the Excel file. I've used it in the past to do a lot of parsing and parsing is very fast. The only problem is that it's not free. But it is open source, so you can download it and use the source code. If you want to use a commercial product, you'll have to pay for it. Anyway, I'm not sure if it would help you, as it seems to be a bit more complex than just reading the file and writing it out. However, it does seem to work well for what you are trying to accomplish. Good luck!\n", "What do you mean by \"extracting the formulae faster\"? They are stored with each cell so you have to go cell by cell. When it comes to parsing, openpyxl includes a tokeniser which you might find useful. In theory this would allow you to read the worksheet XML files directly and only parse the nodes with formulae in them. However, you'd also have to handle the \"shared formulae\" that some applications use. openpyxl automatically converts such formulae into per-cell ones.\nInternally Pandas relies on xlrd to read the files, so the ETL of getting the stuff into Pandas won't be faster than working directly with worksheet objects.\n", 0.861098051071167, 0.8440172672271729, 0.8524720668792725, 0.3178294573643411, 8.589758290207e-232, 688], ["If your seeing this I guess you are looking to run chromium on a raspberry pi with selenium.\nlike this Driver = webdriver.Chrome(\"path/to/chomedriver\") or like this webdriver.Chrome()\n", " I'm not sure what you're trying to do, but I think you'll have to use the web driver in a different way. \nThe web browser is a separate process, so you can't use it as a driver. You'll need to start a new process with the browser, and then use that process as the driver for the pi. This is what I've done, for example:\nimport os\nfrom selinux import launch\nos.environ['DISPLAY'] = 'localhost'\nlaunch(['-n', '-e', \"DISABLED\", '--no-sandbox',\n   'webdriver',  # this is the name of the file you want to launch the chrome in\n])\n", "I have concluded that after hours and a hole night of debugging that you can't because there is no chromedriver compatible with a raspberry pi processor. Even if you download the linux 32bit. You can confirm this by running this in a terminal window path/to/chromedriver it will give you this error \n\ncannot execute binary file: Exec format error\n\nhope this helps anyone that wanted to do this :)\n", 0.8076120018959045, 0.8260630369186401, 0.8167332410812378, 0.313953488372093, 8.976402354421327e-232, 526]], "ncols": 9, "nrows": 264}